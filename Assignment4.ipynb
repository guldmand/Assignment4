{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd007b53",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621bcb29",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2e084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vit-keras tensorflow-addons wandb --quiet opencv-python python-dotenv nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7580d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 06:28:52.734697: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764912532.765355   19389 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764912532.779148   19389 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-05 06:28:52.897514: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, time, math, json, random, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "from wandb.integration.keras import WandbCallback\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0282c822",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4603dcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow version: 2.18.1\n"
     ]
    }
   ],
   "source": [
    "# Tjek GPU\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2c18dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd08bf",
   "metadata": {},
   "source": [
    "### Reproduceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f3e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813d07bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Will use standard TensorFlow LSTM implementation to avoid cuDNN errors\n"
     ]
    }
   ],
   "source": [
    "# Fix cuDNN compatibility issue for LSTM/GRU layers\n",
    "# Force use of standard LSTM implementation (not cuDNN optimized version)\n",
    "print(\"âœ… Will use standard TensorFlow LSTM implementation to avoid cuDNN errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd23f4f",
   "metadata": {},
   "source": [
    "### Notebook configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c12831ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-config\n",
    "BATCH    = 64\n",
    "EPOCHS   = 300\n",
    "LR       = 1e-4\n",
    "VAL_SPLIT= 0.2\n",
    "\n",
    "# RNN parameters\n",
    "max_tokens = 1000 \n",
    "output_sequence_length = 100\n",
    "pad_to_max_tokens = True\n",
    "\n",
    "# TITLE + CHUNKS configuration for sequence-based RNN\n",
    "MAX_BODY_CHUNKS = 5      # Max sentence chunks from review body\n",
    "MAX_CHUNKS_TOTAL = 6     # title + 5 body chunks = 6 total\n",
    "EMBED_DIM = 1024         # BGE-M3 embedding dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89436c",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b598ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get W&B variables from .env\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "project = os.getenv(\"WANDB_PROJECT\")\n",
    "entity = os.getenv(\"WANDB_ENTITY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "158551a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/guldmand/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/guldmand/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguldmand\u001b[0m (\u001b[33mguldmand-university-of-southern-denmark\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguldmand\u001b[0m (\u001b[33mguldmand-university-of-southern-denmark\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… W&B login successful â€” runs are now initialized inside train_one_model()\n"
     ]
    }
   ],
   "source": [
    "# get W&B variables from .env\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "project = os.getenv(\"WANDB_PROJECT\")\n",
    "entity = os.getenv(\"WANDB_ENTITY\")\n",
    "\n",
    "WANDB_PROJECT = project\n",
    "WANDB_ENTITY = entity\n",
    "wandb.login(key=api_key, verify=True)\n",
    "\n",
    "os.makedirs(\"progress\", exist_ok=True)\n",
    "\n",
    "print(\"âœ… W&B login successful â€” runs are now initialized inside train_one_model()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5df7b6",
   "metadata": {},
   "source": [
    "## 1) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e42c3",
   "metadata": {},
   "source": [
    "### 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c37425bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RNN text data\n",
    "X_train = np.load(\"../../data/X_train.npy\", allow_pickle=True)\n",
    "y_train = np.load(\"../../data/y_train.npy\", allow_pickle=True)\n",
    "X_test = np.load(\"../../data/X_test.npy\", allow_pickle=True)\n",
    "\n",
    "# Create pandas DataFrames for easier handling\n",
    "df_Xtrain = pd.DataFrame(X_train,columns=['reviewerID','reviewText','summary'])\n",
    "df_ytrain = pd.DataFrame(y_train,columns=['overall'])\n",
    "df_train = pd.concat([df_ytrain, df_Xtrain], axis=1)\n",
    "df_Xtest = pd.DataFrame(X_test,columns=['reviewerID','reviewText','summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e46519c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029e89a",
   "metadata": {},
   "source": [
    "### 1.1.1 Text Preprocessing\n",
    "\n",
    "Minimal preprocessing to remove noise while preserving semantics.\n",
    "BGE-M3 is trained on raw text with contractions, punctuation, and mixed case - we keep it that way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bdf382f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text cleaning function ready (clean_text)\n",
      "   - Removes: URLs, HTML, extra whitespace\n",
      "   - Keeps: contractions, punctuation, stopwords, case\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Minimal text cleaning for BGE-M3 embeddings.\n",
    "    \n",
    "    We ONLY remove:\n",
    "    - URLs (noise)\n",
    "    - HTML tags (noise)\n",
    "    - Extra whitespace (normalization)\n",
    "    \n",
    "    We KEEP:\n",
    "    - Contractions (I'm, don't, can't) - BGE-M3 is trained on these\n",
    "    - Punctuation (!, ?, ...) - carries sentiment signal\n",
    "    - Stopwords (not, very, but) - crucial for sentiment\n",
    "    - Mixed case - BGE-M3 is case-sensitive\n",
    "    - Numbers and special chars - may have meaning\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Normalize whitespace (replace multiple spaces with single space)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"âœ… Text cleaning function ready (clean_text)\")\n",
    "print(\"   - Removes: URLs, HTML, extra whitespace\")\n",
    "print(\"   - Keeps: contractions, punctuation, stopwords, case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dce280ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text preprocessing...\n",
      "Before cleaning - Train reviews: 30850, Test reviews: 3428\n",
      "\n",
      "âœ… Preprocessing complete!\n",
      "   Empty reviews after cleaning:\n",
      "      Train: 11/30850 (0.04%)\n",
      "      Test:  2/3428 (0.06%)\n",
      "\n",
      "   Sample cleaned review (first 200 chars):\n",
      "   One of my favorite perfumes and the fact that it is unisex is awesome. I'm gifting this for my nephew....\n",
      "\n",
      "âœ… Preprocessing complete!\n",
      "   Empty reviews after cleaning:\n",
      "      Train: 11/30850 (0.04%)\n",
      "      Test:  2/3428 (0.06%)\n",
      "\n",
      "   Sample cleaned review (first 200 chars):\n",
      "   One of my favorite perfumes and the fact that it is unisex is awesome. I'm gifting this for my nephew....\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to all text data\n",
    "print(\"Applying text preprocessing...\")\n",
    "print(f\"Before cleaning - Train reviews: {len(df_Xtrain)}, Test reviews: {len(df_Xtest)}\")\n",
    "\n",
    "# Clean training data\n",
    "df_Xtrain['reviewText'] = df_Xtrain['reviewText'].apply(clean_text)\n",
    "df_Xtrain['summary'] = df_Xtrain['summary'].apply(clean_text)\n",
    "\n",
    "# Clean test data\n",
    "df_Xtest['reviewText'] = df_Xtest['reviewText'].apply(clean_text)\n",
    "df_Xtest['summary'] = df_Xtest['summary'].apply(clean_text)\n",
    "\n",
    "# Check for empty reviews after cleaning\n",
    "empty_train = (df_Xtrain['reviewText'].str.len() == 0).sum()\n",
    "empty_test = (df_Xtest['reviewText'].str.len() == 0).sum()\n",
    "\n",
    "print(f\"\\nâœ… Preprocessing complete!\")\n",
    "print(f\"   Empty reviews after cleaning:\")\n",
    "print(f\"      Train: {empty_train}/{len(df_Xtrain)} ({100*empty_train/len(df_Xtrain):.2f}%)\")\n",
    "print(f\"      Test:  {empty_test}/{len(df_Xtest)} ({100*empty_test/len(df_Xtest):.2f}%)\")\n",
    "print(f\"\\n   Sample cleaned review (first 200 chars):\")\n",
    "print(f\"   {df_Xtrain['reviewText'].iloc[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564ef9c",
   "metadata": {},
   "source": [
    "### 1.2 Stratified split (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3654abc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: (24680, 3) (24680, 1)\n",
      "Val   split: (6170, 3) (6170, 1)\n"
     ]
    }
   ],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    df_Xtrain, df_ytrain,\n",
    "    test_size=VAL_SPLIT,\n",
    "    random_state=SEED,\n",
    "    stratify=df_ytrain\n",
    ")\n",
    "\n",
    "print(\"Train split:\", X_tr.shape, y_tr.shape)\n",
    "print(\"Val   split:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5cefa7",
   "metadata": {},
   "source": [
    "### 1.3 tf.data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cc45a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764912548.668037   19389 gpu_process_state.cc:201] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1764912548.668213   19389 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7221 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text datasets created:\n",
      "   Train: 24680 samples\n",
      "   Val:   6170 samples\n",
      "   Test:  3428 samples\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Extract text from DataFrames (reviewText column) and convert to string\n",
    "X_tr_text = X_tr['reviewText'].astype(str).values\n",
    "X_val_text = X_val['reviewText'].astype(str).values\n",
    "X_test_text = df_Xtest['reviewText'].astype(str).values\n",
    "\n",
    "# Convert labels to 0-indexed (1-5 stars -> 0-4 for sparse_categorical_crossentropy)\n",
    "# Flatten the DataFrame values to 1D array and convert to int32\n",
    "y_tr_indexed = y_tr['overall'].values.flatten().astype(np.int32) - 1\n",
    "y_val_indexed = y_val['overall'].values.flatten().astype(np.int32) - 1\n",
    "\n",
    "def make_train_ds(X_text, y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_text, y))\n",
    "    ds = ds.shuffle(10_000, seed=SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_eval_ds(X_text, y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_text, y))\n",
    "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_test_ds(X_text):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(X_text)\n",
    "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_train_ds(X_tr_text, y_tr_indexed)\n",
    "val_ds   = make_eval_ds(X_val_text, y_val_indexed)\n",
    "test_ds  = make_test_ds(X_test_text)\n",
    "\n",
    "print(f\"âœ… Text datasets created:\")\n",
    "print(f\"   Train: {len(X_tr_text)} samples\")\n",
    "print(f\"   Val:   {len(X_val_text)} samples\")\n",
    "print(f\"   Test:  {len(X_test_text)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d47014",
   "metadata": {},
   "source": [
    "## 2) Embeddings\n",
    "for scentence embeddings we wanna use \n",
    "https://huggingface.co/BAAI/bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeaf349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4f237dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BAAI/bge-m3 (PyTorch for embedding generation)...\n",
      "âœ… BGE-M3 loaded on device: cuda\n",
      "   Embedding dimension: 1024\n",
      "   Max sequence length: 8192\n",
      "   GPU: NVIDIA GeForce GTX 1080\n",
      "   GPU memory: 7.9 GB\n",
      "âœ… BGE-M3 loaded on device: cuda\n",
      "   Embedding dimension: 1024\n",
      "   Max sequence length: 8192\n",
      "   GPU: NVIDIA GeForce GTX 1080\n",
      "   GPU memory: 7.9 GB\n"
     ]
    }
   ],
   "source": [
    "# Setup embeddings using BAAI/bge-m3 from Huggingface\n",
    "# Note: We'll use PyTorch for embedding generation (offline preprocessing),\n",
    "# then use the embeddings in TensorFlow/Keras for RNN training\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load BGE-M3 model and tokenizer (PyTorch for embedding generation only)\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "print(f\"Loading {model_name} (PyTorch for embedding generation)...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Use safetensors to avoid PyTorch version requirement\n",
    "embedding_model = AutoModel.from_pretrained(model_name, use_safetensors=True)\n",
    "\n",
    "# Use GPU (for Colab/Kaggle without watchdog timer)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_model = embedding_model.to(device)\n",
    "embedding_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"âœ… BGE-M3 loaded on device: {device}\")\n",
    "print(f\"   Embedding dimension: {embedding_model.config.hidden_size}\")\n",
    "print(f\"   Max sequence length: {tokenizer.model_max_length}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d60e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunking functions ready (simple_sentence_split, build_chunks)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def simple_sentence_split(text):\n",
    "    \"\"\"\n",
    "    Split text into sentences using simple regex.\n",
    "    Handles common sentence endings: . ! ?\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Split on sentence boundaries (., !, ?)\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    # Clean and filter out empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def build_chunks(df, max_body_chunks=MAX_BODY_CHUNKS):\n",
    "    \"\"\"\n",
    "    Build chunks for each review: [title, sent1, sent2, ..., sentN]\n",
    "    \n",
    "    Returns list of lists, where each inner list has:\n",
    "    - First element: review title (summary)\n",
    "    - Next elements: up to max_body_chunks sentences from review body\n",
    "    \n",
    "    Example: [[\"Great product\", \"I love it\", \"Works perfectly\"], ...]\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        chunks = []\n",
    "        \n",
    "        # Add title as first chunk\n",
    "        title = row['summary'] if pd.notna(row['summary']) else \"\"\n",
    "        chunks.append(title)\n",
    "        \n",
    "        # Split body into sentences\n",
    "        body = row['reviewText'] if pd.notna(row['reviewText']) else \"\"\n",
    "        sentences = simple_sentence_split(body)\n",
    "        \n",
    "        # Add up to max_body_chunks sentences\n",
    "        for i in range(min(len(sentences), max_body_chunks)):\n",
    "            chunks.append(sentences[i])\n",
    "        \n",
    "        all_chunks.append(chunks)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "print(\"âœ… Chunking functions ready (simple_sentence_split, build_chunks)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d2829e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunked embedding function ready (embed_chunks_with_bge)\n"
     ]
    }
   ],
   "source": [
    "def embed_chunks_with_bge(all_chunks, max_chunks=MAX_CHUNKS_TOTAL, embed_dim=EMBED_DIM, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate BGE-M3 embeddings for chunked text sequences.\n",
    "    \n",
    "    Input: List of chunk lists, e.g., [[\"title1\", \"sent1\", \"sent2\"], [\"title2\", \"sent1\"], ...]\n",
    "    Output: Numpy array of shape (N, max_chunks, embed_dim) with zero-padding\n",
    "    \n",
    "    Steps:\n",
    "    1. Flatten all chunks into a single list\n",
    "    2. Generate embeddings for all chunks in batches\n",
    "    3. Reconstruct into (N, max_chunks, embed_dim) with zero-padding\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    \n",
    "    # Step 1: Flatten and track positions\n",
    "    flat_chunks = []\n",
    "    chunk_counts = []\n",
    "    \n",
    "    for chunks in all_chunks:\n",
    "        chunk_counts.append(len(chunks))\n",
    "        flat_chunks.extend(chunks)\n",
    "    \n",
    "    total_chunks = len(flat_chunks)\n",
    "    print(f\"Starting chunked embedding: {len(all_chunks)} reviews, {total_chunks} total chunks\")\n",
    "    \n",
    "    # Step 2: Generate embeddings for all chunks using existing function\n",
    "    flat_embeddings = get_bge_embeddings(flat_chunks, batch_size=batch_size)\n",
    "    \n",
    "    # Step 3: Reconstruct into (N, max_chunks, embed_dim)\n",
    "    result = np.zeros((len(all_chunks), max_chunks, embed_dim), dtype=np.float32)\n",
    "    \n",
    "    chunk_idx = 0\n",
    "    for review_idx, count in enumerate(chunk_counts):\n",
    "        # Take embeddings for this review's chunks\n",
    "        review_embeddings = flat_embeddings[chunk_idx:chunk_idx + count]\n",
    "        \n",
    "        # Place into result array (up to max_chunks)\n",
    "        num_to_copy = min(count, max_chunks)\n",
    "        result[review_idx, :num_to_copy, :] = review_embeddings[:num_to_copy]\n",
    "        \n",
    "        chunk_idx += count\n",
    "    \n",
    "    print(f\"âœ… Created chunked embeddings: shape {result.shape}\")\n",
    "    return result\n",
    "\n",
    "print(\"âœ… Chunked embedding function ready (embed_chunks_with_bge)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82998ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import psutil\n",
    "#print(f\"Physical cores: {psutil.cpu_count(logical=False)}\")\n",
    "#print(f\"Logical cores (with HT): {psutil.cpu_count(logical=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9003c826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BGE-M3 embedding function ready (GPU mode for Colab/Kaggle)\n",
      "âœ… BGE-M3 embedding function ready (optimized with GPU memory management)\n"
     ]
    }
   ],
   "source": [
    "# Create embedding function with GPU (for Colab/Kaggle)\n",
    "def get_bge_embeddings(texts, batch_size=16):\n",
    "    \"\"\"\n",
    "    Generate BGE-M3 embeddings for a list of texts using PyTorch.\n",
    "    Uses mean pooling on subword tokens for sentence-level embeddings.\n",
    "    Returns numpy arrays that can be used with TensorFlow/Keras.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    all_embeddings = []\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Starting embedding generation: {len(texts)} texts, {total_batches} batches, batch_size={batch_size}\")\n",
    "    print(f\"â±ï¸  Estimated time on GPU: ~{total_batches * 0.1:.0f} seconds (~{total_batches * 0.1 / 60:.1f} minutes)\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_num = i // batch_size + 1\n",
    "            \n",
    "            # Progress reporting (every 100 batches or first/last)\n",
    "            if batch_num % 100 == 0 or batch_num == 1 or batch_num == total_batches:\n",
    "                print(f\"  Processing batch {batch_num}/{total_batches}...\")\n",
    "            \n",
    "            # Tokenize with padding and truncation\n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device (GPU or CPU)\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = embedding_model(**encoded)\n",
    "            \n",
    "            # Mean pooling: average over sequence length (subword tokens)\n",
    "            # Mask padding tokens using attention_mask\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "            \n",
    "            # Expand attention mask for broadcasting\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            \n",
    "            # Sum embeddings, mask padding\n",
    "            sum_embeddings = torch.sum(token_embeddings * mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "            \n",
    "            # Mean pooling\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            \n",
    "            # Convert to numpy (move to CPU first if on GPU)\n",
    "            all_embeddings.append(mean_embeddings.cpu().numpy())\n",
    "            \n",
    "            # Memory cleanup every 50 batches\n",
    "            if batch_num % 50 == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "    \n",
    "    print(f\"âœ… Completed {total_batches} batches\")\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"âœ… BGE-M3 embedding function ready (GPU mode for Colab/Kaggle)\")\n",
    "\n",
    "print(\"âœ… BGE-M3 embedding function ready (optimized with GPU memory management)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d6ef26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear CUDA cache and reset GPU state\n",
    "#import torch\n",
    "#import gc\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    torch.cuda.empty_cache()\n",
    "#    gc.collect()\n",
    "#    print(\"âœ… GPU cache cleared\")\n",
    "#    print(f\"   GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "#    print(f\"   GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.1f} MB\")\n",
    "#else:\n",
    "#    print(\"âš ï¸  No CUDA device available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef192dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD: Non-chunked embedding generation (replaced by TITLE + CHUNKS architecture)\n",
    "# # Generate embeddings for train, val, and test sets\n",
    "# print(\"Generating BGE-M3 embeddings...\")\n",
    "# print(\"Using batch_size=8 for GPU stability...\")\n",
    "# print(\"This will take several minutes...\")\n",
    "# \n",
    "# # Generate embeddings with very small batch size\n",
    "# X_tr_embeddings = get_bge_embeddings(X_tr_text.tolist(), batch_size=8)\n",
    "# X_val_embeddings = get_bge_embeddings(X_val_text.tolist(), batch_size=8)\n",
    "# X_test_embeddings = get_bge_embeddings(X_test_text.tolist(), batch_size=8)\n",
    "# \n",
    "# print(f\"\\nâœ… Embeddings generated:\")\n",
    "# print(f\"   Train: {X_tr_embeddings.shape}\")\n",
    "# print(f\"   Val:   {X_val_embeddings.shape}\")\n",
    "# print(f\"   Test:  {X_test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09096cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD: Save non-chunked embeddings (replaced by TITLE + CHUNKS architecture)\n",
    "# # Save embeddings to disk (so we don't have to regenerate them if kernel crashes)\n",
    "# print(\"Saving embeddings to disk...\")\n",
    "# np.save('X_tr_embeddings_bge-m3.npy', X_tr_embeddings)\n",
    "# np.save('X_val_embeddings_bge-m3.npy', X_val_embeddings)\n",
    "# np.save('X_test_embeddings_bge-m3.npy', X_test_embeddings)\n",
    "# print(\"âœ… Embeddings saved:\")\n",
    "# print(\"   - X_tr_embeddings_bge-m3.npy\")\n",
    "# print(\"   - X_val_embeddings_bge-m3.npy\")\n",
    "# print(\"   - X_test_embeddings_bge-m3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d4f8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD: Load non-chunked embeddings (replaced by TITLE + CHUNKS architecture)\n",
    "# # ALTERNATIVE: Load embeddings from disk (skip the 27-minute generation step)\n",
    "# X_tr_embeddings = np.load('X_tr_embeddings_bge-m3.npy')\n",
    "# X_val_embeddings = np.load('X_val_embeddings_bge-m3.npy')\n",
    "# X_test_embeddings = np.load('X_test_embeddings_bge-m3.npy')\n",
    "# print(f\"âœ… Embeddings loaded from disk:\")\n",
    "# print(f\"   Train: {X_tr_embeddings.shape}\")\n",
    "# print(f\"   Val:   {X_val_embeddings.shape}\")\n",
    "# print(f\"   Test:  {X_test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f268125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset creation functions ready (make_train_ds_embeddings, make_eval_ds_embeddings, make_test_ds_embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Dataset creation functions (work for both regular and chunked embeddings)\n",
    "def make_train_ds_embeddings(X_emb, y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_emb, y))\n",
    "    ds = ds.shuffle(10_000, seed=SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_eval_ds_embeddings(X_emb, y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_emb, y))\n",
    "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_test_ds_embeddings(X_emb):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(X_emb)\n",
    "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# OLD: Non-chunked datasets (replaced by chunked version below)\n",
    "# train_ds_emb = make_train_ds_embeddings(X_tr_embeddings, y_tr_indexed)\n",
    "# val_ds_emb   = make_eval_ds_embeddings(X_val_embeddings, y_val_indexed)\n",
    "# test_ds_emb  = make_test_ds_embeddings(X_test_embeddings)\n",
    "# print(\"âœ… Embedding-based datasets created and ready for RNN training\")\n",
    "\n",
    "print(\"âœ… Dataset creation functions ready (make_train_ds_embeddings, make_eval_ds_embeddings, make_test_ds_embeddings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebbb416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ====================================================================\n",
    "# # GENERATE CHUNKED EMBEDDINGS (TITLE + CHUNKS approach)\n",
    "# # This will take 30-40 minutes - run once, then use load cell below\n",
    "# # ====================================================================\n",
    "\n",
    "# print(\"=\"*70)\n",
    "# print(\"GENERATING CHUNKED EMBEDDINGS (TITLE + CHUNKS)\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Step 1: Build chunks for all datasets\n",
    "# print(\"\\n1ï¸âƒ£ Building chunks...\")\n",
    "# train_chunks = build_chunks(X_tr, max_body_chunks=MAX_BODY_CHUNKS)\n",
    "# val_chunks = build_chunks(X_val, max_body_chunks=MAX_BODY_CHUNKS)\n",
    "# test_chunks = build_chunks(df_Xtest, max_body_chunks=MAX_BODY_CHUNKS)\n",
    "\n",
    "# print(f\"   Train: {len(train_chunks)} reviews\")\n",
    "# print(f\"   Val:   {len(val_chunks)} reviews\")\n",
    "# print(f\"   Test:  {len(test_chunks)} reviews\")\n",
    "\n",
    "# # Step 2: Generate chunked embeddings\n",
    "# print(\"\\n2ï¸âƒ£ Generating chunked embeddings (this takes ~15-20 minutes on Colab GPU)...\")\n",
    "# print(\"Using batch_size=16 for GPU stability...\")\n",
    "\n",
    "# X_tr_embeddings_chunked = embed_chunks_with_bge(train_chunks, max_chunks=MAX_CHUNKS_TOTAL, embed_dim=EMBED_DIM, batch_size=16)\n",
    "# X_val_embeddings_chunked = embed_chunks_with_bge(val_chunks, max_chunks=MAX_CHUNKS_TOTAL, embed_dim=EMBED_DIM, batch_size=16)\n",
    "# X_test_embeddings_chunked = embed_chunks_with_bge(test_chunks, max_chunks=MAX_CHUNKS_TOTAL, embed_dim=EMBED_DIM, batch_size=16)\n",
    "\n",
    "# print(f\"\\nâœ… Chunked embeddings generated:\")\n",
    "# print(f\"   Train: {X_tr_embeddings_chunked.shape}\")\n",
    "# print(f\"   Val:   {X_val_embeddings_chunked.shape}\")\n",
    "# print(f\"   Test:  {X_test_embeddings_chunked.shape}\")\n",
    "\n",
    "# # Step 3: Save to disk with _chunked suffix\n",
    "# print(\"\\n3ï¸âƒ£ Saving chunked embeddings to disk...\")\n",
    "# np.save('X_tr_embeddings_chunked_bge-m3.npy', X_tr_embeddings_chunked)\n",
    "# np.save('X_val_embeddings_chunked_bge-m3.npy', X_val_embeddings_chunked)\n",
    "# np.save('X_test_embeddings_chunked_bge-m3.npy', X_test_embeddings_chunked)\n",
    "\n",
    "# print(\"âœ… Chunked embeddings saved:\")\n",
    "# print(\"   - X_tr_embeddings_chunked_bge-m3.npy\")\n",
    "# print(\"   - X_val_embeddings_chunked_bge-m3.npy\")\n",
    "# print(\"   - X_test_embeddings_chunked_bge-m3.npy\")\n",
    "# print(\"\\nğŸ’¡ Old embeddings preserved (no _chunked suffix)\")\n",
    "# print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c8c8719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunked embeddings from disk...\n",
      "âœ… Chunked embeddings loaded:\n",
      "   Train: (24680, 6, 1024)\n",
      "   Val:   (6170, 6, 1024)\n",
      "   Test:  (3428, 6, 1024)\n",
      "âœ… Chunked embeddings loaded:\n",
      "   Train: (24680, 6, 1024)\n",
      "   Val:   (6170, 6, 1024)\n",
      "   Test:  (3428, 6, 1024)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# ALTERNATIVE: Load pre-computed CHUNKED embeddings from disk\n",
    "# Use this to skip the 30-40 minute generation step\n",
    "# ====================================================================\n",
    "\n",
    "print(\"Loading chunked embeddings from disk...\")\n",
    "X_tr_embeddings_chunked = np.load('X_tr_embeddings_chunked_bge-m3.npy')\n",
    "X_val_embeddings_chunked = np.load('X_val_embeddings_chunked_bge-m3.npy')\n",
    "X_test_embeddings_chunked = np.load('X_test_embeddings_chunked_bge-m3.npy')\n",
    "\n",
    "print(f\"âœ… Chunked embeddings loaded:\")\n",
    "print(f\"   Train: {X_tr_embeddings_chunked.shape}\")\n",
    "print(f\"   Val:   {X_val_embeddings_chunked.shape}\")\n",
    "print(f\"   Test:  {X_test_embeddings_chunked.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10d38c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunked embedding datasets created and ready for RNN training\n",
      "   Input shape: (6, 1024) = (6, 1024)\n",
      "   Output shape: (5,) for 5-class classification\n"
     ]
    }
   ],
   "source": [
    "# Create TensorFlow datasets with CHUNKED embeddings\n",
    "# Uses same functions as before, but now with (N, 6, 1024) shape instead of (N, 1024)\n",
    "\n",
    "# Force CPU for dataset creation to avoid GPU watchdog timeout\n",
    "with tf.device('/CPU:0'):\n",
    "    train_ds_chunked = make_train_ds_embeddings(X_tr_embeddings_chunked, y_tr_indexed)\n",
    "    val_ds_chunked   = make_eval_ds_embeddings(X_val_embeddings_chunked, y_val_indexed)\n",
    "    test_ds_chunked  = make_test_ds_embeddings(X_test_embeddings_chunked)\n",
    "\n",
    "print(\"âœ… Chunked embedding datasets created and ready for RNN training\")\n",
    "print(f\"   Input shape: ({MAX_CHUNKS_TOTAL}, {EMBED_DIM}) = (6, 1024)\")\n",
    "print(f\"   Output shape: (5,) for 5-class classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1209657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Class weights computed:\n",
      "   Class distribution in training data:\n",
      "      1 star:   788 samples ( 3.19%) â†’ weight: 6.264\n",
      "      2 star:  1077 samples ( 4.36%) â†’ weight: 4.583\n",
      "      3 star:  2797 samples (11.33%) â†’ weight: 1.765\n",
      "      4 star:  5640 samples (22.85%) â†’ weight: 0.875\n",
      "      5 star: 14378 samples (58.26%) â†’ weight: 0.343\n",
      "\n",
      "ğŸ’¡ Class weights will be used during training to handle imbalance\n",
      "   Minority classes get higher weights â†’ model pays more attention to them\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# COMPUTE CLASS WEIGHTS to handle class imbalance\n",
    "# ====================================================================\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights based on training data\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_tr_indexed),\n",
    "    y=y_tr_indexed\n",
    ")\n",
    "\n",
    "# Convert to dictionary for Keras\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "\n",
    "print(\"âœ… Class weights computed:\")\n",
    "print(f\"   Class distribution in training data:\")\n",
    "for i in range(5):\n",
    "    count = np.sum(y_tr_indexed == i)\n",
    "    percentage = count / len(y_tr_indexed) * 100\n",
    "    print(f\"      {i+1} star: {count:5d} samples ({percentage:5.2f}%) â†’ weight: {class_weights[i]:.3f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Class weights will be used during training to handle imbalance\")\n",
    "print(f\"   Minority classes get higher weights â†’ model pays more attention to them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acfe245",
   "metadata": {},
   "source": [
    "### ğŸ¯ Key Improvements Implemented\n",
    "\n",
    "This notebook now addresses the critical issues from ChatGPT analysis:\n",
    "\n",
    "**âœ… Problem 5 & 7: RNN gets real sequence + Title preserved**\n",
    "- Input changed from `(1024,)` to `(6, 1024)` - true sequence!\n",
    "- TITLE + CHUNKS approach: `[title, sent1, sent2, sent3, sent4, sent5]`\n",
    "- Masking layer handles variable-length sequences\n",
    "- RNN can now learn temporal patterns\n",
    "\n",
    "**âœ… Problem 4: Early stopping on val_loss**\n",
    "- Changed from monitoring `val_accuracy` to `val_loss` (mode='min')\n",
    "- Better overfitting prevention\n",
    "- Should reduce high val_loss (was 0.6-0.8)\n",
    "\n",
    "**âœ… Problem 3: Class imbalance handled**\n",
    "- Computed balanced class weights using sklearn\n",
    "- Applied `class_weight` parameter in `model.fit()`\n",
    "- Minority classes (1-2 stars) get higher attention\n",
    "\n",
    "**âœ… Problem 1: Long reviews handled**\n",
    "- Chunking automatically handles reviews > 256 tokens\n",
    "- Each chunk max 256 tokens (BGE-M3 limit)\n",
    "- No information loss\n",
    "\n",
    "**Expected improvements:**\n",
    "- Val accuracy >> 76% (previous plateau)\n",
    "- Lower val_loss (< 0.6)\n",
    "- Better minority class performance (1-2 stars)\n",
    "- More meaningful hyperparameter differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51559e",
   "metadata": {},
   "source": [
    "## 3) Modelling RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b1fcf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"LSTM_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,336</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ masking (\u001b[38;5;33mMasking\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m590,336\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_hidden (\u001b[38;5;33mDense\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              â”‚           \u001b[38;5;34m325\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">598,917</span> (2.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m598,917\u001b[0m (2.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">598,917</span> (2.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m598,917\u001b[0m (2.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… RNN model builder ready (TITLE + CHUNKS architecture)\n"
     ]
    }
   ],
   "source": [
    "# RNN Model Builder Function\n",
    "def build_rnn_model(\n",
    "    rnn_type='LSTM',           # 'LSTM', 'GRU', 'BiLSTM', 'BiGRU'\n",
    "    rnn_units=128,             # Number of units in RNN layer\n",
    "    activation='relu',         # Activation for dense layers\n",
    "    dropout=0.2,               # Dropout rate\n",
    "    use_batch_norm=False       # Whether to use batch normalization\n",
    "):\n",
    "    \"\"\"\n",
    "    Build RNN model for 5-class sentiment classification from BGE-M3 chunked embeddings.\n",
    "    \n",
    "    Input: (MAX_CHUNKS_TOTAL, EMBED_DIM) - Sequence of BGE-M3 embeddings [title, sent1, ..., sentN]\n",
    "    Output: (5,) - Probabilities for classes 0-4 (1-5 stars)\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential(name=f\"{rnn_type}_model\")\n",
    "    \n",
    "    # Input: Chunked BGE-M3 embeddings (MAX_CHUNKS_TOTAL, EMBED_DIM)\n",
    "    model.add(layers.Input(shape=(MAX_CHUNKS_TOTAL, EMBED_DIM), name='embedding_input'))\n",
    "    \n",
    "    # Masking layer to ignore zero-padded chunks\n",
    "    model.add(layers.Masking(mask_value=0.0, name='masking'))\n",
    "    \n",
    "    # RNN Layer (using unroll=True to avoid cuDNN and ensure compatibility)\n",
    "    if rnn_type == 'LSTM':\n",
    "        model.add(layers.LSTM(rnn_units, unroll=True, name='lstm_layer'))\n",
    "    elif rnn_type == 'GRU':\n",
    "        model.add(layers.GRU(rnn_units, unroll=True, name='gru_layer'))\n",
    "    elif rnn_type == 'BiLSTM':\n",
    "        model.add(layers.Bidirectional(layers.LSTM(rnn_units, unroll=True), name='bilstm_layer'))\n",
    "    elif rnn_type == 'BiGRU':\n",
    "        model.add(layers.Bidirectional(layers.GRU(rnn_units, unroll=True), name='bigru_layer'))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown rnn_type: {rnn_type}\")\n",
    "    \n",
    "    # Dropout\n",
    "    model.add(layers.Dropout(dropout, name='dropout'))\n",
    "    \n",
    "    # Batch Normalization (optional)\n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization(name='batch_norm'))\n",
    "    \n",
    "    # Dense layer\n",
    "    model.add(layers.Dense(64, activation=activation, name='dense_hidden'))\n",
    "    \n",
    "    # Output layer: 5 classes (0-4 for 1-5 stars)\n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test the model builder\n",
    "test_model = build_rnn_model()\n",
    "test_model.summary()\n",
    "print(\"\\nâœ… RNN model builder ready (TITLE + CHUNKS architecture)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c08824",
   "metadata": {},
   "source": [
    "### 3.1 Hyperparameter Tuning\n",
    "\n",
    "We'll systematically test different RNN configurations to find the optimal hyperparameters:\n",
    "- **RNN Type**: LSTM, GRU, BiLSTM, BiGRU\n",
    "- **Optimizers**: Adam, RMSprop, SGD\n",
    "- **Activations**: relu, tanh, selu\n",
    "- **Dropout**: 0.2, 0.3333\n",
    "- **Batch Normalization**: True, False\n",
    "\n",
    "For each configuration, we'll track:\n",
    "- Training accuracy\n",
    "- Validation accuracy\n",
    "- Training time\n",
    "- Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bce6964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Custom W&B callback ready (BestMetricsCallback)\n",
      "   Tracks: epoch/best_val_loss, epoch/best_val_accuracy\n",
      "   Plots will show best metrics over epochs\n"
     ]
    }
   ],
   "source": [
    "# Custom callback to track best validation metrics for W&B\n",
    "class BestMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Custom callback to track and log best validation metrics to W&B.\n",
    "    Tracks best_val_loss and best_val_accuracy throughout training.\n",
    "    Logs to 'epoch/best_val_loss' and 'epoch/best_val_accuracy' for proper plotting.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_val_acc = 0.0\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Define custom metrics in W&B with epoch as x-axis\n",
    "        if wandb.run is not None:\n",
    "            wandb.define_metric(\"epoch/best_val_loss\", step_metric=\"epoch/epoch\")\n",
    "            wandb.define_metric(\"epoch/best_val_accuracy\", step_metric=\"epoch/epoch\")\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            return\n",
    "        \n",
    "        # Update best validation loss\n",
    "        current_val_loss = logs.get('val_loss')\n",
    "        if current_val_loss is not None and current_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = current_val_loss\n",
    "        \n",
    "        # Update best validation accuracy\n",
    "        current_val_acc = logs.get('val_accuracy')\n",
    "        if current_val_acc is not None and current_val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = current_val_acc\n",
    "        \n",
    "        # Log to W&B under 'epoch/' namespace to match other metrics\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                'epoch/best_val_loss': self.best_val_loss,\n",
    "                'epoch/best_val_accuracy': self.best_val_acc\n",
    "            })\n",
    "\n",
    "print(\"âœ… Custom W&B callback ready (BestMetricsCallback)\")\n",
    "print(\"   Tracks: epoch/best_val_loss, epoch/best_val_accuracy\")\n",
    "print(\"   Plots will show best metrics over epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1ead6",
   "metadata": {},
   "source": [
    "### ğŸ§ª Quick Architecture Test (1-2 configs)\n",
    "\n",
    "Before running full hyperparameter tuning, let's verify the new TITLE + CHUNKS architecture works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# QUICK TEST: Verify TITLE + CHUNKS architecture works\n",
    "# Test 2 configs: BiGRU with relu and tanh\n",
    "# ====================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ§ª QUICK ARCHITECTURE TEST\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing 2 configurations to verify new architecture works correctly\")\n",
    "print(\"If these pass without errors, we can proceed with full tuning\\n\")\n",
    "\n",
    "# Test configs\n",
    "test_configs = [\n",
    "    {'rnn_type': 'BiGRU', 'activation': 'relu', 'dropout': 0.2, 'rnn_units': 128},\n",
    "    {'rnn_type': 'BiGRU', 'activation': 'tanh', 'dropout': 0.2, 'rnn_units': 128}\n",
    "]\n",
    "\n",
    "quick_test_results = []\n",
    "\n",
    "for i, config in enumerate(test_configs, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Test {i}/{len(test_configs)}: {config['rnn_type']} | {config['activation']} | units={config['rnn_units']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Initialize W&B run\n",
    "    run_name = f\"quicktest_{config['rnn_type']}_{config['activation']}\"\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        name=run_name,\n",
    "        config=config,\n",
    "        reinit=True,\n",
    "        tags=['quick_test', 'architecture_verification']\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    test_model = build_rnn_model(\n",
    "        rnn_type=config['rnn_type'],\n",
    "        rnn_units=config['rnn_units'],\n",
    "        activation=config['activation'],\n",
    "        dropout=config['dropout'],\n",
    "        use_batch_norm=True\n",
    "    )\n",
    "    \n",
    "    # Compile\n",
    "    test_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        mode='min',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # W&B callbacks\n",
    "    wandb_metrics_cb = WandbMetricsLogger()\n",
    "    best_metrics_cb = BestMetricsCallback()\n",
    "    \n",
    "    # Train for just 5 epochs to verify it works\n",
    "    print(f\"\\nTraining for 5 epochs (quick test)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = test_model.fit(\n",
    "        train_ds_chunked,\n",
    "        validation_data=val_ds_chunked,\n",
    "        epochs=30,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[early_stop, wandb_metrics_cb, best_metrics_cb],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Get metrics\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    # Log final summary to W&B\n",
    "    wandb.log({\n",
    "        'final_train_acc': final_train_acc,\n",
    "        'final_val_acc': final_val_acc,\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss,\n",
    "        'training_time_sec': train_time\n",
    "    })\n",
    "    \n",
    "    quick_test_results.append({\n",
    "        'config': f\"{config['rnn_type']}-{config['activation']}\",\n",
    "        'train_acc': final_train_acc,\n",
    "        'val_acc': final_val_acc,\n",
    "        'train_loss': final_train_loss,\n",
    "        'val_loss': final_val_loss,\n",
    "        'time_sec': train_time\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nâœ… Test {i} completed:\")\n",
    "    print(f\"   Train Acc: {final_train_acc:.4f} | Val Acc: {final_val_acc:.4f}\")\n",
    "    print(f\"   Train Loss: {final_train_loss:.4f} | Val Loss: {final_val_loss:.4f}\")\n",
    "    print(f\"   Time: {train_time:.1f}s\")\n",
    "    \n",
    "    # Finish W&B run\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Clear session\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ QUICK TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_df = pd.DataFrame(quick_test_results)\n",
    "print(test_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nâœ… All tests passed! Architecture is working correctly.\")\n",
    "print(\"ğŸ’¡ Key observations:\")\n",
    "print(f\"   - Input shape (6, 1024) works âœ“\")\n",
    "print(f\"   - Masking layer handles variable sequences âœ“\")\n",
    "print(f\"   - Class weights applied correctly âœ“\")\n",
    "print(f\"   - Early stopping on val_loss works âœ“\")\n",
    "print(f\"   - Val accuracy > 0.0 (model is learning) âœ“\")\n",
    "print(\"\\nğŸš€ Ready for full hyperparameter tuning!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# HYPERPARAMETER SEARCH - PART 1: LSTM\n",
    "# 3 optimizers Ã— 3 activations Ã— 2 dropouts Ã— 3 units = 54 configs\n",
    "# Estimated time: ~20-30 minutes\n",
    "# ====================================================================\n",
    "import uuid\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” PART 1/4: LSTM HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define hyperparameter grid for LSTM\n",
    "rnn_types = ['LSTM']                             # Only LSTM\n",
    "optimizers = ['adam', 'adamw', 'sgd']            # All optimizers\n",
    "activations = ['relu', 'tanh', 'selu']           # All activations from course\n",
    "dropouts = [0.2, 0.3333]                         # Two dropout rates\n",
    "batch_norms = [True]                             # Always use batch normalization\n",
    "rnn_units_list = [64, 128, 256]                  # Test different unit sizes\n",
    "learning_rates = [1e-3]                          # Standard learning rate\n",
    "momentums = [0.95]                               # Momentum for SGD only (not Adam/AdamW)\n",
    "\n",
    "# Training configuration\n",
    "TUNING_EPOCHS = 50  # Epochs per config (with early stopping if no improvement)\n",
    "EARLY_STOP_PATIENCE = 6  # Stop if val_loss doesn't improve for 6 epochs\n",
    "\n",
    "# Results storage for LSTM\n",
    "results_lstm = []\n",
    "best_val_acc_lstm = 0\n",
    "best_config_lstm = None\n",
    "\n",
    "total_configs = len(rnn_types) * len(optimizers) * len(activations) * len(dropouts) * len(batch_norms) * len(rnn_units_list) * len(learning_rates) * len(momentums)\n",
    "print(f\"\\nğŸ“Š Testing {total_configs} LSTM configurations...\")\n",
    "print(f\"â±ï¸  Estimated time: ~{total_configs * 2} minutes (assuming ~2 min per config)\\n\")\n",
    "print(f\"â° Note: Each config runs max {TUNING_EPOCHS} epochs with early stopping (patience={EARLY_STOP_PATIENCE})\\n\")\n",
    "\n",
    "config_num = 0\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "    for optimizer in optimizers:\n",
    "        for activation in activations:\n",
    "            for dropout in dropouts:\n",
    "                for batch_norm in batch_norms:\n",
    "                    for rnn_units in rnn_units_list:\n",
    "                        for lr in learning_rates:\n",
    "                            for momentum in momentums:\n",
    "                                config_num += 1\n",
    "                                \n",
    "                                print(f\"\\n{'='*70}\")\n",
    "                                print(f\"ğŸ”„ Progress: {config_num}/{total_configs} configurations\")\n",
    "                                print(f\"Config: {rnn_type} | {optimizer} | lr={lr} | mom={momentum} | {activation} | dropout={dropout} | units={rnn_units}\")\n",
    "                                print(f\"{'='*70}\")\n",
    "                                \n",
    "                                # Generate unique run ID\n",
    "                                RUN_ID = uuid.uuid4().hex[:8]\n",
    "                                RUN_NAME = f\"RNN-CHUNKED-{rnn_type}-{optimizer}-seed{SEED}-{RUN_ID}\"\n",
    "                                BASE_DIR = os.path.join(\"progress\", RUN_NAME)\n",
    "                                os.makedirs(BASE_DIR, exist_ok=True)\n",
    "                                \n",
    "                                # Initialize W&B run\n",
    "                                run = wandb.init(\n",
    "                                    project=WANDB_PROJECT,\n",
    "                                    entity=WANDB_ENTITY,\n",
    "                                    name=RUN_NAME,\n",
    "                                    dir=BASE_DIR,\n",
    "                                    config={\n",
    "                                        \"architecture\": \"TITLE_CHUNKS\",\n",
    "                                        \"input_shape\": f\"({MAX_CHUNKS_TOTAL}, {EMBED_DIM})\",\n",
    "                                        \"rnn_type\": rnn_type,\n",
    "                                        \"optimizer\": optimizer,\n",
    "                                        \"learning_rate\": lr,\n",
    "                                        \"momentum\": momentum if optimizer == 'sgd' else None,\n",
    "                                        \"activation\": activation,\n",
    "                                        \"dropout\": dropout,\n",
    "                                        \"batch_norm\": batch_norm,\n",
    "                                        \"rnn_units\": rnn_units,\n",
    "                                        \"max_epochs\": TUNING_EPOCHS,\n",
    "                                        \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "                                        \"early_stop_monitor\": \"val_loss\",\n",
    "                                        \"batch_size\": BATCH,\n",
    "                                        \"seed\": SEED,\n",
    "                                        \"run_id\": RUN_ID,\n",
    "                                        \"config_number\": config_num,\n",
    "                                        \"total_configs\": total_configs,\n",
    "                                        \"class_weights\": \"balanced\",\n",
    "                                    },\n",
    "                                )\n",
    "                                \n",
    "                                # Build model\n",
    "                                model = build_rnn_model(\n",
    "                                    rnn_type=rnn_type,\n",
    "                                    rnn_units=rnn_units,\n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout,\n",
    "                                    use_batch_norm=batch_norm\n",
    "                                )\n",
    "                                \n",
    "                                # Compile model with specific learning rate and momentum\n",
    "                                if optimizer == 'adam':\n",
    "                                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                                elif optimizer == 'adamw':\n",
    "                                    opt = tf.keras.optimizers.AdamW(learning_rate=lr)\n",
    "                                elif optimizer == 'sgd':\n",
    "                                    opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "                                else:\n",
    "                                    opt = optimizer  # Fallback\n",
    "                                \n",
    "                                model.compile(\n",
    "                                    optimizer=opt,\n",
    "                                    loss='sparse_categorical_crossentropy',\n",
    "                                    metrics=['accuracy']\n",
    "                                )\n",
    "                                \n",
    "                                # Early stopping callback - MONITORS val_loss (mode='min')\n",
    "                                early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss',\n",
    "                                    patience=EARLY_STOP_PATIENCE,\n",
    "                                    mode='min',\n",
    "                                    restore_best_weights=True,\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                \n",
    "                                # W&B callbacks\n",
    "                                wandb_callback = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "                                best_metrics_cb = BestMetricsCallback()\n",
    "                                \n",
    "                                # Train model with class weights\n",
    "                                start_time = time.time()\n",
    "                                history = model.fit(\n",
    "                                    train_ds_chunked,\n",
    "                                    validation_data=val_ds_chunked,\n",
    "                                    epochs=TUNING_EPOCHS,\n",
    "                                    class_weight=class_weights,\n",
    "                                    callbacks=[early_stop, wandb_callback, best_metrics_cb],\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                train_time = time.time() - start_time\n",
    "                                \n",
    "                                # Get final metrics\n",
    "                                train_acc = history.history['accuracy'][-1]\n",
    "                                val_acc = history.history['val_accuracy'][-1]\n",
    "                                train_loss = history.history['loss'][-1]\n",
    "                                val_loss = history.history['val_loss'][-1]\n",
    "                                epochs_trained = len(history.history['accuracy'])\n",
    "                                \n",
    "                                # Get predictions for confusion matrix\n",
    "                                y_val_pred = model.predict(val_ds_chunked, verbose=0)\n",
    "                                y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "                                \n",
    "                                # Confusion matrix\n",
    "                                cm = confusion_matrix(y_val_indexed, y_val_pred_labels)\n",
    "                                \n",
    "                                # Store results\n",
    "                                results_lstm.append({\n",
    "                                    'rnn_type': rnn_type,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                    'activation': activation,\n",
    "                                    'dropout': dropout,\n",
    "                                    'batch_norm': batch_norm,\n",
    "                                    'rnn_units': rnn_units,\n",
    "                                    'train_acc': train_acc,\n",
    "                                    'val_acc': val_acc,\n",
    "                                    'train_loss': train_loss,\n",
    "                                    'val_loss': val_loss,\n",
    "                                    'epochs': epochs_trained,\n",
    "                                    'time_sec': train_time,\n",
    "                                    'confusion_matrix': cm\n",
    "                                })\n",
    "                                \n",
    "                                # Print results\n",
    "                                print(f\"âœ… Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "                                print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "                                print(f\"   Epochs: {epochs_trained}/{TUNING_EPOCHS} | Time: {train_time:.1f}s\")\n",
    "                                \n",
    "                                # Track best model\n",
    "                                if val_acc > best_val_acc_lstm:\n",
    "                                    best_val_acc_lstm = val_acc\n",
    "                                    best_config_lstm = {\n",
    "                                        'rnn_type': rnn_type,\n",
    "                                        'optimizer': optimizer,\n",
    "                                        'learning_rate': lr,\n",
    "                                        'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                        'activation': activation,\n",
    "                                        'dropout': dropout,\n",
    "                                        'batch_norm': batch_norm,\n",
    "                                        'rnn_units': rnn_units\n",
    "                                    }\n",
    "                                    print(f\"   ğŸŒŸ NEW BEST MODEL! Val Acc: {val_acc:.4f}\")\n",
    "                                \n",
    "                                # Log final metrics to W&B\n",
    "                                wandb.run.summary[\"best_val_acc\"] = val_acc\n",
    "                                wandb.run.summary[\"best_epoch\"] = epochs_trained\n",
    "                                wandb.run.summary[\"final_train_acc\"] = train_acc\n",
    "                                wandb.run.summary[\"final_train_loss\"] = train_loss\n",
    "                                wandb.run.summary[\"final_val_loss\"] = val_loss\n",
    "                                wandb.run.summary[\"training_time_sec\"] = train_time\n",
    "                                \n",
    "                                # Finish W&B run\n",
    "                                wandb.finish()\n",
    "                                \n",
    "                                # Clear session to free memory\n",
    "                                tf.keras.backend.clear_session()\n",
    "                                \n",
    "                                # Aggressive memory cleanup to prevent crashes\n",
    "                                import gc\n",
    "                                gc.collect()\n",
    "                                \n",
    "                                # Also clear GPU memory if using GPU\n",
    "                                try:\n",
    "                                    import torch\n",
    "                                    if torch.cuda.is_available():\n",
    "                                        torch.cuda.empty_cache()\n",
    "                                except:\n",
    "                                    pass\n",
    "                                \n",
    "                                # Save progress every 10 configs\n",
    "                                if config_num % 10 == 0:\n",
    "                                    temp_df = pd.DataFrame(results_lstm)\n",
    "                                    temp_df.drop(columns=['confusion_matrix']).to_csv('rnn_chunked_LSTM_partial.csv', index=False)\n",
    "                                    print(f\"   ğŸ’¾ Progress saved ({config_num}/{total_configs})\")\n",
    "\n",
    "# Create results DataFrame for LSTM\n",
    "results_lstm_df = pd.DataFrame(results_lstm)\n",
    "\n",
    "# Save final results\n",
    "results_lstm_csv = results_lstm_df.drop(columns=['confusion_matrix'])\n",
    "results_lstm_csv.to_csv('rnn_chunked_LSTM_FULL.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… LSTM HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(f\"\\nğŸ† BEST LSTM CONFIGURATION (Val Acc: {best_val_acc_lstm:.4f}):\")\n",
    "for key, value in best_config_lstm.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Results saved to 'rnn_chunked_LSTM_FULL.csv'\")\n",
    "print(f\"   Total LSTM configurations tested: {len(results_lstm)}\")\n",
    "print(f\"   Best validation accuracy: {results_lstm_csv['val_acc'].max():.4f}\")\n",
    "print(f\"   Mean validation accuracy: {results_lstm_csv['val_acc'].mean():.4f}\")\n",
    "print(f\"   Std validation accuracy: {results_lstm_csv['val_acc'].std():.4f}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af02a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# HYPERPARAMETER SEARCH - PART 2: GRU\n",
    "# 3 optimizers Ã— 3 activations Ã— 2 dropouts Ã— 3 units = 54 configs\n",
    "# Estimated time: ~20-30 minutes\n",
    "# ====================================================================\n",
    "import uuid\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” PART 2/4: GRU HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define hyperparameter grid for GRU\n",
    "rnn_types = ['GRU']                              # Only GRU\n",
    "optimizers = ['adam', 'adamw', 'sgd']            # All optimizers\n",
    "activations = ['relu', 'tanh', 'selu']           # All activations from course\n",
    "dropouts = [0.2, 0.3333]                         # Two dropout rates\n",
    "batch_norms = [True]                             # Always use batch normalization\n",
    "rnn_units_list = [64, 128, 256]                  # Test different unit sizes\n",
    "learning_rates = [1e-3]                          # Standard learning rate\n",
    "momentums = [0.95]                               # Momentum for SGD only (not Adam/AdamW)\n",
    "\n",
    "# Training configuration\n",
    "TUNING_EPOCHS = 50  # Epochs per config (with early stopping if no improvement)\n",
    "EARLY_STOP_PATIENCE = 6  # Stop if val_loss doesn't improve for 6 epochs\n",
    "\n",
    "# Results storage for GRU\n",
    "results_gru = []\n",
    "best_val_acc_gru = 0\n",
    "best_config_gru = None\n",
    "\n",
    "total_configs = len(rnn_types) * len(optimizers) * len(activations) * len(dropouts) * len(batch_norms) * len(rnn_units_list) * len(learning_rates) * len(momentums)\n",
    "print(f\"\\nğŸ“Š Testing {total_configs} GRU configurations...\")\n",
    "print(f\"â±ï¸  Estimated time: ~{total_configs * 2} minutes (assuming ~2 min per config)\\n\")\n",
    "print(f\"â° Note: Each config runs max {TUNING_EPOCHS} epochs with early stopping (patience={EARLY_STOP_PATIENCE})\\n\")\n",
    "\n",
    "config_num = 0\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "    for optimizer in optimizers:\n",
    "        for activation in activations:\n",
    "            for dropout in dropouts:\n",
    "                for batch_norm in batch_norms:\n",
    "                    for rnn_units in rnn_units_list:\n",
    "                        for lr in learning_rates:\n",
    "                            for momentum in momentums:\n",
    "                                config_num += 1\n",
    "                                \n",
    "                                print(f\"\\n{'='*70}\")\n",
    "                                print(f\"ğŸ”„ Progress: {config_num}/{total_configs} configurations\")\n",
    "                                print(f\"Config: {rnn_type} | {optimizer} | lr={lr} | mom={momentum} | {activation} | dropout={dropout} | units={rnn_units}\")\n",
    "                                print(f\"{'='*70}\")\n",
    "                                \n",
    "                                # Generate unique run ID\n",
    "                                RUN_ID = uuid.uuid4().hex[:8]\n",
    "                                RUN_NAME = f\"RNN-CHUNKED-{rnn_type}-{optimizer}-seed{SEED}-{RUN_ID}\"\n",
    "                                BASE_DIR = os.path.join(\"progress\", RUN_NAME)\n",
    "                                os.makedirs(BASE_DIR, exist_ok=True)\n",
    "                                \n",
    "                                # Initialize W&B run\n",
    "                                run = wandb.init(\n",
    "                                    project=WANDB_PROJECT,\n",
    "                                    entity=WANDB_ENTITY,\n",
    "                                    name=RUN_NAME,\n",
    "                                    dir=BASE_DIR,\n",
    "                                    config={\n",
    "                                        \"architecture\": \"TITLE_CHUNKS\",\n",
    "                                        \"input_shape\": f\"({MAX_CHUNKS_TOTAL}, {EMBED_DIM})\",\n",
    "                                        \"rnn_type\": rnn_type,\n",
    "                                        \"optimizer\": optimizer,\n",
    "                                        \"learning_rate\": lr,\n",
    "                                        \"momentum\": momentum if optimizer == 'sgd' else None,\n",
    "                                        \"activation\": activation,\n",
    "                                        \"dropout\": dropout,\n",
    "                                        \"batch_norm\": batch_norm,\n",
    "                                        \"rnn_units\": rnn_units,\n",
    "                                        \"max_epochs\": TUNING_EPOCHS,\n",
    "                                        \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "                                        \"early_stop_monitor\": \"val_loss\",\n",
    "                                        \"batch_size\": BATCH,\n",
    "                                        \"seed\": SEED,\n",
    "                                        \"run_id\": RUN_ID,\n",
    "                                        \"config_number\": config_num,\n",
    "                                        \"total_configs\": total_configs,\n",
    "                                        \"class_weights\": \"balanced\",\n",
    "                                    },\n",
    "                                )\n",
    "                                \n",
    "                                # Build model\n",
    "                                model = build_rnn_model(\n",
    "                                    rnn_type=rnn_type,\n",
    "                                    rnn_units=rnn_units,\n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout,\n",
    "                                    use_batch_norm=batch_norm\n",
    "                                )\n",
    "                                \n",
    "                                # Compile model with specific learning rate and momentum\n",
    "                                if optimizer == 'adam':\n",
    "                                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                                elif optimizer == 'adamw':\n",
    "                                    opt = tf.keras.optimizers.AdamW(learning_rate=lr)\n",
    "                                elif optimizer == 'sgd':\n",
    "                                    opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "                                else:\n",
    "                                    opt = optimizer  # Fallback\n",
    "                                \n",
    "                                model.compile(\n",
    "                                    optimizer=opt,\n",
    "                                    loss='sparse_categorical_crossentropy',\n",
    "                                    metrics=['accuracy']\n",
    "                                )\n",
    "                                \n",
    "                                # Early stopping callback - MONITORS val_loss (mode='min')\n",
    "                                early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss',\n",
    "                                    patience=EARLY_STOP_PATIENCE,\n",
    "                                    mode='min',\n",
    "                                    restore_best_weights=True,\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                \n",
    "                                # W&B callbacks\n",
    "                                wandb_callback = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "                                best_metrics_cb = BestMetricsCallback()\n",
    "                                \n",
    "                                # Train model with class weights\n",
    "                                start_time = time.time()\n",
    "                                history = model.fit(\n",
    "                                    train_ds_chunked,\n",
    "                                    validation_data=val_ds_chunked,\n",
    "                                    epochs=TUNING_EPOCHS,\n",
    "                                    class_weight=class_weights,\n",
    "                                    callbacks=[early_stop, wandb_callback, best_metrics_cb],\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                train_time = time.time() - start_time\n",
    "                                \n",
    "                                # Get final metrics\n",
    "                                train_acc = history.history['accuracy'][-1]\n",
    "                                val_acc = history.history['val_accuracy'][-1]\n",
    "                                train_loss = history.history['loss'][-1]\n",
    "                                val_loss = history.history['val_loss'][-1]\n",
    "                                epochs_trained = len(history.history['accuracy'])\n",
    "                                \n",
    "                                # Get predictions for confusion matrix\n",
    "                                y_val_pred = model.predict(val_ds_chunked, verbose=0)\n",
    "                                y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "                                \n",
    "                                # Confusion matrix\n",
    "                                cm = confusion_matrix(y_val_indexed, y_val_pred_labels)\n",
    "                                \n",
    "                                # Store results\n",
    "                                results_gru.append({\n",
    "                                    'rnn_type': rnn_type,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                    'activation': activation,\n",
    "                                    'dropout': dropout,\n",
    "                                    'batch_norm': batch_norm,\n",
    "                                    'rnn_units': rnn_units,\n",
    "                                    'train_acc': train_acc,\n",
    "                                    'val_acc': val_acc,\n",
    "                                    'train_loss': train_loss,\n",
    "                                    'val_loss': val_loss,\n",
    "                                    'epochs': epochs_trained,\n",
    "                                    'time_sec': train_time,\n",
    "                                    'confusion_matrix': cm\n",
    "                                })\n",
    "                                \n",
    "                                # Print results\n",
    "                                print(f\"âœ… Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "                                print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "                                print(f\"   Epochs: {epochs_trained}/{TUNING_EPOCHS} | Time: {train_time:.1f}s\")\n",
    "                                \n",
    "                                # Track best model\n",
    "                                if val_acc > best_val_acc_gru:\n",
    "                                    best_val_acc_gru = val_acc\n",
    "                                    best_config_gru = {\n",
    "                                        'rnn_type': rnn_type,\n",
    "                                        'optimizer': optimizer,\n",
    "                                        'learning_rate': lr,\n",
    "                                        'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                        'activation': activation,\n",
    "                                        'dropout': dropout,\n",
    "                                        'batch_norm': batch_norm,\n",
    "                                        'rnn_units': rnn_units\n",
    "                                    }\n",
    "                                    print(f\"   ğŸŒŸ NEW BEST MODEL! Val Acc: {val_acc:.4f}\")\n",
    "                                \n",
    "                                # Log final metrics to W&B\n",
    "                                wandb.run.summary[\"best_val_acc\"] = val_acc\n",
    "                                wandb.run.summary[\"best_epoch\"] = epochs_trained\n",
    "                                wandb.run.summary[\"final_train_acc\"] = train_acc\n",
    "                                wandb.run.summary[\"final_train_loss\"] = train_loss\n",
    "                                wandb.run.summary[\"final_val_loss\"] = val_loss\n",
    "                                wandb.run.summary[\"training_time_sec\"] = train_time\n",
    "                                \n",
    "                                # Finish W&B run\n",
    "                                wandb.finish()\n",
    "                                \n",
    "                                # Clear session to free memory\n",
    "                                tf.keras.backend.clear_session()\n",
    "                                \n",
    "                                # Aggressive memory cleanup to prevent crashes\n",
    "                                import gc\n",
    "                                gc.collect()\n",
    "                                \n",
    "                                # Also clear GPU memory if using GPU\n",
    "                                try:\n",
    "                                    import torch\n",
    "                                    if torch.cuda.is_available():\n",
    "                                        torch.cuda.empty_cache()\n",
    "                                except:\n",
    "                                    pass\n",
    "                                \n",
    "                                # Save progress every 10 configs\n",
    "                                if config_num % 10 == 0:\n",
    "                                    temp_df = pd.DataFrame(results_gru)\n",
    "                                    temp_df.drop(columns=['confusion_matrix']).to_csv('rnn_chunked_GRU_partial.csv', index=False)\n",
    "                                    print(f\"   ğŸ’¾ Progress saved ({config_num}/{total_configs})\")\n",
    "\n",
    "# Create results DataFrame for GRU\n",
    "results_gru_df = pd.DataFrame(results_gru)\n",
    "\n",
    "# Save final results\n",
    "results_gru_csv = results_gru_df.drop(columns=['confusion_matrix'])\n",
    "results_gru_csv.to_csv('rnn_chunked_GRU_FULL.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… GRU HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(f\"\\nğŸ† BEST GRU CONFIGURATION (Val Acc: {best_val_acc_gru:.4f}):\")\n",
    "for key, value in best_config_gru.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Results saved to 'rnn_chunked_GRU_FULL.csv'\")\n",
    "print(f\"   Total GRU configurations tested: {len(results_gru)}\")\n",
    "print(f\"   Best validation accuracy: {results_gru_csv['val_acc'].max():.4f}\")\n",
    "print(f\"   Mean validation accuracy: {results_gru_csv['val_acc'].mean():.4f}\")\n",
    "print(f\"   Std validation accuracy: {results_gru_csv['val_acc'].std():.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# TOP 5 GRU RESULTATER\n",
    "# ====================================================================\n",
    "import pandas as pd\n",
    "\n",
    "# Load GRU results from saved CSV\n",
    "df_gru = pd.read_csv('rnn_chunked_GRU_FULL.csv')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ† TOP 5 GRU KONFIGURATIONER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sort by validation accuracy and show top 5\n",
    "top5 = df_gru.nlargest(5, 'val_acc')\n",
    "\n",
    "for idx, (i, row) in enumerate(top5.iterrows(), 1):\n",
    "    print(f\"\\n#{idx} - Val Acc: {row['val_acc']:.4f} ({row['val_acc']*100:.2f}%)\")\n",
    "    print(f\"   Optimizer: {row['optimizer']}\")\n",
    "    print(f\"   Activation: {row['activation']}\")\n",
    "    print(f\"   Dropout: {row['dropout']}\")\n",
    "    print(f\"   RNN Units: {row['rnn_units']}\")\n",
    "    print(f\"   Epochs: {row['epochs']}\")\n",
    "    print(f\"   Train Acc: {row['train_acc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ğŸ“Š Samlet statistik for alle {len(df_gru)} GRU konfigurationer:\")\n",
    "print(f\"   Best:  {df_gru['val_acc'].max():.4f} ({df_gru['val_acc'].max()*100:.2f}%)\")\n",
    "print(f\"   Mean:  {df_gru['val_acc'].mean():.4f} ({df_gru['val_acc'].mean()*100:.2f}%)\")\n",
    "print(f\"   Std:   {df_gru['val_acc'].std():.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07049e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# HYPERPARAMETER SEARCH - PART 4: BiGRU\n",
    "# 3 optimizers Ã— 3 activations Ã— 2 dropouts Ã— 3 units = 54 configs\n",
    "# Estimated time: ~20-30 minutes\n",
    "# ====================================================================\n",
    "import uuid\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” PART 4/4: BiGRU HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define hyperparameter grid for BiGRU\n",
    "rnn_types = ['BiGRU']                            # Only BiGRU\n",
    "optimizers = ['adam', 'adamw', 'sgd']            # All optimizers\n",
    "activations = ['relu', 'tanh', 'selu']           # All activations from course\n",
    "dropouts = [0.2, 0.3333]                         # Two dropout rates\n",
    "batch_norms = [True]                             # Always use batch normalization\n",
    "rnn_units_list = [64, 128, 256]                  # Test different unit sizes\n",
    "learning_rates = [1e-3]                          # Standard learning rate\n",
    "momentums = [0.95]                               # Momentum for SGD only (not Adam/AdamW)\n",
    "\n",
    "# Training configuration\n",
    "TUNING_EPOCHS = 50  # Epochs per config (with early stopping if no improvement)\n",
    "EARLY_STOP_PATIENCE = 6  # Stop if val_loss doesn't improve for 6 epochs\n",
    "\n",
    "# Results storage for BiGRU\n",
    "results_bigru = []\n",
    "best_val_acc_bigru = 0\n",
    "best_config_bigru = None\n",
    "\n",
    "total_configs = len(rnn_types) * len(optimizers) * len(activations) * len(dropouts) * len(batch_norms) * len(rnn_units_list) * len(learning_rates) * len(momentums)\n",
    "print(f\"\\nğŸ“Š Testing {total_configs} BiGRU configurations...\")\n",
    "print(f\"â±ï¸  Estimated time: ~{total_configs * 2} minutes (assuming ~2 min per config)\\n\")\n",
    "print(f\"â° Note: Each config runs max {TUNING_EPOCHS} epochs with early stopping (patience={EARLY_STOP_PATIENCE})\\n\")\n",
    "\n",
    "config_num = 0\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "    for optimizer in optimizers:\n",
    "        for activation in activations:\n",
    "            for dropout in dropouts:\n",
    "                for batch_norm in batch_norms:\n",
    "                    for rnn_units in rnn_units_list:\n",
    "                        for lr in learning_rates:\n",
    "                            for momentum in momentums:\n",
    "                                config_num += 1\n",
    "                                \n",
    "                                print(f\"\\n{'='*70}\")\n",
    "                                print(f\"ğŸ”„ Progress: {config_num}/{total_configs} configurations\")\n",
    "                                print(f\"Config: {rnn_type} | {optimizer} | lr={lr} | mom={momentum} | {activation} | dropout={dropout} | units={rnn_units}\")\n",
    "                                print(f\"{'='*70}\")\n",
    "                                \n",
    "                                # Generate unique run ID\n",
    "                                RUN_ID = uuid.uuid4().hex[:8]\n",
    "                                RUN_NAME = f\"RNN-CHUNKED-{rnn_type}-{optimizer}-seed{SEED}-{RUN_ID}\"\n",
    "                                BASE_DIR = os.path.join(\"progress\", RUN_NAME)\n",
    "                                os.makedirs(BASE_DIR, exist_ok=True)\n",
    "                                \n",
    "                                # Initialize W&B run\n",
    "                                run = wandb.init(\n",
    "                                    project=WANDB_PROJECT,\n",
    "                                    entity=WANDB_ENTITY,\n",
    "                                    name=RUN_NAME,\n",
    "                                    dir=BASE_DIR,\n",
    "                                    config={\n",
    "                                        \"architecture\": \"TITLE_CHUNKS\",\n",
    "                                        \"input_shape\": f\"({MAX_CHUNKS_TOTAL}, {EMBED_DIM})\",\n",
    "                                        \"rnn_type\": rnn_type,\n",
    "                                        \"optimizer\": optimizer,\n",
    "                                        \"learning_rate\": lr,\n",
    "                                        \"momentum\": momentum if optimizer == 'sgd' else None,\n",
    "                                        \"activation\": activation,\n",
    "                                        \"dropout\": dropout,\n",
    "                                        \"batch_norm\": batch_norm,\n",
    "                                        \"rnn_units\": rnn_units,\n",
    "                                        \"max_epochs\": TUNING_EPOCHS,\n",
    "                                        \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "                                        \"early_stop_monitor\": \"val_loss\",\n",
    "                                        \"batch_size\": BATCH,\n",
    "                                        \"seed\": SEED,\n",
    "                                        \"run_id\": RUN_ID,\n",
    "                                        \"config_number\": config_num,\n",
    "                                        \"total_configs\": total_configs,\n",
    "                                        \"class_weights\": \"balanced\",\n",
    "                                    },\n",
    "                                )\n",
    "                                \n",
    "                                # Build model\n",
    "                                model = build_rnn_model(\n",
    "                                    rnn_type=rnn_type,\n",
    "                                    rnn_units=rnn_units,\n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout,\n",
    "                                    use_batch_norm=batch_norm\n",
    "                                )\n",
    "                                \n",
    "                                # Compile model with specific learning rate and momentum\n",
    "                                if optimizer == 'adam':\n",
    "                                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                                elif optimizer == 'adamw':\n",
    "                                    opt = tf.keras.optimizers.AdamW(learning_rate=lr)\n",
    "                                elif optimizer == 'sgd':\n",
    "                                    opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "                                else:\n",
    "                                    opt = optimizer  # Fallback\n",
    "                                \n",
    "                                model.compile(\n",
    "                                    optimizer=opt,\n",
    "                                    loss='sparse_categorical_crossentropy',\n",
    "                                    metrics=['accuracy']\n",
    "                                )\n",
    "                                \n",
    "                                # Early stopping callback - MONITORS val_loss (mode='min')\n",
    "                                early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss',\n",
    "                                    patience=EARLY_STOP_PATIENCE,\n",
    "                                    mode='min',\n",
    "                                    restore_best_weights=True,\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                \n",
    "                                # W&B callbacks\n",
    "                                wandb_callback = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "                                best_metrics_cb = BestMetricsCallback()\n",
    "                                \n",
    "                                # Train model with class weights\n",
    "                                start_time = time.time()\n",
    "                                history = model.fit(\n",
    "                                    train_ds_chunked,\n",
    "                                    validation_data=val_ds_chunked,\n",
    "                                    epochs=TUNING_EPOCHS,\n",
    "                                    class_weight=class_weights,\n",
    "                                    callbacks=[early_stop, wandb_callback, best_metrics_cb],\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                train_time = time.time() - start_time\n",
    "                                \n",
    "                                # Get final metrics\n",
    "                                train_acc = history.history['accuracy'][-1]\n",
    "                                val_acc = history.history['val_accuracy'][-1]\n",
    "                                train_loss = history.history['loss'][-1]\n",
    "                                val_loss = history.history['val_loss'][-1]\n",
    "                                epochs_trained = len(history.history['accuracy'])\n",
    "                                \n",
    "                                # Get predictions for confusion matrix\n",
    "                                y_val_pred = model.predict(val_ds_chunked, verbose=0)\n",
    "                                y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "                                \n",
    "                                # Confusion matrix\n",
    "                                cm = confusion_matrix(y_val_indexed, y_val_pred_labels)\n",
    "                                \n",
    "                                # Store results\n",
    "                                results_bigru.append({\n",
    "                                    'rnn_type': rnn_type,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                    'activation': activation,\n",
    "                                    'dropout': dropout,\n",
    "                                    'batch_norm': batch_norm,\n",
    "                                    'rnn_units': rnn_units,\n",
    "                                    'train_acc': train_acc,\n",
    "                                    'val_acc': val_acc,\n",
    "                                    'train_loss': train_loss,\n",
    "                                    'val_loss': val_loss,\n",
    "                                    'epochs': epochs_trained,\n",
    "                                    'time_sec': train_time,\n",
    "                                    'confusion_matrix': cm\n",
    "                                })\n",
    "                                \n",
    "                                # Print results\n",
    "                                print(f\"âœ… Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "                                print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "                                print(f\"   Epochs: {epochs_trained}/{TUNING_EPOCHS} | Time: {train_time:.1f}s\")\n",
    "                                \n",
    "                                # Track best model\n",
    "                                if val_acc > best_val_acc_bigru:\n",
    "                                    best_val_acc_bigru = val_acc\n",
    "                                    best_config_bigru = {\n",
    "                                        'rnn_type': rnn_type,\n",
    "                                        'optimizer': optimizer,\n",
    "                                        'learning_rate': lr,\n",
    "                                        'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                        'activation': activation,\n",
    "                                        'dropout': dropout,\n",
    "                                        'batch_norm': batch_norm,\n",
    "                                        'rnn_units': rnn_units\n",
    "                                    }\n",
    "                                    print(f\"   ğŸŒŸ NEW BEST MODEL! Val Acc: {val_acc:.4f}\")\n",
    "                                \n",
    "                                # Log final metrics to W&B\n",
    "                                wandb.run.summary[\"best_val_acc\"] = val_acc\n",
    "                                wandb.run.summary[\"best_epoch\"] = epochs_trained\n",
    "                                wandb.run.summary[\"final_train_acc\"] = train_acc\n",
    "                                wandb.run.summary[\"final_train_loss\"] = train_loss\n",
    "                                wandb.run.summary[\"final_val_loss\"] = val_loss\n",
    "                                wandb.run.summary[\"training_time_sec\"] = train_time\n",
    "                                \n",
    "                                # Finish W&B run\n",
    "                                wandb.finish()\n",
    "                                \n",
    "                                # Delete model and variables explicitly before clearing session\n",
    "                                del model\n",
    "                                del opt\n",
    "                                del history\n",
    "                                del y_val_pred\n",
    "                                del y_val_pred_labels\n",
    "                                \n",
    "                                # Clear Keras session to free memory\n",
    "                                tf.keras.backend.clear_session()\n",
    "                                \n",
    "                                # AGGRESSIVE memory cleanup to prevent crashes\n",
    "                                import gc\n",
    "                                gc.collect()\n",
    "                                gc.collect()  # Run twice for thorough cleanup\n",
    "                                \n",
    "                                # Clear GPU memory if using GPU\n",
    "                                try:\n",
    "                                    import torch\n",
    "                                    if torch.cuda.is_available():\n",
    "                                        torch.cuda.empty_cache()\n",
    "                                        torch.cuda.synchronize()  # Wait for GPU operations to finish\n",
    "                                except:\n",
    "                                    pass\n",
    "                                \n",
    "                                # Small delay to let system stabilize\n",
    "                                time.sleep(0.5)\n",
    "                                \n",
    "                                # Save progress every 10 configs\n",
    "                                if config_num % 10 == 0:\n",
    "                                    temp_df = pd.DataFrame(results_bigru)\n",
    "                                    temp_df.drop(columns=['confusion_matrix']).to_csv('rnn_chunked_BiGRU_partial.csv', index=False)\n",
    "                                    print(f\"   ğŸ’¾ Progress saved ({config_num}/{total_configs})\")\n",
    "\n",
    "# Create results DataFrame for BiGRU\n",
    "results_bigru_df = pd.DataFrame(results_bigru)\n",
    "\n",
    "# Save final results\n",
    "results_bigru_csv = results_bigru_df.drop(columns=['confusion_matrix'])\n",
    "results_bigru_csv.to_csv('rnn_chunked_BiGRU_FULL.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… BiGRU HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(f\"\\nğŸ† BEST BiGRU CONFIGURATION (Val Acc: {best_val_acc_bigru:.4f}):\")\n",
    "for key, value in best_config_bigru.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Results saved to 'rnn_chunked_BiGRU_FULL.csv'\")\n",
    "print(f\"   Total BiGRU configurations tested: {len(results_bigru)}\")\n",
    "print(f\"   Best validation accuracy: {results_bigru_csv['val_acc'].max():.4f}\")\n",
    "print(f\"   Mean validation accuracy: {results_bigru_csv['val_acc'].mean():.4f}\")\n",
    "print(f\"   Std validation accuracy: {results_bigru_csv['val_acc'].std():.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1403ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# TOP 5 BiLSTM RESULTATER\n",
    "# ====================================================================\n",
    "import pandas as pd\n",
    "\n",
    "# Load BiLSTM results from saved CSV\n",
    "df_bilstm = pd.read_csv('rnn_chunked_BiLSTM_FULL.csv')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ† TOP 5 BiLSTM KONFIGURATIONER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sort by validation accuracy and show top 5\n",
    "top5 = df_bilstm.nlargest(5, 'val_acc')\n",
    "\n",
    "for idx, (i, row) in enumerate(top5.iterrows(), 1):\n",
    "    print(f\"\\n#{idx} - Val Acc: {row['val_acc']:.4f} ({row['val_acc']*100:.2f}%)\")\n",
    "    print(f\"   Optimizer: {row['optimizer']}\")\n",
    "    print(f\"   Activation: {row['activation']}\")\n",
    "    print(f\"   Dropout: {row['dropout']}\")\n",
    "    print(f\"   RNN Units: {row['rnn_units']}\")\n",
    "    print(f\"   Epochs: {row['epochs']}\")\n",
    "    print(f\"   Train Acc: {row['train_acc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ğŸ“Š Samlet statistik for alle {len(df_bilstm)} BiLSTM konfigurationer:\")\n",
    "print(f\"   Best:  {df_bilstm['val_acc'].max():.4f} ({df_bilstm['val_acc'].max()*100:.2f}%)\")\n",
    "print(f\"   Mean:  {df_bilstm['val_acc'].mean():.4f} ({df_bilstm['val_acc'].mean()*100:.2f}%)\")\n",
    "print(f\"   Std:   {df_bilstm['val_acc'].std():.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88148c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# HYPERPARAMETER SEARCH - PART 4: BiGRU\n",
    "# 3 optimizers Ã— 3 activations Ã— 2 dropouts Ã— 3 units = 54 configs\n",
    "# Estimated time: ~20-30 minutes\n",
    "# ====================================================================\n",
    "import uuid\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” PART 4/4: BiGRU HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define hyperparameter grid for BiGRU\n",
    "rnn_types = ['BiGRU']                            # Only BiGRU\n",
    "optimizers = ['adam', 'adamw', 'sgd']            # All optimizers\n",
    "activations = ['relu', 'tanh', 'selu']           # All activations from course\n",
    "dropouts = [0.2, 0.3333]                         # Two dropout rates\n",
    "batch_norms = [True]                             # Always use batch normalization\n",
    "rnn_units_list = [64, 128, 256]                  # Test different unit sizes\n",
    "learning_rates = [1e-3]                          # Standard learning rate\n",
    "momentums = [0.95]                               # Momentum for SGD only (not Adam/AdamW)\n",
    "\n",
    "# Training configuration\n",
    "TUNING_EPOCHS = 50  # Epochs per config (with early stopping if no improvement)\n",
    "EARLY_STOP_PATIENCE = 6  # Stop if val_loss doesn't improve for 6 epochs\n",
    "\n",
    "# Results storage for BiGRU\n",
    "results_bigru = []\n",
    "best_val_acc_bigru = 0\n",
    "best_config_bigru = None\n",
    "\n",
    "total_configs = len(rnn_types) * len(optimizers) * len(activations) * len(dropouts) * len(batch_norms) * len(rnn_units_list) * len(learning_rates) * len(momentums)\n",
    "print(f\"\\nğŸ“Š Testing {total_configs} BiGRU configurations...\")\n",
    "print(f\"â±ï¸  Estimated time: ~{total_configs * 2} minutes (assuming ~2 min per config)\\n\")\n",
    "print(f\"â° Note: Each config runs max {TUNING_EPOCHS} epochs with early stopping (patience={EARLY_STOP_PATIENCE})\\n\")\n",
    "\n",
    "config_num = 0\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "    for optimizer in optimizers:\n",
    "        for activation in activations:\n",
    "            for dropout in dropouts:\n",
    "                for batch_norm in batch_norms:\n",
    "                    for rnn_units in rnn_units_list:\n",
    "                        for lr in learning_rates:\n",
    "                            for momentum in momentums:\n",
    "                                config_num += 1\n",
    "                                \n",
    "                                print(f\"\\n{'='*70}\")\n",
    "                                print(f\"ğŸ”„ Progress: {config_num}/{total_configs} configurations\")\n",
    "                                print(f\"Config: {rnn_type} | {optimizer} | lr={lr} | mom={momentum} | {activation} | dropout={dropout} | units={rnn_units}\")\n",
    "                                print(f\"{'='*70}\")\n",
    "                                \n",
    "                                # Generate unique run ID\n",
    "                                RUN_ID = uuid.uuid4().hex[:8]\n",
    "                                RUN_NAME = f\"RNN-CHUNKED-{rnn_type}-{optimizer}-seed{SEED}-{RUN_ID}\"\n",
    "                                BASE_DIR = os.path.join(\"progress\", RUN_NAME)\n",
    "                                os.makedirs(BASE_DIR, exist_ok=True)\n",
    "                                \n",
    "                                # Initialize W&B run\n",
    "                                run = wandb.init(\n",
    "                                    project=WANDB_PROJECT,\n",
    "                                    entity=WANDB_ENTITY,\n",
    "                                    name=RUN_NAME,\n",
    "                                    dir=BASE_DIR,\n",
    "                                    config={\n",
    "                                        \"architecture\": \"TITLE_CHUNKS\",\n",
    "                                        \"input_shape\": f\"({MAX_CHUNKS_TOTAL}, {EMBED_DIM})\",\n",
    "                                        \"rnn_type\": rnn_type,\n",
    "                                        \"optimizer\": optimizer,\n",
    "                                        \"learning_rate\": lr,\n",
    "                                        \"momentum\": momentum if optimizer == 'sgd' else None,\n",
    "                                        \"activation\": activation,\n",
    "                                        \"dropout\": dropout,\n",
    "                                        \"batch_norm\": batch_norm,\n",
    "                                        \"rnn_units\": rnn_units,\n",
    "                                        \"max_epochs\": TUNING_EPOCHS,\n",
    "                                        \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "                                        \"early_stop_monitor\": \"val_loss\",\n",
    "                                        \"batch_size\": BATCH,\n",
    "                                        \"seed\": SEED,\n",
    "                                        \"run_id\": RUN_ID,\n",
    "                                        \"config_number\": config_num,\n",
    "                                        \"total_configs\": total_configs,\n",
    "                                        \"class_weights\": \"balanced\",\n",
    "                                    },\n",
    "                                )\n",
    "                                \n",
    "                                # Build model\n",
    "                                model = build_rnn_model(\n",
    "                                    rnn_type=rnn_type,\n",
    "                                    rnn_units=rnn_units,\n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout,\n",
    "                                    use_batch_norm=batch_norm\n",
    "                                )\n",
    "                                \n",
    "                                # Compile model with specific learning rate and momentum\n",
    "                                if optimizer == 'adam':\n",
    "                                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                                elif optimizer == 'adamw':\n",
    "                                    opt = tf.keras.optimizers.AdamW(learning_rate=lr)\n",
    "                                elif optimizer == 'sgd':\n",
    "                                    opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "                                else:\n",
    "                                    opt = optimizer  # Fallback\n",
    "                                \n",
    "                                model.compile(\n",
    "                                    optimizer=opt,\n",
    "                                    loss='sparse_categorical_crossentropy',\n",
    "                                    metrics=['accuracy']\n",
    "                                )\n",
    "                                \n",
    "                                # Early stopping callback - MONITORS val_loss (mode='min')\n",
    "                                early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss',\n",
    "                                    patience=EARLY_STOP_PATIENCE,\n",
    "                                    mode='min',\n",
    "                                    restore_best_weights=True,\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                \n",
    "                                # W&B callbacks\n",
    "                                wandb_callback = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "                                best_metrics_cb = BestMetricsCallback()\n",
    "                                \n",
    "                                # Train model with class weights\n",
    "                                start_time = time.time()\n",
    "                                history = model.fit(\n",
    "                                    train_ds_chunked,\n",
    "                                    validation_data=val_ds_chunked,\n",
    "                                    epochs=TUNING_EPOCHS,\n",
    "                                    class_weight=class_weights,\n",
    "                                    callbacks=[early_stop, wandb_callback, best_metrics_cb],\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                train_time = time.time() - start_time\n",
    "                                \n",
    "                                # Get final metrics\n",
    "                                train_acc = history.history['accuracy'][-1]\n",
    "                                val_acc = history.history['val_accuracy'][-1]\n",
    "                                train_loss = history.history['loss'][-1]\n",
    "                                val_loss = history.history['val_loss'][-1]\n",
    "                                epochs_trained = len(history.history['accuracy'])\n",
    "                                \n",
    "                                # Get predictions for confusion matrix\n",
    "                                y_val_pred = model.predict(val_ds_chunked, verbose=0)\n",
    "                                y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "                                \n",
    "                                # Confusion matrix\n",
    "                                cm = confusion_matrix(y_val_indexed, y_val_pred_labels)\n",
    "                                \n",
    "                                # Store results\n",
    "                                results_bigru.append({\n",
    "                                    'rnn_type': rnn_type,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                    'activation': activation,\n",
    "                                    'dropout': dropout,\n",
    "                                    'batch_norm': batch_norm,\n",
    "                                    'rnn_units': rnn_units,\n",
    "                                    'train_acc': train_acc,\n",
    "                                    'val_acc': val_acc,\n",
    "                                    'train_loss': train_loss,\n",
    "                                    'val_loss': val_loss,\n",
    "                                    'epochs': epochs_trained,\n",
    "                                    'time_sec': train_time,\n",
    "                                    'confusion_matrix': cm\n",
    "                                })\n",
    "                                \n",
    "                                # Print results\n",
    "                                print(f\"âœ… Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "                                print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "                                print(f\"   Epochs: {epochs_trained}/{TUNING_EPOCHS} | Time: {train_time:.1f}s\")\n",
    "                                \n",
    "                                # Track best model\n",
    "                                if val_acc > best_val_acc_bigru:\n",
    "                                    best_val_acc_bigru = val_acc\n",
    "                                    best_config_bigru = {\n",
    "                                        'rnn_type': rnn_type,\n",
    "                                        'optimizer': optimizer,\n",
    "                                        'learning_rate': lr,\n",
    "                                        'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                        'activation': activation,\n",
    "                                        'dropout': dropout,\n",
    "                                        'batch_norm': batch_norm,\n",
    "                                        'rnn_units': rnn_units\n",
    "                                    }\n",
    "                                    print(f\"   ğŸŒŸ NEW BEST MODEL! Val Acc: {val_acc:.4f}\")\n",
    "                                \n",
    "                                # Log final metrics to W&B\n",
    "                                wandb.run.summary[\"best_val_acc\"] = val_acc\n",
    "                                wandb.run.summary[\"best_epoch\"] = epochs_trained\n",
    "                                wandb.run.summary[\"final_train_acc\"] = train_acc\n",
    "                                wandb.run.summary[\"final_train_loss\"] = train_loss\n",
    "                                wandb.run.summary[\"final_val_loss\"] = val_loss\n",
    "                                wandb.run.summary[\"training_time_sec\"] = train_time\n",
    "                                \n",
    "                                # Finish W&B run\n",
    "                                wandb.finish()\n",
    "                                \n",
    "                                # Clear session to free memory\n",
    "                                tf.keras.backend.clear_session()\n",
    "                                \n",
    "                                # Aggressive memory cleanup to prevent crashes\n",
    "                                import gc\n",
    "                                gc.collect()\n",
    "                                \n",
    "                                # Also clear GPU memory if using GPU\n",
    "                                try:\n",
    "                                    import torch\n",
    "                                    if torch.cuda.is_available():\n",
    "                                        torch.cuda.empty_cache()\n",
    "                                except:\n",
    "                                    pass\n",
    "                                \n",
    "                                # Save progress every 10 configs\n",
    "                                if config_num % 10 == 0:\n",
    "                                    temp_df = pd.DataFrame(results_bigru)\n",
    "                                    temp_df.drop(columns=['confusion_matrix']).to_csv('rnn_chunked_BiGRU_partial.csv', index=False)\n",
    "                                    print(f\"   ğŸ’¾ Progress saved ({config_num}/{total_configs})\")\n",
    "\n",
    "# Create results DataFrame for BiGRU\n",
    "results_bigru_df = pd.DataFrame(results_bigru)\n",
    "\n",
    "# Save final results\n",
    "results_bigru_csv = results_bigru_df.drop(columns=['confusion_matrix'])\n",
    "results_bigru_csv.to_csv('rnn_chunked_BiGRU_FULL.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… BiGRU HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(f\"\\nğŸ† BEST BiGRU CONFIGURATION (Val Acc: {best_val_acc_bigru:.4f}):\")\n",
    "for key, value in best_config_bigru.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Results saved to 'rnn_chunked_BiGRU_FULL.csv'\")\n",
    "print(f\"   Total BiGRU configurations tested: {len(results_bigru)}\")\n",
    "print(f\"   Best validation accuracy: {results_bigru_csv['val_acc'].max():.4f}\")\n",
    "print(f\"   Mean validation accuracy: {results_bigru_csv['val_acc'].mean():.4f}\")\n",
    "print(f\"   Std validation accuracy: {results_bigru_csv['val_acc'].std():.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61aac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiGRU Recovery Cell - Complete remaining configs (31-54)\n",
    "# ====================================================================\n",
    "# HYPERPARAMETER SEARCH - PART 4: BiGRU\n",
    "# 3 optimizers Ã— 3 activations Ã— 2 dropouts Ã— 3 units = 54 configs\n",
    "# Estimated time: ~20-30 minutes\n",
    "# ====================================================================\n",
    "import uuid\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” PART 4/4: BiGRU HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define hyperparameter grid for BiGRU\n",
    "rnn_types = ['BiGRU']                            # Only BiGRU\n",
    "optimizers = ['sgd']            \t\t\t\t # All optimizers\n",
    "activations = ['relu', 'tanh', 'selu']           # All activations from course\n",
    "dropouts = [0.2, 0.3333]                         # Two dropout rates\n",
    "batch_norms = [True]                             # Always use batch normalization\n",
    "rnn_units_list = [64, 128, 256]                  # Test different unit sizes\n",
    "learning_rates = [1e-3]                          # Standard learning rate\n",
    "momentums = [0.95]                               # Momentum for SGD only (not Adam/AdamW)\n",
    "\n",
    "# Training configuration\n",
    "TUNING_EPOCHS = 50  # Epochs per config (with early stopping if no improvement)\n",
    "EARLY_STOP_PATIENCE = 6  # Stop if val_loss doesn't improve for 6 epochs\n",
    "\n",
    "# Results storage for BiGRU\n",
    "results_bigru = []\n",
    "best_val_acc_bigru = 0\n",
    "best_config_bigru = None\n",
    "\n",
    "total_configs = len(rnn_types) * len(optimizers) * len(activations) * len(dropouts) * len(batch_norms) * len(rnn_units_list) * len(learning_rates) * len(momentums)\n",
    "print(f\"\\nğŸ“Š Testing {total_configs} BiGRU configurations...\")\n",
    "print(f\"â±ï¸  Estimated time: ~{total_configs * 2} minutes (assuming ~2 min per config)\\n\")\n",
    "print(f\"â° Note: Each config runs max {TUNING_EPOCHS} epochs with early stopping (patience={EARLY_STOP_PATIENCE})\\n\")\n",
    "\n",
    "config_num = 0\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "    for optimizer in optimizers:\n",
    "        for activation in activations:\n",
    "            for dropout in dropouts:\n",
    "                for batch_norm in batch_norms:\n",
    "                    for rnn_units in rnn_units_list:\n",
    "                        for lr in learning_rates:\n",
    "                            for momentum in momentums:\n",
    "                                config_num += 1\n",
    "                                \n",
    "                                print(f\"\\n{'='*70}\")\n",
    "                                print(f\"ğŸ”„ Progress: {config_num}/{total_configs} configurations\")\n",
    "                                print(f\"Config: {rnn_type} | {optimizer} | lr={lr} | mom={momentum} | {activation} | dropout={dropout} | units={rnn_units}\")\n",
    "                                print(f\"{'='*70}\")\n",
    "                                \n",
    "                                # Generate unique run ID\n",
    "                                RUN_ID = uuid.uuid4().hex[:8]\n",
    "                                RUN_NAME = f\"RNN-CHUNKED-{rnn_type}-{optimizer}-seed{SEED}-{RUN_ID}\"\n",
    "                                BASE_DIR = os.path.join(\"progress\", RUN_NAME)\n",
    "                                os.makedirs(BASE_DIR, exist_ok=True)\n",
    "                                \n",
    "                                # Initialize W&B run\n",
    "                                run = wandb.init(\n",
    "                                    project=WANDB_PROJECT,\n",
    "                                    entity=WANDB_ENTITY,\n",
    "                                    name=RUN_NAME,\n",
    "                                    dir=BASE_DIR,\n",
    "                                    config={\n",
    "                                        \"architecture\": \"TITLE_CHUNKS\",\n",
    "                                        \"input_shape\": f\"({MAX_CHUNKS_TOTAL}, {EMBED_DIM})\",\n",
    "                                        \"rnn_type\": rnn_type,\n",
    "                                        \"optimizer\": optimizer,\n",
    "                                        \"learning_rate\": lr,\n",
    "                                        \"momentum\": momentum if optimizer == 'sgd' else None,\n",
    "                                        \"activation\": activation,\n",
    "                                        \"dropout\": dropout,\n",
    "                                        \"batch_norm\": batch_norm,\n",
    "                                        \"rnn_units\": rnn_units,\n",
    "                                        \"max_epochs\": TUNING_EPOCHS,\n",
    "                                        \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "                                        \"early_stop_monitor\": \"val_loss\",\n",
    "                                        \"batch_size\": BATCH,\n",
    "                                        \"seed\": SEED,\n",
    "                                        \"run_id\": RUN_ID,\n",
    "                                        \"config_number\": config_num,\n",
    "                                        \"total_configs\": total_configs,\n",
    "                                        \"class_weights\": \"balanced\",\n",
    "                                    },\n",
    "                                )\n",
    "                                \n",
    "                                # Build model\n",
    "                                model = build_rnn_model(\n",
    "                                    rnn_type=rnn_type,\n",
    "                                    rnn_units=rnn_units,\n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout,\n",
    "                                    use_batch_norm=batch_norm\n",
    "                                )\n",
    "                                \n",
    "                                # Compile model with specific learning rate and momentum\n",
    "                                if optimizer == 'adam':\n",
    "                                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                                elif optimizer == 'adamw':\n",
    "                                    opt = tf.keras.optimizers.AdamW(learning_rate=lr)\n",
    "                                elif optimizer == 'sgd':\n",
    "                                    opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "                                else:\n",
    "                                    opt = optimizer  # Fallback\n",
    "                                \n",
    "                                model.compile(\n",
    "                                    optimizer=opt,\n",
    "                                    loss='sparse_categorical_crossentropy',\n",
    "                                    metrics=['accuracy']\n",
    "                                )\n",
    "                                \n",
    "                                # Early stopping callback - MONITORS val_loss (mode='min')\n",
    "                                early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss',\n",
    "                                    patience=EARLY_STOP_PATIENCE,\n",
    "                                    mode='min',\n",
    "                                    restore_best_weights=True,\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                \n",
    "                                # W&B callbacks\n",
    "                                wandb_callback = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "                                best_metrics_cb = BestMetricsCallback()\n",
    "                                \n",
    "                                # Train model with class weights\n",
    "                                start_time = time.time()\n",
    "                                history = model.fit(\n",
    "                                    train_ds_chunked,\n",
    "                                    validation_data=val_ds_chunked,\n",
    "                                    epochs=TUNING_EPOCHS,\n",
    "                                    class_weight=class_weights,\n",
    "                                    callbacks=[early_stop, wandb_callback, best_metrics_cb],\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                train_time = time.time() - start_time\n",
    "                                \n",
    "                                # Get final metrics\n",
    "                                train_acc = history.history['accuracy'][-1]\n",
    "                                val_acc = history.history['val_accuracy'][-1]\n",
    "                                train_loss = history.history['loss'][-1]\n",
    "                                val_loss = history.history['val_loss'][-1]\n",
    "                                epochs_trained = len(history.history['accuracy'])\n",
    "                                \n",
    "                                # Get predictions for confusion matrix\n",
    "                                y_val_pred = model.predict(val_ds_chunked, verbose=0)\n",
    "                                y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "                                \n",
    "                                # Confusion matrix\n",
    "                                cm = confusion_matrix(y_val_indexed, y_val_pred_labels)\n",
    "                                \n",
    "                                # Store results\n",
    "                                results_bigru.append({\n",
    "                                    'rnn_type': rnn_type,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                    'activation': activation,\n",
    "                                    'dropout': dropout,\n",
    "                                    'batch_norm': batch_norm,\n",
    "                                    'rnn_units': rnn_units,\n",
    "                                    'train_acc': train_acc,\n",
    "                                    'val_acc': val_acc,\n",
    "                                    'train_loss': train_loss,\n",
    "                                    'val_loss': val_loss,\n",
    "                                    'epochs': epochs_trained,\n",
    "                                    'time_sec': train_time,\n",
    "                                    'confusion_matrix': cm\n",
    "                                })\n",
    "                                \n",
    "                                # Print results\n",
    "                                print(f\"âœ… Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "                                print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "                                print(f\"   Epochs: {epochs_trained}/{TUNING_EPOCHS} | Time: {train_time:.1f}s\")\n",
    "                                \n",
    "                                # Track best model\n",
    "                                if val_acc > best_val_acc_bigru:\n",
    "                                    best_val_acc_bigru = val_acc\n",
    "                                    best_config_bigru = {\n",
    "                                        'rnn_type': rnn_type,\n",
    "                                        'optimizer': optimizer,\n",
    "                                        'learning_rate': lr,\n",
    "                                        'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                        'activation': activation,\n",
    "                                        'dropout': dropout,\n",
    "                                        'batch_norm': batch_norm,\n",
    "                                        'rnn_units': rnn_units\n",
    "                                    }\n",
    "                                    print(f\"   ğŸŒŸ NEW BEST MODEL! Val Acc: {val_acc:.4f}\")\n",
    "                                \n",
    "                                # Log final metrics to W&B\n",
    "                                wandb.run.summary[\"best_val_acc\"] = val_acc\n",
    "                                wandb.run.summary[\"best_epoch\"] = epochs_trained\n",
    "                                wandb.run.summary[\"final_train_acc\"] = train_acc\n",
    "                                wandb.run.summary[\"final_train_loss\"] = train_loss\n",
    "                                wandb.run.summary[\"final_val_loss\"] = val_loss\n",
    "                                wandb.run.summary[\"training_time_sec\"] = train_time\n",
    "                                \n",
    "                                # Finish W&B run\n",
    "                                wandb.finish()\n",
    "                                \n",
    "                                # Clear session to free memory\n",
    "                                tf.keras.backend.clear_session()\n",
    "                                \n",
    "                                # Aggressive memory cleanup to prevent crashes\n",
    "                                import gc\n",
    "                                gc.collect()\n",
    "                                \n",
    "                                # Also clear GPU memory if using GPU\n",
    "                                try:\n",
    "                                    import torch\n",
    "                                    if torch.cuda.is_available():\n",
    "                                        torch.cuda.empty_cache()\n",
    "                                except:\n",
    "                                    pass\n",
    "                                \n",
    "                                # Save progress every 10 configs\n",
    "                                if config_num % 10 == 0:\n",
    "                                    temp_df = pd.DataFrame(results_bigru)\n",
    "                                    temp_df.drop(columns=['confusion_matrix']).to_csv('rnn_chunked_BiGRU_partial.csv', index=False)\n",
    "                                    print(f\"   ğŸ’¾ Progress saved ({config_num}/{total_configs})\")\n",
    "\n",
    "# Create results DataFrame for BiGRU\n",
    "results_bigru_df = pd.DataFrame(results_bigru)\n",
    "\n",
    "# Save final results\n",
    "results_bigru_csv = results_bigru_df.drop(columns=['confusion_matrix'])\n",
    "results_bigru_csv.to_csv('rnn_chunked_BiGRU_FULL.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… BiGRU HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(f\"\\nğŸ† BEST BiGRU CONFIGURATION (Val Acc: {best_val_acc_bigru:.4f}):\")\n",
    "for key, value in best_config_bigru.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Results saved to 'rnn_chunked_BiGRU_FULL.csv'\")\n",
    "print(f\"   Total BiGRU configurations tested: {len(results_bigru)}\")\n",
    "print(f\"   Best validation accuracy: {results_bigru_csv['val_acc'].max():.4f}\")\n",
    "print(f\"   Mean validation accuracy: {results_bigru_csv['val_acc'].mean():.4f}\")\n",
    "print(f\"   Std validation accuracy: {results_bigru_csv['val_acc'].std():.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c408ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# COMBINE ALL BiGRU RESULTS AND SHOW TOP 10\n",
    "# ====================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š COMBINING ALL BiGRU RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Read all BiGRU CSV files\n",
    "import glob\n",
    "\n",
    "bigru_files = glob.glob('rnn_chunked_BiGRU*.csv')\n",
    "print(f\"\\nâœ“ Found {len(bigru_files)} BiGRU result files:\")\n",
    "for f in bigru_files:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Combine all results\n",
    "all_bigru_results = []\n",
    "for file in bigru_files:\n",
    "    df = pd.read_csv(file)\n",
    "    all_bigru_results.append(df)\n",
    "    print(f\"  âœ“ Loaded {len(df)} configs from {file}\")\n",
    "\n",
    "# Concatenate and remove duplicates\n",
    "df_bigru_combined = pd.concat(all_bigru_results, ignore_index=True)\n",
    "print(f\"\\nğŸ“Š Total configs before deduplication: {len(df_bigru_combined)}\")\n",
    "\n",
    "# Remove duplicates based on configuration parameters\n",
    "df_bigru_combined = df_bigru_combined.drop_duplicates(\n",
    "    subset=['optimizer', 'activation', 'dropout', 'rnn_units'],\n",
    "    keep='last'  # Keep the most recent run if duplicates exist\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Total unique configs: {len(df_bigru_combined)}\")\n",
    "\n",
    "# Sort by validation accuracy\n",
    "df_bigru_combined_sorted = df_bigru_combined.sort_values('val_acc', ascending=False)\n",
    "\n",
    "# Display TOP 10\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ğŸ† TOP 10 BiGRU CONFIGURATIONS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "top_10 = df_bigru_combined_sorted.head(10)[['optimizer', 'activation', 'dropout', 'rnn_units', 'train_acc', 'val_acc', 'epochs', 'time_sec']]\n",
    "print(top_10.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ğŸŒŸ BEST BiGRU: {df_bigru_combined_sorted.iloc[0]['optimizer']}-{df_bigru_combined_sorted.iloc[0]['activation']}-{df_bigru_combined_sorted.iloc[0]['dropout']}-{df_bigru_combined_sorted.iloc[0]['rnn_units']}\")\n",
    "print(f\"   Val Acc: {df_bigru_combined_sorted.iloc[0]['val_acc']:.4f}\")\n",
    "print(f\"   Train Acc: {df_bigru_combined_sorted.iloc[0]['train_acc']:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save combined results\n",
    "df_bigru_combined_sorted.to_csv('rnn_chunked_BiGRU_ALL_COMBINED.csv', index=False)\n",
    "print(f\"\\nğŸ’¾ Saved all combined results to: rnn_chunked_BiGRU_ALL_COMBINED.csv\")\n",
    "print(f\"   Total unique configurations: {len(df_bigru_combined_sorted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra BIGRU RUN\n",
    "# Check results_bigru variable directly from memory\n",
    "print(f\"Total configs in results_bigru variable: {len(results_bigru)}\")\n",
    "print(f\"\\nFirst 3 configs:\")\n",
    "for i, r in enumerate(results_bigru[:3]):\n",
    "    print(f\"  {i+1}. {r['optimizer']}-{r['activation']}-{r['dropout']}-{r['rnn_units']}: {r['val_acc']:.4f}\")\n",
    "\n",
    "print(f\"\\nLast 3 configs:\")\n",
    "for i, r in enumerate(results_bigru[-3:]):\n",
    "    print(f\"  {len(results_bigru)-2+i}. {r['optimizer']}-{r['activation']}-{r['dropout']}-{r['rnn_units']}: {r['val_acc']:.4f}\")\n",
    "\n",
    "# Convert to DataFrame and show top 10\n",
    "df_from_memory = pd.DataFrame(results_bigru)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ğŸ† TOP 10 BiGRU FROM MEMORY (results_bigru)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "df_sorted = df_from_memory.sort_values('val_acc', ascending=False)\n",
    "top_10_memory = df_sorted.head(10)[['optimizer', 'activation', 'dropout', 'rnn_units', 'train_acc', 'val_acc', 'epochs', 'time_sec']]\n",
    "print(top_10_memory.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cffcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiGRU Recovery Cell - Complete remaining configs (31-54)\n",
    "# ====================================================================\n",
    "# HYPERPARAMETER SEARCH - PART 4: BiGRU\n",
    "# 3 optimizers Ã— 3 activations Ã— 2 dropouts Ã— 3 units = 54 configs\n",
    "# Estimated time: ~20-30 minutes\n",
    "# ====================================================================\n",
    "import uuid\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” PART 4/4: BiGRU HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define hyperparameter grid for BiGRU\n",
    "rnn_types = ['BiGRU']                            # Only BiGRU\n",
    "#optimizers = ['sgd']            \t\t\t\t # All optimizers\n",
    "#optimizers = ['adam', 'adamw', 'sgd']\n",
    "optimizers = ['adam']\n",
    "activations = ['relu', 'tanh', 'selu']           # All activations from course\n",
    "dropouts = [0.2, 0.3333]                         # Two dropout rates\n",
    "batch_norms = [True]                             # Always use batch normalization\n",
    "rnn_units_list = [64, 128, 256]                  # Test different unit sizes\n",
    "learning_rates = [1e-3]                          # Standard learning rate\n",
    "momentums = [0.95]                               # Momentum for SGD only (not Adam/AdamW)\n",
    "\n",
    "# Training configuration\n",
    "TUNING_EPOCHS = 50  # Epochs per config (with early stopping if no improvement)\n",
    "EARLY_STOP_PATIENCE = 6  # Stop if val_loss doesn't improve for 6 epochs\n",
    "\n",
    "# Results storage for BiGRU\n",
    "results_bigru_adam = []\n",
    "best_val_acc_bigru_adam = 0\n",
    "best_config_bigru_adam = None\n",
    "\n",
    "total_configs = len(rnn_types) * len(optimizers) * len(activations) * len(dropouts) * len(batch_norms) * len(rnn_units_list) * len(learning_rates) * len(momentums)\n",
    "print(f\"\\nğŸ“Š Testing {total_configs} BiGRU configurations...\")\n",
    "print(f\"â±ï¸  Estimated time: ~{total_configs * 2} minutes (assuming ~2 min per config)\\n\")\n",
    "print(f\"â° Note: Each config runs max {TUNING_EPOCHS} epochs with early stopping (patience={EARLY_STOP_PATIENCE})\\n\")\n",
    "\n",
    "config_num = 0\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "    for optimizer in optimizers:\n",
    "        for activation in activations:\n",
    "            for dropout in dropouts:\n",
    "                for batch_norm in batch_norms:\n",
    "                    for rnn_units in rnn_units_list:\n",
    "                        for lr in learning_rates:\n",
    "                            for momentum in momentums:\n",
    "                                config_num += 1\n",
    "                                \n",
    "                                print(f\"\\n{'='*70}\")\n",
    "                                print(f\"ğŸ”„ Progress: {config_num}/{total_configs} configurations\")\n",
    "                                print(f\"Config: {rnn_type} | {optimizer} | lr={lr} | mom={momentum} | {activation} | dropout={dropout} | units={rnn_units}\")\n",
    "                                print(f\"{'='*70}\")\n",
    "                                \n",
    "                                # Generate unique run ID\n",
    "                                RUN_ID = uuid.uuid4().hex[:8]\n",
    "                                RUN_NAME = f\"RNN-CHUNKED-{rnn_type}-{optimizer}-seed{SEED}-{RUN_ID}\"\n",
    "                                BASE_DIR = os.path.join(\"progress\", RUN_NAME)\n",
    "                                os.makedirs(BASE_DIR, exist_ok=True)\n",
    "                                \n",
    "                                # Initialize W&B run\n",
    "                                run = wandb.init(\n",
    "                                    project=WANDB_PROJECT,\n",
    "                                    entity=WANDB_ENTITY,\n",
    "                                    name=RUN_NAME,\n",
    "                                    dir=BASE_DIR,\n",
    "                                    config={\n",
    "                                        \"architecture\": \"TITLE_CHUNKS\",\n",
    "                                        \"input_shape\": f\"({MAX_CHUNKS_TOTAL}, {EMBED_DIM})\",\n",
    "                                        \"rnn_type\": rnn_type,\n",
    "                                        \"optimizer\": optimizer,\n",
    "                                        \"learning_rate\": lr,\n",
    "                                        \"momentum\": momentum if optimizer == 'sgd' else None,\n",
    "                                        \"activation\": activation,\n",
    "                                        \"dropout\": dropout,\n",
    "                                        \"batch_norm\": batch_norm,\n",
    "                                        \"rnn_units\": rnn_units,\n",
    "                                        \"max_epochs\": TUNING_EPOCHS,\n",
    "                                        \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "                                        \"early_stop_monitor\": \"val_loss\",\n",
    "                                        \"batch_size\": BATCH,\n",
    "                                        \"seed\": SEED,\n",
    "                                        \"run_id\": RUN_ID,\n",
    "                                        \"config_number\": config_num,\n",
    "                                        \"total_configs\": total_configs,\n",
    "                                        \"class_weights\": \"balanced\",\n",
    "                                    },\n",
    "                                )\n",
    "                                \n",
    "                                # Build model\n",
    "                                model = build_rnn_model(\n",
    "                                    rnn_type=rnn_type,\n",
    "                                    rnn_units=rnn_units,\n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout,\n",
    "                                    use_batch_norm=batch_norm\n",
    "                                )\n",
    "                                \n",
    "                                # Compile model with specific learning rate and momentum\n",
    "                                if optimizer == 'adam':\n",
    "                                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                                elif optimizer == 'adamw':\n",
    "                                    opt = tf.keras.optimizers.AdamW(learning_rate=lr)\n",
    "                                elif optimizer == 'sgd':\n",
    "                                    opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "                                else:\n",
    "                                    opt = optimizer  # Fallback\n",
    "                                \n",
    "                                model.compile(\n",
    "                                    optimizer=opt,\n",
    "                                    loss='sparse_categorical_crossentropy',\n",
    "                                    metrics=['accuracy']\n",
    "                                )\n",
    "                                \n",
    "                                # Early stopping callback - MONITORS val_loss (mode='min')\n",
    "                                early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss',\n",
    "                                    patience=EARLY_STOP_PATIENCE,\n",
    "                                    mode='min',\n",
    "                                    restore_best_weights=True,\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                \n",
    "                                # W&B callbacks\n",
    "                                wandb_callback = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "                                best_metrics_cb = BestMetricsCallback()\n",
    "                                \n",
    "                                # Train model with class weights\n",
    "                                start_time = time.time()\n",
    "                                history = model.fit(\n",
    "                                    train_ds_chunked,\n",
    "                                    validation_data=val_ds_chunked,\n",
    "                                    epochs=TUNING_EPOCHS,\n",
    "                                    class_weight=class_weights,\n",
    "                                    callbacks=[early_stop, wandb_callback, best_metrics_cb],\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                train_time = time.time() - start_time\n",
    "                                \n",
    "                                # Get final metrics\n",
    "                                train_acc = history.history['accuracy'][-1]\n",
    "                                val_acc = history.history['val_accuracy'][-1]\n",
    "                                train_loss = history.history['loss'][-1]\n",
    "                                val_loss = history.history['val_loss'][-1]\n",
    "                                epochs_trained = len(history.history['accuracy'])\n",
    "                                \n",
    "                                # Get predictions for confusion matrix\n",
    "                                y_val_pred = model.predict(val_ds_chunked, verbose=0)\n",
    "                                y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "                                \n",
    "                                # Confusion matrix\n",
    "                                cm = confusion_matrix(y_val_indexed, y_val_pred_labels)\n",
    "                                \n",
    "                                # Store results\n",
    "                                results_bigru_adam.append({\n",
    "                                    'rnn_type': rnn_type,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                    'activation': activation,\n",
    "                                    'dropout': dropout,\n",
    "                                    'batch_norm': batch_norm,\n",
    "                                    'rnn_units': rnn_units,\n",
    "                                    'train_acc': train_acc,\n",
    "                                    'val_acc': val_acc,\n",
    "                                    'train_loss': train_loss,\n",
    "                                    'val_loss': val_loss,\n",
    "                                    'epochs': epochs_trained,\n",
    "                                    'time_sec': train_time,\n",
    "                                    'confusion_matrix': cm\n",
    "                                })\n",
    "                                \n",
    "                                # Print results\n",
    "                                print(f\"âœ… Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "                                print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "                                print(f\"   Epochs: {epochs_trained}/{TUNING_EPOCHS} | Time: {train_time:.1f}s\")\n",
    "                                \n",
    "                                # Track best model\n",
    "                                if val_acc > best_val_acc_bigru_adam:\n",
    "                                    best_val_acc_bigru_adam = val_acc\n",
    "                                    best_config_bigru_adam = {\n",
    "                                        'rnn_type': rnn_type,\n",
    "                                        'optimizer': optimizer,\n",
    "                                        'learning_rate': lr,\n",
    "                                        'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                        'activation': activation,\n",
    "                                        'dropout': dropout,\n",
    "                                        'batch_norm': batch_norm,\n",
    "                                        'rnn_units': rnn_units\n",
    "                                    }\n",
    "                                    print(f\"   ğŸŒŸ NEW BEST MODEL! Val Acc: {val_acc:.4f}\")\n",
    "                                \n",
    "                                # Log final metrics to W&B\n",
    "                                wandb.run.summary[\"best_val_acc\"] = val_acc\n",
    "                                wandb.run.summary[\"best_epoch\"] = epochs_trained\n",
    "                                wandb.run.summary[\"final_train_acc\"] = train_acc\n",
    "                                wandb.run.summary[\"final_train_loss\"] = train_loss\n",
    "                                wandb.run.summary[\"final_val_loss\"] = val_loss\n",
    "                                wandb.run.summary[\"training_time_sec\"] = train_time\n",
    "                                \n",
    "                                # Finish W&B run\n",
    "                                wandb.finish()\n",
    "                                \n",
    "                                # Clear session to free memory\n",
    "                                tf.keras.backend.clear_session()\n",
    "                                \n",
    "                                # Aggressive memory cleanup to prevent crashes\n",
    "                                import gc\n",
    "                                gc.collect()\n",
    "                                \n",
    "                                # Also clear GPU memory if using GPU\n",
    "                                try:\n",
    "                                    import torch\n",
    "                                    if torch.cuda.is_available():\n",
    "                                        torch.cuda.empty_cache()\n",
    "                                except:\n",
    "                                    pass\n",
    "                                \n",
    "                                # Save progress every 10 configs\n",
    "                                if config_num % 10 == 0:\n",
    "                                    temp_df = pd.DataFrame(results_bigru_adam)\n",
    "                                    temp_df.drop(columns=['confusion_matrix']).to_csv('rnn_chunked_BiGRU_ADAM_partial.csv', index=False)\n",
    "                                    print(f\"   ğŸ’¾ Progress saved ({config_num}/{total_configs})\")\n",
    "\n",
    "# Create results DataFrame for BiGRU\n",
    "results_bigru_adam_df = pd.DataFrame(results_bigru_adam)\n",
    "\n",
    "# Save final results\n",
    "results_bigru_adam_csv = results_bigru_adam_df.drop(columns=['confusion_matrix'])\n",
    "results_bigru_adam_csv.to_csv('rnn_chunked_BiGRU_ADAM_partial.csv', index=False)\n",
    "print(\"\\nâœ… BiGRU ADAM  HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(f\"\\nğŸ† BEST BiGRU CONFIGURATION (Val Acc: {best_val_acc_bigru_adam:.4f}):\")\n",
    "for key, value in best_config_bigru_adam.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Results saved to 'rnn_chunked_BiGRU_ADAM_partial.csv'\")\n",
    "print(f\"   Total BiGRU configurations tested: {len(results_bigru_adam)}\")\n",
    "print(f\"   Best validation accuracy: {results_bigru_adam_csv['val_acc'].max():.4f}\")\n",
    "print(f\"   Mean validation accuracy: {results_bigru_adam_csv['val_acc'].mean():.4f}\")\n",
    "print(f\"   Std validation accuracy: {results_bigru_adam_csv['val_acc'].std():.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04acc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiGRU Recovery Cell - Complete remaining configs (31-54)\n",
    "# ====================================================================\n",
    "# HYPERPARAMETER SEARCH - PART 4: BiGRU\n",
    "# 3 optimizers Ã— 3 activations Ã— 2 dropouts Ã— 3 units = 54 configs\n",
    "# Estimated time: ~20-30 minutes\n",
    "# ====================================================================\n",
    "import uuid\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” PART 4/4: BiGRU HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define hyperparameter grid for BiGRU\n",
    "rnn_types = ['BiGRU']                            # Only BiGRU\n",
    "#optimizers = ['sgd']            \t\t\t\t # All optimizers\n",
    "#optimizers = ['adam', 'adamw', 'sgd']\n",
    "optimizers = ['adamw']\n",
    "activations = ['relu', 'tanh', 'selu']           # All activations from course\n",
    "dropouts = [0.2, 0.3333]                         # Two dropout rates\n",
    "batch_norms = [True]                             # Always use batch normalization\n",
    "rnn_units_list = [64, 128, 256]                  # Test different unit sizes\n",
    "learning_rates = [1e-3]                          # Standard learning rate\n",
    "momentums = [0.95]                               # Momentum for SGD only (not Adam/AdamW)\n",
    "\n",
    "# Training configuration\n",
    "TUNING_EPOCHS = 50  # Epochs per config (with early stopping if no improvement)\n",
    "EARLY_STOP_PATIENCE = 6  # Stop if val_loss doesn't improve for 6 epochs\n",
    "\n",
    "# Results storage for BiGRU\n",
    "results_bigru_adamw = []\n",
    "best_val_acc_bigru_adamw = 0\n",
    "best_config_bigru_adamw = None\n",
    "\n",
    "total_configs = len(rnn_types) * len(optimizers) * len(activations) * len(dropouts) * len(batch_norms) * len(rnn_units_list) * len(learning_rates) * len(momentums)\n",
    "print(f\"\\nğŸ“Š Testing {total_configs} BiGRU configurations...\")\n",
    "print(f\"â±ï¸  Estimated time: ~{total_configs * 2} minutes (assuming ~2 min per config)\\n\")\n",
    "print(f\"â° Note: Each config runs max {TUNING_EPOCHS} epochs with early stopping (patience={EARLY_STOP_PATIENCE})\\n\")\n",
    "\n",
    "config_num = 0\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "    for optimizer in optimizers:\n",
    "        for activation in activations:\n",
    "            for dropout in dropouts:\n",
    "                for batch_norm in batch_norms:\n",
    "                    for rnn_units in rnn_units_list:\n",
    "                        for lr in learning_rates:\n",
    "                            for momentum in momentums:\n",
    "                                config_num += 1\n",
    "                                \n",
    "                                print(f\"\\n{'='*70}\")\n",
    "                                print(f\"ğŸ”„ Progress: {config_num}/{total_configs} configurations\")\n",
    "                                print(f\"Config: {rnn_type} | {optimizer} | lr={lr} | mom={momentum} | {activation} | dropout={dropout} | units={rnn_units}\")\n",
    "                                print(f\"{'='*70}\")\n",
    "                                \n",
    "                                # Generate unique run ID\n",
    "                                RUN_ID = uuid.uuid4().hex[:8]\n",
    "                                RUN_NAME = f\"RNN-CHUNKED-{rnn_type}-{optimizer}-seed{SEED}-{RUN_ID}\"\n",
    "                                BASE_DIR = os.path.join(\"progress\", RUN_NAME)\n",
    "                                os.makedirs(BASE_DIR, exist_ok=True)\n",
    "                                \n",
    "                                # Initialize W&B run\n",
    "                                run = wandb.init(\n",
    "                                    project=WANDB_PROJECT,\n",
    "                                    entity=WANDB_ENTITY,\n",
    "                                    name=RUN_NAME,\n",
    "                                    dir=BASE_DIR,\n",
    "                                    config={\n",
    "                                        \"architecture\": \"TITLE_CHUNKS\",\n",
    "                                        \"input_shape\": f\"({MAX_CHUNKS_TOTAL}, {EMBED_DIM})\",\n",
    "                                        \"rnn_type\": rnn_type,\n",
    "                                        \"optimizer\": optimizer,\n",
    "                                        \"learning_rate\": lr,\n",
    "                                        \"momentum\": momentum if optimizer == 'sgd' else None,\n",
    "                                        \"activation\": activation,\n",
    "                                        \"dropout\": dropout,\n",
    "                                        \"batch_norm\": batch_norm,\n",
    "                                        \"rnn_units\": rnn_units,\n",
    "                                        \"max_epochs\": TUNING_EPOCHS,\n",
    "                                        \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "                                        \"early_stop_monitor\": \"val_loss\",\n",
    "                                        \"batch_size\": BATCH,\n",
    "                                        \"seed\": SEED,\n",
    "                                        \"run_id\": RUN_ID,\n",
    "                                        \"config_number\": config_num,\n",
    "                                        \"total_configs\": total_configs,\n",
    "                                        \"class_weights\": \"balanced\",\n",
    "                                    },\n",
    "                                )\n",
    "                                \n",
    "                                # Build model\n",
    "                                model = build_rnn_model(\n",
    "                                    rnn_type=rnn_type,\n",
    "                                    rnn_units=rnn_units,\n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout,\n",
    "                                    use_batch_norm=batch_norm\n",
    "                                )\n",
    "                                \n",
    "                                # Compile model with specific learning rate and momentum\n",
    "                                if optimizer == 'adam':\n",
    "                                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                                elif optimizer == 'adamw':\n",
    "                                    opt = tf.keras.optimizers.AdamW(learning_rate=lr)\n",
    "                                elif optimizer == 'sgd':\n",
    "                                    opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "                                else:\n",
    "                                    opt = optimizer  # Fallback\n",
    "                                \n",
    "                                model.compile(\n",
    "                                    optimizer=opt,\n",
    "                                    loss='sparse_categorical_crossentropy',\n",
    "                                    metrics=['accuracy']\n",
    "                                )\n",
    "                                \n",
    "                                # Early stopping callback - MONITORS val_loss (mode='min')\n",
    "                                early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss',\n",
    "                                    patience=EARLY_STOP_PATIENCE,\n",
    "                                    mode='min',\n",
    "                                    restore_best_weights=True,\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                \n",
    "                                # W&B callbacks\n",
    "                                wandb_callback = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "                                best_metrics_cb = BestMetricsCallback()\n",
    "                                \n",
    "                                # Train model with class weights\n",
    "                                start_time = time.time()\n",
    "                                history = model.fit(\n",
    "                                    train_ds_chunked,\n",
    "                                    validation_data=val_ds_chunked,\n",
    "                                    epochs=TUNING_EPOCHS,\n",
    "                                    class_weight=class_weights,\n",
    "                                    callbacks=[early_stop, wandb_callback, best_metrics_cb],\n",
    "                                    verbose=1\n",
    "                                )\n",
    "                                train_time = time.time() - start_time\n",
    "                                \n",
    "                                # Get final metrics\n",
    "                                train_acc = history.history['accuracy'][-1]\n",
    "                                val_acc = history.history['val_accuracy'][-1]\n",
    "                                train_loss = history.history['loss'][-1]\n",
    "                                val_loss = history.history['val_loss'][-1]\n",
    "                                epochs_trained = len(history.history['accuracy'])\n",
    "                                \n",
    "                                # Get predictions for confusion matrix\n",
    "                                y_val_pred = model.predict(val_ds_chunked, verbose=0)\n",
    "                                y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "                                \n",
    "                                # Confusion matrix\n",
    "                                cm = confusion_matrix(y_val_indexed, y_val_pred_labels)\n",
    "                                \n",
    "                                # Store results\n",
    "                                results_bigru_adamw.append({\n",
    "                                    'rnn_type': rnn_type,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                    'activation': activation,\n",
    "                                    'dropout': dropout,\n",
    "                                    'batch_norm': batch_norm,\n",
    "                                    'rnn_units': rnn_units,\n",
    "                                    'train_acc': train_acc,\n",
    "                                    'val_acc': val_acc,\n",
    "                                    'train_loss': train_loss,\n",
    "                                    'val_loss': val_loss,\n",
    "                                    'epochs': epochs_trained,\n",
    "                                    'time_sec': train_time,\n",
    "                                    'confusion_matrix': cm\n",
    "                                })\n",
    "                                \n",
    "                                # Print results\n",
    "                                print(f\"âœ… Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "                                print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "                                print(f\"   Epochs: {epochs_trained}/{TUNING_EPOCHS} | Time: {train_time:.1f}s\")\n",
    "                                \n",
    "                                # Track best model\n",
    "                                if val_acc > best_val_acc_bigru_adamw:\n",
    "                                    best_val_acc_bigru_adamw = val_acc\n",
    "                                    best_config_bigru_adamw = {\n",
    "                                        'rnn_type': rnn_type,\n",
    "                                        'optimizer': optimizer,\n",
    "                                        'learning_rate': lr,\n",
    "                                        'momentum': momentum if optimizer == 'sgd' else None,\n",
    "                                        'activation': activation,\n",
    "                                        'dropout': dropout,\n",
    "                                        'batch_norm': batch_norm,\n",
    "                                        'rnn_units': rnn_units\n",
    "                                    }\n",
    "                                    print(f\"   ğŸŒŸ NEW BEST MODEL! Val Acc: {val_acc:.4f}\")\n",
    "                                \n",
    "                                # Log final metrics to W&B\n",
    "                                wandb.run.summary[\"best_val_acc\"] = val_acc\n",
    "                                wandb.run.summary[\"best_epoch\"] = epochs_trained\n",
    "                                wandb.run.summary[\"final_train_acc\"] = train_acc\n",
    "                                wandb.run.summary[\"final_train_loss\"] = train_loss\n",
    "                                wandb.run.summary[\"final_val_loss\"] = val_loss\n",
    "                                wandb.run.summary[\"training_time_sec\"] = train_time\n",
    "                                \n",
    "                                # Finish W&B run\n",
    "                                wandb.finish()\n",
    "                                \n",
    "                                # Clear session to free memory\n",
    "                                tf.keras.backend.clear_session()\n",
    "                                \n",
    "                                # Aggressive memory cleanup to prevent crashes\n",
    "                                import gc\n",
    "                                gc.collect()\n",
    "                                \n",
    "                                # Also clear GPU memory if using GPU\n",
    "                                try:\n",
    "                                    import torch\n",
    "                                    if torch.cuda.is_available():\n",
    "                                        torch.cuda.empty_cache()\n",
    "                                except:\n",
    "                                    pass\n",
    "                                \n",
    "                                # Save progress every 10 configs\n",
    "                                if config_num % 10 == 0:\n",
    "                                    temp_df = pd.DataFrame(results_bigru_adamw)\n",
    "                                    temp_df.drop(columns=['confusion_matrix']).to_csv('rnn_chunked_BiGRU_ADAMW_partial.csv', index=False)\n",
    "                                    print(f\"   ğŸ’¾ Progress saved ({config_num}/{total_configs})\")\n",
    "\n",
    "# Create results DataFrame for BiGRU\n",
    "results_bigru_adamw_df = pd.DataFrame(results_bigru_adamw)\n",
    "\n",
    "# Save final results\n",
    "results_bigru_adamw_csv = results_bigru_adamw_df.drop(columns=['confusion_matrix'])\n",
    "results_bigru_adamw_csv.to_csv('rnn_chunked_BiGRU_ADAMW_partial.csv', index=False)\n",
    "print(\"\\nâœ… BiGRU ADAMW  HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(f\"\\nğŸ† BEST BiGRU CONFIGURATION (Val Acc: {best_val_acc_bigru_adamw:.4f}):\")\n",
    "for key, value in best_config_bigru_adamw.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Results saved to 'rnn_chunked_BiGRU_ADAMW_partial.csv'\")\n",
    "print(f\"   Total BiGRU configurations tested: {len(results_bigru_adamw)}\")\n",
    "print(f\"   Best validation accuracy: {results_bigru_adamw_csv['val_acc'].max():.4f}\")\n",
    "print(f\"   Mean validation accuracy: {results_bigru_adamw_csv['val_acc'].mean():.4f}\")\n",
    "print(f\"   Std validation accuracy: {results_bigru_adamw_csv['val_acc'].std():.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c197ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# COMBINE ALL BiGRU RESULTS (SGD, ADAM, ADAMW) AND SHOW TOP 10\n",
    "# ====================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š COMBINING ALL BiGRU RESULTS FROM ALL 3 OPTIMIZERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load all 3 CSV files\n",
    "df_sgd = pd.read_csv('rnn_chunked_BiGRU_FULL.csv')\n",
    "df_adam = pd.read_csv('rnn_chunked_BiGRU_ADAM_partial.csv')\n",
    "df_adamw = pd.read_csv('rnn_chunked_BiGRU_ADAMW_partial.csv')\n",
    "\n",
    "print(f\"\\nâœ“ SGD configs: {len(df_sgd)}\")\n",
    "print(f\"âœ“ ADAM configs: {len(df_adam)}\")\n",
    "print(f\"âœ“ ADAMW configs: {len(df_adamw)}\")\n",
    "\n",
    "# Combine all results\n",
    "df_all_bigru = pd.concat([df_sgd, df_adam, df_adamw], ignore_index=True)\n",
    "print(f\"\\nâœ“ Total BiGRU configs: {len(df_all_bigru)}/54\")\n",
    "\n",
    "# Sort by validation accuracy\n",
    "df_all_bigru_sorted = df_all_bigru.sort_values('val_acc', ascending=False)\n",
    "\n",
    "# Display TOP 10\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ğŸ† TOP 10 BiGRU CONFIGURATIONS (ALL OPTIMIZERS)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "top_10 = df_all_bigru_sorted.head(10)[['optimizer', 'activation', 'dropout', 'rnn_units', 'train_acc', 'val_acc', 'epochs', 'time_sec']]\n",
    "print(top_10.to_string(index=False))\n",
    "\n",
    "# Show best overall\n",
    "best = df_all_bigru_sorted.iloc[0]\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ğŸŒŸ ABSOLUTE BEST BiGRU: {best['optimizer']}-{best['activation']}-{best['dropout']}-{best['rnn_units']}\")\n",
    "print(f\"   Val Acc: {best['val_acc']:.4f}\")\n",
    "print(f\"   Train Acc: {best['train_acc']:.4f}\")\n",
    "print(f\"   Epochs: {int(best['epochs'])}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save combined results\n",
    "df_all_bigru_sorted.to_csv('rnn_chunked_BiGRU_ALL_FULL.csv', index=False)\n",
    "print(f\"\\nğŸ’¾ Saved all combined results to: rnn_chunked_BiGRU_ALL_FULL.csv\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nğŸ“ˆ BiGRU SUMMARY STATISTICS:\")\n",
    "print(f\"   Best val_acc: {df_all_bigru_sorted['val_acc'].max():.4f}\")\n",
    "print(f\"   Mean val_acc: {df_all_bigru_sorted['val_acc'].mean():.4f}\")\n",
    "print(f\"   Std val_acc: {df_all_bigru_sorted['val_acc'].std():.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# COMBINE ALL RESULTS FROM ALL 4 RNN TYPES\n",
    "# ====================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š COMBINING RESULTS FROM ALL RNN TYPES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine all results into one DataFrame\n",
    "all_results = []\n",
    "\n",
    "# Add LSTM results if exists\n",
    "if 'results_lstm' in globals() and len(results_lstm) > 0:\n",
    "    all_results.extend(results_lstm)\n",
    "    print(f\"âœ… Added {len(results_lstm)} LSTM results\")\n",
    "\n",
    "# Add GRU results if exists\n",
    "if 'results_gru' in globals() and len(results_gru) > 0:\n",
    "    all_results.extend(results_gru)\n",
    "    print(f\"âœ… Added {len(results_gru)} GRU results\")\n",
    "\n",
    "# Add BiLSTM results if exists\n",
    "if 'results_bilstm' in globals() and len(results_bilstm) > 0:\n",
    "    all_results.extend(results_bilstm)\n",
    "    print(f\"âœ… Added {len(results_bilstm)} BiLSTM results\")\n",
    "\n",
    "# Add BiGRU results if exists\n",
    "if 'results_bigru' in globals() and len(results_bigru) > 0:\n",
    "    all_results.extend(results_bigru)\n",
    "    print(f\"âœ… Added {len(results_bigru)} BiGRU results\")\n",
    "\n",
    "# Create combined DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save combined results\n",
    "results_csv = results_df.drop(columns=['confusion_matrix'])\n",
    "results_csv.to_csv('rnn_chunked_hyperparameter_results_ALL.csv', index=False)\n",
    "\n",
    "print(f\"\\nğŸ“Š Combined Results Summary:\")\n",
    "print(f\"   Total configurations: {len(results_df)}\")\n",
    "print(f\"   Best validation accuracy: {results_csv['val_acc'].max():.4f}\")\n",
    "print(f\"   Mean validation accuracy: {results_csv['val_acc'].mean():.4f}\")\n",
    "print(f\"   Std validation accuracy: {results_csv['val_acc'].std():.4f}\")\n",
    "\n",
    "# Find overall best configuration\n",
    "best_idx = results_df['val_acc'].idxmax()\n",
    "best_result = results_df.iloc[best_idx]\n",
    "best_config = {\n",
    "    'rnn_type': best_result['rnn_type'],\n",
    "    'optimizer': best_result['optimizer'],\n",
    "    'learning_rate': best_result['learning_rate'],\n",
    "    'momentum': best_result['momentum'],\n",
    "    'activation': best_result['activation'],\n",
    "    'dropout': best_result['dropout'],\n",
    "    'batch_norm': best_result['batch_norm'],\n",
    "    'rnn_units': best_result['rnn_units']\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ† OVERALL BEST CONFIGURATION (Val Acc: {best_result['val_acc']:.4f}):\")\n",
    "for key, value in best_config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ Results by RNN Type:\")\n",
    "rnn_summary = results_csv.groupby('rnn_type')['val_acc'].agg(['count', 'mean', 'std', 'max']).sort_values('max', ascending=False)\n",
    "print(rnn_summary)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ… All results saved to 'rnn_chunked_hyperparameter_results_ALL.csv'\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155dbdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# ANALYZE RESULTS - Top 10 Configurations\n",
    "# NOTE: Run the \"COMBINE ALL RESULTS\" cell first!\n",
    "# ====================================================================\n",
    "\n",
    "if 'results_df' not in globals():\n",
    "    print(\"âš ï¸  ERROR: results_df not found!\")\n",
    "    print(\"   Please run the 'COMBINE ALL RESULTS' cell first.\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ“ˆ TOP 10 CONFIGURATIONS BY VALIDATION ACCURACY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    top_10 = results_df.nlargest(10, 'val_acc')[['rnn_type', 'optimizer', 'learning_rate', 'momentum', 'activation', \n",
    "                                                   'dropout', 'rnn_units',\n",
    "                                                   'val_acc', 'train_acc', 'epochs']]\n",
    "    print(top_10.to_string(index=False))\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)    print(bottom_10.to_string(index=False))\n",
    "\n",
    "    print(\"ğŸ“‰ BOTTOM 10 CONFIGURATIONS BY VALIDATION ACCURACY\")                                                       'val_acc', 'train_acc', 'epochs']]\n",
    "\n",
    "    print(\"=\"*70)                                                       'dropout', 'rnn_units',\n",
    "\n",
    "        bottom_10 = results_df.nsmallest(10, 'val_acc')[['rnn_type', 'optimizer', 'learning_rate', 'momentum', 'activation', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# VISUALIZE BEST MODEL'S CONFUSION MATRIX\n",
    "# NOTE: Run the \"COMBINE ALL RESULTS\" cell first!\n",
    "# ====================================================================\n",
    "\n",
    "if 'results_df' not in globals():\n",
    "    print(\"âš ï¸  ERROR: results_df not found!\")\n",
    "    print(\"   Please run the 'COMBINE ALL RESULTS' cell first.\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ¯ CONFUSION MATRIX - BEST MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    best_idx = results_df['val_acc'].idxmax()\n",
    "    best_result = results_df.iloc[best_idx]\n",
    "    best_cm = best_result['confusion_matrix']\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(best_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix - Best Model\\n{best_result[\"rnn_type\"]} | Val Acc: {best_result[\"val_acc\"]:.4f}')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['1 star', '2 stars', '3 stars', '4 stars', '5 stars']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = best_cm.max() / 2.\n",
    "    for i in range(best_cm.shape[0]):\n",
    "        for j in range(best_cm.shape[1]):\n",
    "            plt.text(j, i, format(best_cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if best_cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed metrics\n",
    "    print(f\"\\nğŸ“Š Best Configuration Details:\")\n",
    "    print(f\"   RNN Type: {best_result['rnn_type']}\")\n",
    "    print(f\"   Optimizer: {best_result['optimizer']}\")\n",
    "    print(f\"   Learning Rate: {best_result['learning_rate']}\")\n",
    "    print(f\"   Momentum: {best_result['momentum']}\")\n",
    "    print(f\"   Activation: {best_result['activation']}\")\n",
    "    print(f\"   Dropout: {best_result['dropout']}\")\n",
    "    print(f\"   RNN Units: {best_result['rnn_units']}\")\n",
    "    print(f\"   Val Accuracy: {best_result['val_acc']:.4f}\")\n",
    "    print(f\"   Train Accuracy: {best_result['train_acc']:.4f}\")\n",
    "    print(f\"   Training Time: {best_result['time_sec']:.1f}s\")\n",
    "\n",
    "print(f\"   Learning Rate: {best_result['learning_rate']}\")\n",
    "print(f\"   Training Time: {best_result['time_sec']:.1f}s\")\n",
    "\n",
    "print(f\"   Momentum: {best_result['momentum']}\")\n",
    "print(f\"   Train Accuracy: {best_result['train_acc']:.4f}\")\n",
    "\n",
    "print(f\"   Activation: {best_result['activation']}\")\n",
    "print(f\"   Val Accuracy: {best_result['val_acc']:.4f}\")\n",
    "\n",
    "print(f\"   Dropout: {best_result['dropout']}\")\n",
    "print(f\"   RNN Units: {best_result['rnn_units']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# HYPERPARAMETER IMPACT ANALYSIS\n",
    "# NOTE: Run the \"COMBINE ALL RESULTS\" cell first!\n",
    "# ====================================================================\n",
    "\n",
    "if 'results_df' not in globals():\n",
    "    print(\"âš ï¸  ERROR: results_df not found!\")\n",
    "    print(\"   Please run the 'COMBINE ALL RESULTS' cell first.\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nğŸ“Š Average Validation Accuracy by Optimizer:\")\n",
    "    opt_impact = results_df.groupby('optimizer')['val_acc'].agg(['mean', 'std', 'max']).sort_values('mean', ascending=False)\n",
    "    print(opt_impact)\n",
    "    \n",
    "    print(\"\\nğŸ“Š Average Validation Accuracy by Activation:\")\n",
    "    act_impact = results_df.groupby('activation')['val_acc'].agg(['mean', 'std', 'max']).sort_values('mean', ascending=False)\n",
    "    print(act_impact)\n",
    "    \n",
    "    print(\"\\nğŸ“Š Average Validation Accuracy by Dropout:\")\n",
    "    drop_impact = results_df.groupby('dropout')['val_acc'].agg(['mean', 'std', 'max']).sort_values('mean', ascending=False)\n",
    "    print(drop_impact)\n",
    "    \n",
    "    print(\"\\nğŸ“Š Average Validation Accuracy by Learning Rate:\")\n",
    "    lr_impact = results_df.groupby('learning_rate')['val_acc'].agg(['mean', 'std', 'max']).sort_values('mean', ascending=False)\n",
    "    print(lr_impact)\n",
    "    \n",
    "    print(\"\\nğŸ“Š Average Validation Accuracy by Momentum (SGD only):\")\n",
    "    sgd_results = results_df[results_df['optimizer'] == 'sgd']\n",
    "    if len(sgd_results) > 0:\n",
    "        momentum_impact = sgd_results.groupby('momentum')['val_acc'].agg(['mean', 'std', 'max']).sort_values('mean', ascending=False)\n",
    "        print(momentum_impact)\n",
    "    else:\n",
    "        print(\"   No SGD results available yet\")\n",
    "\n",
    "    \n",
    "print(units_impact)\n",
    "\n",
    "    print(\"\\nğŸ“Š Average Validation Accuracy by RNN Units:\")\n",
    "print(\"\\nğŸ“Š Average Validation Accuracy by Momentum (SGD & RMSprop only):\")units_impact = results_df.groupby('rnn_units')['val_acc'].agg(['mean', 'std', 'max']).sort_values('mean', ascending=False)\n",
    "\n",
    "    units_impact = results_df.groupby('rnn_units')['val_acc'].agg(['mean', 'std', 'max']).sort_values('mean', ascending=False)\n",
    "momentum_impact = results_df[results_df['optimizer'] != 'adam'].groupby('momentum')['val_acc'].agg(['mean', 'std', 'max']).sort_values('mean', ascending=False)print(\"\\nğŸ“Š Average Validation Accuracy by RNN Units:\")\n",
    "\n",
    "    print(units_impact)\n",
    "print(momentum_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cfe15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# SAVE COMBINED RESULTS TO CSV (LEGACY FORMAT)\n",
    "# NOTE: Run the \"COMBINE ALL RESULTS\" cell first!\n",
    "# This cell is optional - results are already saved in individual CSVs\n",
    "# ====================================================================\n",
    "\n",
    "if 'results_df' not in globals():\n",
    "    print(\"âš ï¸  ERROR: results_df not found!\")\n",
    "    print(\"   Please run the 'COMBINE ALL RESULTS' cell first.\")\n",
    "else:\n",
    "\n",
    "    # Save hyperparameter tuning results to CSV (legacy filename)    print(\"   - rnn_chunked_hyperparameter_results_ALL.csv (combined)\")\n",
    "\n",
    "    results_csv = results_df.drop(columns=['confusion_matrix'])  # Drop confusion matrix for CSV export    print(\"   - rnn_chunked_BiGRU_FULL.csv\")\n",
    "\n",
    "    results_csv.to_csv('rnn_hyperparameter_results.csv', index=False)    \n",
    "    print(\"   - rnn_chunked_BiLSTM_FULL.csv\")\n",
    "\n",
    "    print(\"   - rnn_chunked_GRU_FULL.csv\")\n",
    "\n",
    "    print(\"âœ… Results saved to 'rnn_hyperparameter_results.csv' (legacy format)\")    \n",
    "    print(\"   - rnn_chunked_LSTM_FULL.csv\")\n",
    "\n",
    "    print(f\"   Total configurations tested: {len(results_csv)}\")    \n",
    "    print(\"\\nğŸ’¡ Note: Individual RNN type results are already saved as:\")\n",
    "\n",
    "    print(f\"   Best validation accuracy: {results_csv['val_acc'].max():.4f}\")    \n",
    "    print(f\"   Std validation accuracy: {results_csv['val_acc'].std():.4f}\")\n",
    "\n",
    "    print(f\"   Worst validation accuracy: {results_csv['val_acc'].min():.4f}\")    \n",
    "    print(f\"   Mean validation accuracy: {results_csv['val_acc'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f801858",
   "metadata": {},
   "source": [
    "### 3.2 Train Final Model with Best Hyperparameters\n",
    "\n",
    "Now we'll train the final model using the best hyperparameters found during tuning, with:\n",
    "- More epochs\n",
    "- Early stopping\n",
    "- Model checkpointing\n",
    "- Weights & Biases logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# FINAL BiGRU MODEL - BEST HYPERPARAMETERS (FIXED VERSION V2)\n",
    "# Train on full dataset (train + validation) for production\n",
    "# ====================================================================\n",
    "\n",
    "# Model configuration from best W&B run\n",
    "FINAL_MODEL_NAME = \"BiGRU_FINAL_adam_tanh_256\"\n",
    "FINAL_RNN_TYPE = 'BiGRU'\n",
    "FINAL_OPTIMIZER = 'adam'\n",
    "FINAL_ACTIVATION = 'tanh'\n",
    "FINAL_DROPOUT = 0.2\n",
    "FINAL_RNN_UNITS = 256\n",
    "FINAL_BATCH_NORM = True\n",
    "FINAL_LEARNING_RATE = 0.001\n",
    "\n",
    "# Extended training configuration for final model\n",
    "FINAL_EPOCHS = 110  # Extended from 50\n",
    "FINAL_PATIENCE = 15  # Extended from 6\n",
    "FINAL_BATCH_SIZE = 64\n",
    "FINAL_VAL_SPLIT = 0.2  # Use 20% of combined data for validation monitoring\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"ğŸš€ TRAINING FINAL MODEL: {FINAL_MODEL_NAME}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“‹ CONFIGURATION:\")\n",
    "print(f\"   Architecture: {FINAL_RNN_TYPE}\")\n",
    "print(f\"   Optimizer: {FINAL_OPTIMIZER}\")\n",
    "print(f\"   Activation: {FINAL_ACTIVATION}\")\n",
    "print(f\"   Dropout: {FINAL_DROPOUT}\")\n",
    "print(f\"   RNN Units: {FINAL_RNN_UNITS}\")\n",
    "print(f\"   Batch Norm: {FINAL_BATCH_NORM}\")\n",
    "print(f\"   Learning Rate: {FINAL_LEARNING_RATE}\")\n",
    "print(f\"   Max Epochs: {FINAL_EPOCHS}\")\n",
    "print(f\"   Early Stop Patience: {FINAL_PATIENCE}\")\n",
    "print(f\"   Batch Size: {FINAL_BATCH_SIZE}\")\n",
    "print(f\"   Validation Split: {FINAL_VAL_SPLIT}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine train and validation data for final training\n",
    "print(\"\\nğŸ”„ Combining train and validation datasets...\")\n",
    "\n",
    "# Concatenate train and validation\n",
    "X_full = np.concatenate([X_tr_embeddings_chunked, X_val_embeddings_chunked], axis=0)\n",
    "y_full = np.concatenate([y_tr_indexed, y_val_indexed], axis=0)\n",
    "\n",
    "print(f\"âœ“ Full combined set: {X_full.shape[0]} samples\")\n",
    "print(f\"   Original train: {X_tr_embeddings_chunked.shape[0]} samples\")\n",
    "print(f\"   Original validation: {X_val_embeddings_chunked.shape[0]} samples\")\n",
    "\n",
    "# Shuffle combined data\n",
    "shuffle_indices = np.random.RandomState(seed=SEED).permutation(len(X_full))\n",
    "X_full = X_full[shuffle_indices]\n",
    "y_full = y_full[shuffle_indices]\n",
    "\n",
    "# Split into new train/val for monitoring\n",
    "split_idx = int(len(X_full) * (1 - FINAL_VAL_SPLIT))\n",
    "X_train_final = X_full[:split_idx]\n",
    "y_train_final = y_full[:split_idx]\n",
    "X_val_final = X_full[split_idx:]\n",
    "y_val_final = y_full[split_idx:]\n",
    "\n",
    "print(f\"\\nğŸ“Š New split for final training:\")\n",
    "print(f\"   Training: {X_train_final.shape[0]} samples ({(1-FINAL_VAL_SPLIT)*100:.0f}%)\")\n",
    "print(f\"   Validation: {X_val_final.shape[0]} samples ({FINAL_VAL_SPLIT*100:.0f}%)\")\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_ds_final = tf.data.Dataset.from_tensor_slices((X_train_final, y_train_final))\n",
    "train_ds_final = train_ds_final.shuffle(buffer_size=10000, seed=SEED).batch(FINAL_BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds_final = tf.data.Dataset.from_tensor_slices((X_val_final, y_val_final))\n",
    "val_ds_final = val_ds_final.batch(FINAL_BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ Building final {FINAL_RNN_TYPE} model...\")\n",
    "\n",
    "# Build final model\n",
    "final_model = build_rnn_model(\n",
    "    rnn_type=FINAL_RNN_TYPE,\n",
    "    rnn_units=FINAL_RNN_UNITS,\n",
    "    activation=FINAL_ACTIVATION,\n",
    "    dropout=FINAL_DROPOUT,\n",
    "    use_batch_norm=FINAL_BATCH_NORM\n",
    ")\n",
    "\n",
    "# Compile with best optimizer\n",
    "final_optimizer = tf.keras.optimizers.Adam(learning_rate=FINAL_LEARNING_RATE)\n",
    "\n",
    "final_model.compile(\n",
    "    optimizer=final_optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']  # REMOVED AUC to simplify\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nğŸ“Š Model Architecture:\")\n",
    "final_model.summary()\n",
    "\n",
    "# Initialize W&B for final training\n",
    "print(\"\\nğŸ”— Initializing Weights & Biases tracking...\")\n",
    "final_run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    entity=WANDB_ENTITY,\n",
    "    name=FINAL_MODEL_NAME,\n",
    "    config={\n",
    "        \"model_type\": \"FINAL_PRODUCTION\",\n",
    "        \"architecture\": FINAL_RNN_TYPE,\n",
    "        \"optimizer\": FINAL_OPTIMIZER,\n",
    "        \"learning_rate\": FINAL_LEARNING_RATE,\n",
    "        \"activation\": FINAL_ACTIVATION,\n",
    "        \"dropout\": FINAL_DROPOUT,\n",
    "        \"rnn_units\": FINAL_RNN_UNITS,\n",
    "        \"batch_norm\": FINAL_BATCH_NORM,\n",
    "        \"epochs\": FINAL_EPOCHS,\n",
    "        \"patience\": FINAL_PATIENCE,\n",
    "        \"batch_size\": FINAL_BATCH_SIZE,\n",
    "        \"training_samples\": X_train_final.shape[0],\n",
    "        \"validation_samples\": X_val_final.shape[0],\n",
    "        \"val_split\": FINAL_VAL_SPLIT,\n",
    "        \"seed\": SEED,\n",
    "        \"class_weights\": \"none\",  # Disabled for testing\n",
    "        \"data\": \"train+validation_reshuffled\"\n",
    "    },\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop_final = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=FINAL_PATIENCE,\n",
    "    mode='min',\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Model checkpoint to save best model\n",
    "checkpoint_path = f'models/{FINAL_MODEL_NAME}_best.keras'\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "wandb_callback = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "\n",
    "# Train final model\n",
    "print(f\"\\nğŸ¯ Training final model on {X_train_final.shape[0]} samples...\")\n",
    "print(f\"   Max epochs: {FINAL_EPOCHS}\")\n",
    "print(f\"   Early stopping patience: {FINAL_PATIENCE}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "final_history = final_model.fit(\n",
    "    train_ds_final,\n",
    "    validation_data=val_ds_final,\n",
    "    epochs=FINAL_EPOCHS,\n",
    "    # REMOVED class_weight to test\n",
    "    callbacks=[early_stop_final, checkpoint_callback, wandb_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… FINAL MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get final metrics\n",
    "final_train_acc = final_history.history['accuracy'][-1]\n",
    "final_val_acc = final_history.history['val_accuracy'][-1]\n",
    "final_train_loss = final_history.history['loss'][-1]\n",
    "final_val_loss = final_history.history['val_loss'][-1]\n",
    "epochs_trained = len(final_history.history['accuracy'])\n",
    "\n",
    "best_epoch = np.argmax(final_history.history['val_accuracy']) + 1\n",
    "best_val_acc = np.max(final_history.history['val_accuracy'])\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Training Summary:\")\n",
    "print(f\"   Total epochs trained: {epochs_trained}\")\n",
    "print(f\"   Best epoch: {best_epoch}\")\n",
    "print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"   Final train accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"   Final validation accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"   Final train loss: {final_train_loss:.4f}\")\n",
    "print(f\"   Final validation loss: {final_val_loss:.4f}\")\n",
    "print(f\"   Training time: {training_time/60:.2f} minutes\")\n",
    "print(f\"   Model saved to: {checkpoint_path}\")\n",
    "\n",
    "# ====================================================================\n",
    "# EVALUATE ON TEST SET\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§ª EVALUATING ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model\n",
    "print(f\"\\nğŸ“‚ Loading best model from: {checkpoint_path}\")\n",
    "best_final_model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(f\"\\nğŸ“Š Evaluating on test set ({X_test_embeddings_chunked.shape[0]} samples)...\")\n",
    "test_loss, test_acc = best_final_model.evaluate(test_ds_chunked, verbose=1)\n",
    "\n",
    "print(f\"\\nğŸ¯ TEST SET RESULTS:\")\n",
    "print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Generate predictions on test set\n",
    "print(f\"\\nğŸ”® Generating predictions on test set...\")\n",
    "test_predictions = best_final_model.predict(test_ds_chunked, verbose=1)\n",
    "test_pred_classes = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Save predictions\n",
    "predictions_file = f'{FINAL_MODEL_NAME}_test_predictions.csv'\n",
    "test_df = pd.DataFrame({\n",
    "    'prediction': test_pred_classes,\n",
    "    'confidence': np.max(test_predictions, axis=1)\n",
    "})\n",
    "test_df.to_csv(predictions_file, index=False)\n",
    "print(f\"âœ“ Predictions saved to: {predictions_file}\")\n",
    "\n",
    "# Log final results to W&B\n",
    "wandb.log({\n",
    "    \"final/best_epoch\": best_epoch,\n",
    "    \"final/best_val_acc\": best_val_acc,\n",
    "    \"final/test_acc\": test_acc,\n",
    "    \"final/test_loss\": test_loss,\n",
    "    \"final/training_time_minutes\": training_time/60\n",
    "})\n",
    "\n",
    "# ====================================================================\n",
    "# PLOT TRAINING HISTORY\n",
    "# ====================================================================\n",
    "print(\"\\nğŸ“Š Creating training history plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0].plot(final_history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(final_history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0].axvline(x=best_epoch-1, color='red', linestyle='--', alpha=0.5, label=f'Best Epoch ({best_epoch})')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title(f'{FINAL_MODEL_NAME} - Training Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "axes[1].plot(final_history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(final_history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1].axvline(x=best_epoch-1, color='red', linestyle='--', alpha=0.5, label=f'Best Epoch ({best_epoch})')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title(f'{FINAL_MODEL_NAME} - Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_filename = f'{FINAL_MODEL_NAME}_training_history.png'\n",
    "plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"âœ“ Training history plot saved to: {plot_filename}\")\n",
    "\n",
    "# Finish W&B run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ ALL DONE! FINAL MODEL TRAINING AND EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“¦ Outputs:\")\n",
    "print(f\"   â€¢ Model: {checkpoint_path}\")\n",
    "print(f\"   â€¢ Predictions: {predictions_file}\")\n",
    "print(f\"   â€¢ Training plot: {plot_filename}\")\n",
    "print(f\"\\nğŸ† FINAL TEST ACCURACY: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31132731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ TRAINING FINAL MODEL: BiGRU_FINAL_adam_tanh_256_FULLDATA\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ CONFIGURATION:\n",
      "   Architecture: BiGRU\n",
      "   Optimizer: adam\n",
      "   Activation: tanh\n",
      "   Dropout: 0.2\n",
      "   RNN Units: 256\n",
      "   Batch Norm: True\n",
      "   Learning Rate: 0.001\n",
      "   Epochs: 100\n",
      "   Batch Size: 64\n",
      "================================================================================\n",
      "\n",
      "ğŸ”„ Combining train and validation datasets...\n",
      "âœ“ Full training set: 30850 samples\n",
      "   Original train: 24680 samples\n",
      "   Original validation: 6170 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 06:29:43.347190: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 758169600 exceeds 10% of free system memory.\n",
      "2025-12-05 06:29:43.978140: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 758169600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—ï¸ Building final BiGRU model...\n",
      "\n",
      "ğŸ“Š Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"BiGRU_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"BiGRU_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bigru_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,969,152</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_norm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ masking (\u001b[38;5;33mMasking\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bigru_layer (\u001b[38;5;33mBidirectional\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚     \u001b[38;5;34m1,969,152\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_norm (\u001b[38;5;33mBatchNormalization\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚         \u001b[38;5;34m2,048\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_hidden (\u001b[38;5;33mDense\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m32,832\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              â”‚           \u001b[38;5;34m325\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,004,357</span> (7.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,004,357\u001b[0m (7.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,003,333</span> (7.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,003,333\u001b[0m (7.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> (4.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,024\u001b[0m (4.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”— Initializing Weights & Biases tracking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guldmand/DataScience/ml/DS807_AppliedML/Assignment4_RNN_classification/group/jannik/wandb/run-20251205_062944-f0gcmkvi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked/runs/f0gcmkvi' target=\"_blank\">BiGRU_FINAL_adam_tanh_256_FULLDATA</a></strong> to <a href='https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked' target=\"_blank\">https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked/runs/f0gcmkvi' target=\"_blank\">https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked/runs/f0gcmkvi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Training final model on 30850 samples...\n",
      "   Epochs: 100\n",
      "================================================================================\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 06:29:45.781374: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 758169600 exceeds 10% of free system memory.\n",
      "I0000 00:00:1764912590.278636   19503 service.cc:148] XLA service 0x5bd496f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1764912590.278672   19503 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2025-12-05 06:29:50.475859: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1764912590.278636   19503 service.cc:148] XLA service 0x5bd496f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1764912590.278672   19503 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2025-12-05 06:29:50.475859: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1764912591.085136   19503 cuda_dnn.cc:529] Loaded cuDNN version 91501\n",
      "I0000 00:00:1764912591.085136   19503 cuda_dnn.cc:529] Loaded cuDNN version 91501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 36/483\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3654 - loss: 1.5936"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764912595.483355   19503 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5832 - loss: 1.1159\n",
      "Epoch 1: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 1: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6447 - loss: 0.9753\n",
      "Epoch 2/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6447 - loss: 0.9753\n",
      "Epoch 2/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6963 - loss: 0.8139\n",
      "Epoch 2: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 2: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7057 - loss: 0.7942\n",
      "Epoch 3/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7057 - loss: 0.7942\n",
      "Epoch 3/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7217 - loss: 0.7377\n",
      "Epoch 3: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 3: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7235 - loss: 0.7253\n",
      "Epoch 4/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7235 - loss: 0.7253\n",
      "Epoch 4/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7425 - loss: 0.6547\n",
      "Epoch 4: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 4: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7417 - loss: 0.6440\n",
      "Epoch 5/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7417 - loss: 0.6440\n",
      "Epoch 5/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7471 - loss: 0.6173\n",
      "Epoch 5: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 5: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7555 - loss: 0.5946\n",
      "Epoch 6/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7555 - loss: 0.5946\n",
      "Epoch 6/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7580 - loss: 0.5452\n",
      "Epoch 6: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 6: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7661 - loss: 0.5351\n",
      "Epoch 7/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7661 - loss: 0.5351\n",
      "Epoch 7/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7808 - loss: 0.4817\n",
      "Epoch 7: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 7: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7794 - loss: 0.4846\n",
      "Epoch 8/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7794 - loss: 0.4846\n",
      "Epoch 8/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7903 - loss: 0.4540\n",
      "Epoch 8: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 8: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7891 - loss: 0.4494\n",
      "Epoch 9/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7891 - loss: 0.4494\n",
      "Epoch 9/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8072 - loss: 0.3882\n",
      "Epoch 9: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 9: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8039 - loss: 0.3976\n",
      "Epoch 10/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8039 - loss: 0.3976\n",
      "Epoch 10/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8026 - loss: 0.3898\n",
      "Epoch 10: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 10: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8075 - loss: 0.3832\n",
      "Epoch 11/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8075 - loss: 0.3832\n",
      "Epoch 11/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8199 - loss: 0.3693\n",
      "Epoch 11: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 11: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8194 - loss: 0.3614\n",
      "Epoch 12/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8194 - loss: 0.3614\n",
      "Epoch 12/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8362 - loss: 0.3194\n",
      "Epoch 12: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 12: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8285 - loss: 0.3252\n",
      "Epoch 13/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8285 - loss: 0.3252\n",
      "Epoch 13/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8324 - loss: 0.3262\n",
      "Epoch 13: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 13: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8342 - loss: 0.3241\n",
      "Epoch 14/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8342 - loss: 0.3241\n",
      "Epoch 14/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8359 - loss: 0.2900\n",
      "Epoch 14: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 14: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8355 - loss: 0.3008\n",
      "Epoch 15/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8355 - loss: 0.3008\n",
      "Epoch 15/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8425 - loss: 0.2898\n",
      "Epoch 15: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 15: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8432 - loss: 0.2877\n",
      "Epoch 16/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8432 - loss: 0.2877\n",
      "Epoch 16/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8489 - loss: 0.2791\n",
      "Epoch 16: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 16: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8485 - loss: 0.2803\n",
      "Epoch 17/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8485 - loss: 0.2803\n",
      "Epoch 17/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8451 - loss: 0.2842\n",
      "Epoch 17: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 17: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8533 - loss: 0.2664\n",
      "Epoch 18/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8533 - loss: 0.2664\n",
      "Epoch 18/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8416 - loss: 0.3003\n",
      "Epoch 18: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 18: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8484 - loss: 0.2795\n",
      "Epoch 19/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8484 - loss: 0.2795\n",
      "Epoch 19/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8464 - loss: 0.2904\n",
      "Epoch 19: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 19: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8551 - loss: 0.2619\n",
      "Epoch 20/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8551 - loss: 0.2619\n",
      "Epoch 20/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8713 - loss: 0.2270\n",
      "Epoch 20: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 20: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8725 - loss: 0.2225\n",
      "Epoch 21/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8725 - loss: 0.2225\n",
      "Epoch 21/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8731 - loss: 0.2170\n",
      "Epoch 21: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 21: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8712 - loss: 0.2183\n",
      "Epoch 22/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8712 - loss: 0.2183\n",
      "Epoch 22/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8697 - loss: 0.2328\n",
      "Epoch 22: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 22: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8712 - loss: 0.2331\n",
      "Epoch 23/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8712 - loss: 0.2331\n",
      "Epoch 23/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8633 - loss: 0.2326\n",
      "Epoch 23: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 23: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8707 - loss: 0.2266\n",
      "Epoch 24/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8707 - loss: 0.2266\n",
      "Epoch 24/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8422 - loss: 0.3329\n",
      "Epoch 24: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 24: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8577 - loss: 0.2840\n",
      "Epoch 25/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8577 - loss: 0.2840\n",
      "Epoch 25/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8838 - loss: 0.2015\n",
      "Epoch 25: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 25: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8851 - loss: 0.1972\n",
      "Epoch 26/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8851 - loss: 0.1972\n",
      "Epoch 26/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8881 - loss: 0.1871\n",
      "Epoch 26: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 26: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8876 - loss: 0.1898\n",
      "Epoch 27/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8876 - loss: 0.1898\n",
      "Epoch 27/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8769 - loss: 0.2258\n",
      "Epoch 27: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 27: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8815 - loss: 0.2118\n",
      "Epoch 28/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8815 - loss: 0.2118\n",
      "Epoch 28/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8966 - loss: 0.1700\n",
      "Epoch 28: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 28: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8962 - loss: 0.1732\n",
      "Epoch 29/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8962 - loss: 0.1732\n",
      "Epoch 29/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8945 - loss: 0.1846\n",
      "Epoch 29: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8945 - loss: 0.1846\n",
      "Epoch 29: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8991 - loss: 0.1778\n",
      "Epoch 30/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8991 - loss: 0.1778\n",
      "Epoch 30/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8671 - loss: 0.2431\n",
      "Epoch 30: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8671 - loss: 0.2431\n",
      "Epoch 30: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8800 - loss: 0.2200\n",
      "Epoch 31/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8800 - loss: 0.2200\n",
      "Epoch 31/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8768 - loss: 0.2244\n",
      "Epoch 31: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 31: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8854 - loss: 0.2038\n",
      "Epoch 32/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8854 - loss: 0.2038\n",
      "Epoch 32/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9061 - loss: 0.1575\n",
      "Epoch 32: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 32: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9072 - loss: 0.1547\n",
      "Epoch 33/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9072 - loss: 0.1547\n",
      "Epoch 33/100\n",
      "\u001b[1m472/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9143 - loss: 0.1453\n",
      "Epoch 33: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 33: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9112 - loss: 0.1504\n",
      "Epoch 34/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9112 - loss: 0.1504\n",
      "Epoch 34/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9158 - loss: 0.1395\n",
      "Epoch 34: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 34: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9171 - loss: 0.1358\n",
      "Epoch 35/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9171 - loss: 0.1358\n",
      "Epoch 35/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8523 - loss: 0.3271\n",
      "Epoch 35: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 35: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8671 - loss: 0.2717\n",
      "Epoch 36/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8671 - loss: 0.2717\n",
      "Epoch 36/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8988 - loss: 0.1654\n",
      "Epoch 36: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 36: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9046 - loss: 0.1629\n",
      "Epoch 37/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9046 - loss: 0.1629\n",
      "Epoch 37/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9152 - loss: 0.1403\n",
      "Epoch 37: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 37: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9153 - loss: 0.1438\n",
      "Epoch 38/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9153 - loss: 0.1438\n",
      "Epoch 38/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9224 - loss: 0.1368\n",
      "Epoch 38: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 38: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9192 - loss: 0.1367\n",
      "Epoch 39/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9192 - loss: 0.1367\n",
      "Epoch 39/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9292 - loss: 0.1118\n",
      "Epoch 39: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 39: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9251 - loss: 0.1242\n",
      "Epoch 40/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9251 - loss: 0.1242\n",
      "Epoch 40/100\n",
      "\u001b[1m472/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9197 - loss: 0.1403\n",
      "Epoch 40: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 40: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9206 - loss: 0.1378\n",
      "Epoch 41/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9206 - loss: 0.1378\n",
      "Epoch 41/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9253 - loss: 0.1239\n",
      "Epoch 41: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 41: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9284 - loss: 0.1166\n",
      "Epoch 42/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9284 - loss: 0.1166\n",
      "Epoch 42/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9275 - loss: 0.1297\n",
      "Epoch 42: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 42: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9238 - loss: 0.1399\n",
      "Epoch 43/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9238 - loss: 0.1399\n",
      "Epoch 43/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9169 - loss: 0.1485\n",
      "Epoch 43: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 43: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9232 - loss: 0.1312\n",
      "Epoch 44/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9232 - loss: 0.1312\n",
      "Epoch 44/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9276 - loss: 0.1265\n",
      "Epoch 44: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 44: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9286 - loss: 0.1249\n",
      "Epoch 45/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9286 - loss: 0.1249\n",
      "Epoch 45/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9120 - loss: 0.1618\n",
      "Epoch 45: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9120 - loss: 0.1618\n",
      "Epoch 45: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9219 - loss: 0.1369\n",
      "Epoch 46/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9219 - loss: 0.1369\n",
      "Epoch 46/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9235 - loss: 0.1316\n",
      "Epoch 46: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 46: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9268 - loss: 0.1322\n",
      "Epoch 47/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9268 - loss: 0.1322\n",
      "Epoch 47/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9242 - loss: 0.1322\n",
      "Epoch 47: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 47: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9293 - loss: 0.1191\n",
      "Epoch 48/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9293 - loss: 0.1191\n",
      "Epoch 48/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9364 - loss: 0.1134\n",
      "Epoch 48: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9364 - loss: 0.1134\n",
      "Epoch 48: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9323 - loss: 0.1244\n",
      "Epoch 49/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9323 - loss: 0.1244\n",
      "Epoch 49/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9202 - loss: 0.1507\n",
      "Epoch 49: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 49: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9261 - loss: 0.1393\n",
      "Epoch 50/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9261 - loss: 0.1393\n",
      "Epoch 50/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9195 - loss: 0.1430\n",
      "Epoch 50: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 50: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9289 - loss: 0.1241\n",
      "Epoch 51/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9289 - loss: 0.1241\n",
      "Epoch 51/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9431 - loss: 0.0921\n",
      "Epoch 51: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 51: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9439 - loss: 0.0947\n",
      "Epoch 52/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9439 - loss: 0.0947\n",
      "Epoch 52/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9444 - loss: 0.0938\n",
      "Epoch 52: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 52: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9442 - loss: 0.0946\n",
      "Epoch 53/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9442 - loss: 0.0946\n",
      "Epoch 53/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9419 - loss: 0.1078\n",
      "Epoch 53: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 53: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9393 - loss: 0.1100\n",
      "Epoch 54/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9393 - loss: 0.1100\n",
      "Epoch 54/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9389 - loss: 0.1100\n",
      "Epoch 54: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 54: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9411 - loss: 0.1079\n",
      "Epoch 55/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9411 - loss: 0.1079\n",
      "Epoch 55/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9441 - loss: 0.1122\n",
      "Epoch 55: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 55: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9404 - loss: 0.1175\n",
      "Epoch 56/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9404 - loss: 0.1175\n",
      "Epoch 56/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9413 - loss: 0.1107\n",
      "Epoch 56: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 56: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9412 - loss: 0.1067\n",
      "Epoch 57/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9412 - loss: 0.1067\n",
      "Epoch 57/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9173 - loss: 0.1725\n",
      "Epoch 57: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 57: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9259 - loss: 0.1422\n",
      "Epoch 58/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9259 - loss: 0.1422\n",
      "Epoch 58/100\n",
      "\u001b[1m472/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9433 - loss: 0.1074\n",
      "Epoch 58: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 58: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9433 - loss: 0.1054\n",
      "Epoch 59/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9433 - loss: 0.1054\n",
      "Epoch 59/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9492 - loss: 0.0902\n",
      "Epoch 59: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 59: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9499 - loss: 0.0902\n",
      "Epoch 60/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9499 - loss: 0.0902\n",
      "Epoch 60/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9503 - loss: 0.0884\n",
      "Epoch 60: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 60: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9503 - loss: 0.0884\n",
      "Epoch 61/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9503 - loss: 0.0884\n",
      "Epoch 61/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9581 - loss: 0.0745\n",
      "Epoch 61: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 61: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9545 - loss: 0.0774\n",
      "Epoch 62/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9545 - loss: 0.0774\n",
      "Epoch 62/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9586 - loss: 0.0719\n",
      "Epoch 62: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 62: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9547 - loss: 0.0815\n",
      "Epoch 63/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9547 - loss: 0.0815\n",
      "Epoch 63/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9266 - loss: 0.1479\n",
      "Epoch 63: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 63: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9306 - loss: 0.1452\n",
      "Epoch 64/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9306 - loss: 0.1452\n",
      "Epoch 64/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9458 - loss: 0.1066\n",
      "Epoch 64: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 64: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9482 - loss: 0.1029\n",
      "Epoch 65/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9482 - loss: 0.1029\n",
      "Epoch 65/100\n",
      "\u001b[1m472/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9487 - loss: 0.0923\n",
      "Epoch 65: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 65: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9499 - loss: 0.0937\n",
      "Epoch 66/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9499 - loss: 0.0937\n",
      "Epoch 66/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9493 - loss: 0.0956\n",
      "Epoch 66: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 66: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9518 - loss: 0.0921\n",
      "Epoch 67/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9518 - loss: 0.0921\n",
      "Epoch 67/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9394 - loss: 0.1294\n",
      "Epoch 67: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9394 - loss: 0.1294\n",
      "Epoch 67: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9404 - loss: 0.1242\n",
      "Epoch 68/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9404 - loss: 0.1242\n",
      "Epoch 68/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9492 - loss: 0.0904\n",
      "Epoch 68: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9492 - loss: 0.0904\n",
      "Epoch 68: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9505 - loss: 0.0864\n",
      "Epoch 69/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9505 - loss: 0.0864\n",
      "Epoch 69/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9424 - loss: 0.1028\n",
      "Epoch 69: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 69: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9470 - loss: 0.0950\n",
      "Epoch 70/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9470 - loss: 0.0950\n",
      "Epoch 70/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9583 - loss: 0.0764\n",
      "Epoch 70: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 70: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9566 - loss: 0.0806\n",
      "Epoch 71/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9566 - loss: 0.0806\n",
      "Epoch 71/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9588 - loss: 0.0739\n",
      "Epoch 71: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 71: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9581 - loss: 0.0768\n",
      "Epoch 72/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9581 - loss: 0.0768\n",
      "Epoch 72/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9576 - loss: 0.0875\n",
      "Epoch 72: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 72: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9528 - loss: 0.0954\n",
      "Epoch 73/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9528 - loss: 0.0954\n",
      "Epoch 73/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9565 - loss: 0.0811\n",
      "Epoch 73: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 73: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9541 - loss: 0.0871\n",
      "Epoch 74/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9541 - loss: 0.0871\n",
      "Epoch 74/100\n",
      "\u001b[1m471/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9470 - loss: 0.1093\n",
      "Epoch 74: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 74: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9505 - loss: 0.0949\n",
      "Epoch 75/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9505 - loss: 0.0949\n",
      "Epoch 75/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9592 - loss: 0.0716\n",
      "Epoch 75: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 75: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9607 - loss: 0.0684\n",
      "Epoch 76/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9607 - loss: 0.0684\n",
      "Epoch 76/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9586 - loss: 0.0781\n",
      "Epoch 76: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 76: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9572 - loss: 0.0797\n",
      "Epoch 77/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9572 - loss: 0.0797\n",
      "Epoch 77/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9563 - loss: 0.0860\n",
      "Epoch 77: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 77: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9590 - loss: 0.0793\n",
      "Epoch 78/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9590 - loss: 0.0793\n",
      "Epoch 78/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9583 - loss: 0.0798\n",
      "Epoch 78: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 78: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9583 - loss: 0.0795\n",
      "Epoch 79/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9583 - loss: 0.0795\n",
      "Epoch 79/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9641 - loss: 0.0670\n",
      "Epoch 79: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 79: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9648 - loss: 0.0635\n",
      "Epoch 80/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9648 - loss: 0.0635\n",
      "Epoch 80/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9444 - loss: 0.1167\n",
      "Epoch 80: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 80: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9513 - loss: 0.0963\n",
      "Epoch 81/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9513 - loss: 0.0963\n",
      "Epoch 81/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9591 - loss: 0.0747\n",
      "Epoch 81: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 81: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9595 - loss: 0.0779\n",
      "Epoch 82/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9595 - loss: 0.0779\n",
      "Epoch 82/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9584 - loss: 0.0901\n",
      "Epoch 82: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 82: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9616 - loss: 0.0772\n",
      "Epoch 83/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9616 - loss: 0.0772\n",
      "Epoch 83/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9597 - loss: 0.0931\n",
      "Epoch 83: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 83: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9594 - loss: 0.0849\n",
      "Epoch 84/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9594 - loss: 0.0849\n",
      "Epoch 84/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9554 - loss: 0.0865\n",
      "Epoch 84: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 84: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9545 - loss: 0.0936\n",
      "Epoch 85/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9545 - loss: 0.0936\n",
      "Epoch 85/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9583 - loss: 0.0814\n",
      "Epoch 85: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 85: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9596 - loss: 0.0766\n",
      "Epoch 86/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9596 - loss: 0.0766\n",
      "Epoch 86/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9696 - loss: 0.0612\n",
      "Epoch 86: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 86: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9695 - loss: 0.0594\n",
      "Epoch 87/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9695 - loss: 0.0594\n",
      "Epoch 87/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9689 - loss: 0.0567\n",
      "Epoch 87: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9689 - loss: 0.0567\n",
      "Epoch 87: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9675 - loss: 0.0632\n",
      "Epoch 88/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9675 - loss: 0.0632\n",
      "Epoch 88/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9627 - loss: 0.0816\n",
      "Epoch 88: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 88: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9619 - loss: 0.0753\n",
      "Epoch 89/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9619 - loss: 0.0753\n",
      "Epoch 89/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9616 - loss: 0.0769\n",
      "Epoch 89: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 89: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9614 - loss: 0.0826\n",
      "Epoch 90/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9614 - loss: 0.0826\n",
      "Epoch 90/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9647 - loss: 0.0648\n",
      "Epoch 90: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 90: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.0623\n",
      "Epoch 91/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.0623\n",
      "Epoch 91/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9667 - loss: 0.0731\n",
      "Epoch 91: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 91: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9620 - loss: 0.0799\n",
      "Epoch 92/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9620 - loss: 0.0799\n",
      "Epoch 92/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9666 - loss: 0.0655\n",
      "Epoch 92: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 92: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.0599\n",
      "Epoch 93/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.0599\n",
      "Epoch 93/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9684 - loss: 0.0582\n",
      "Epoch 93: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 93: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9666 - loss: 0.0625\n",
      "Epoch 94/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9666 - loss: 0.0625\n",
      "Epoch 94/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9621 - loss: 0.0777\n",
      "Epoch 94: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 94: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9632 - loss: 0.0784\n",
      "Epoch 95/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9632 - loss: 0.0784\n",
      "Epoch 95/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9387 - loss: 0.1257\n",
      "Epoch 95: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9387 - loss: 0.1257\n",
      "Epoch 95: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9471 - loss: 0.1038\n",
      "Epoch 96/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9471 - loss: 0.1038\n",
      "Epoch 96/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9621 - loss: 0.0736\n",
      "Epoch 96: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 96: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9621 - loss: 0.0745\n",
      "Epoch 97/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9621 - loss: 0.0745\n",
      "Epoch 97/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9658 - loss: 0.0657\n",
      "Epoch 97: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 97: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9684 - loss: 0.0597\n",
      "Epoch 98/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9684 - loss: 0.0597\n",
      "Epoch 98/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9503 - loss: 0.0898\n",
      "Epoch 98: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 98: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9605 - loss: 0.0725\n",
      "Epoch 99/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9605 - loss: 0.0725\n",
      "Epoch 99/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9691 - loss: 0.0601\n",
      "Epoch 99: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 99: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9700 - loss: 0.0561\n",
      "Epoch 100/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9700 - loss: 0.0561\n",
      "Epoch 100/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9732 - loss: 0.0539\n",
      "Epoch 100: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 100: saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9718 - loss: 0.0551\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9718 - loss: 0.0551\n",
      "\n",
      "================================================================================\n",
      "âœ… FINAL MODEL TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ Training Summary:\n",
      "   Epochs trained: 100\n",
      "   Final train accuracy: 0.9718\n",
      "   Final train loss: 0.0551\n",
      "   Training time: 4.56 minutes\n",
      "   Model saved to: models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‚ Loading final model from: models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "================================================================================\n",
      "âœ… FINAL MODEL TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ Training Summary:\n",
      "   Epochs trained: 100\n",
      "   Final train accuracy: 0.9718\n",
      "   Final train loss: 0.0551\n",
      "   Training time: 4.56 minutes\n",
      "   Model saved to: models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‚ Loading final model from: models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "ğŸ“Š Evaluating on test set (3428 samples)...\n",
      "\n",
      "ğŸ“Š Evaluating on test set (3428 samples)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 162\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ“Š Evaluating on test set (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test_embeddings_chunked.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m test_loss, test_acc = \u001b[43mbest_final_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ds_chunked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ¯ TEST SET RESULTS:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    165\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/conda_envs/ml/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/conda_envs/ml/lib/python3.11/site-packages/optree/ops.py:766\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[39m\n\u001b[32m    764\u001b[39m leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[32m    765\u001b[39m flat_args = [leaves] + [treespec.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: None values not supported."
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# FINAL BiGRU MODEL - TRAIN ON FULL DATA (TRAIN + VAL COMBINED)\n",
    "# ====================================================================\n",
    "import uuid\n",
    "\n",
    "# Model configuration from best hyperparameters\n",
    "FINAL_MODEL_NAME = \"BiGRU_FINAL_adam_tanh_256_FULLDATA\"\n",
    "FINAL_RNN_TYPE = 'BiGRU'\n",
    "FINAL_OPTIMIZER = 'adam'\n",
    "FINAL_ACTIVATION = 'tanh'\n",
    "FINAL_DROPOUT = 0.2\n",
    "FINAL_RNN_UNITS = 256\n",
    "FINAL_BATCH_NORM = True\n",
    "FINAL_LEARNING_RATE = 0.001\n",
    "\n",
    "# Training configuration\n",
    "FINAL_EPOCHS = 100  # Based on tuning results showing convergence around 40-50 epochs\n",
    "FINAL_BATCH_SIZE = 64\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"ğŸš€ TRAINING FINAL MODEL: {FINAL_MODEL_NAME}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“‹ CONFIGURATION:\")\n",
    "print(f\"   Architecture: {FINAL_RNN_TYPE}\")\n",
    "print(f\"   Optimizer: {FINAL_OPTIMIZER}\")\n",
    "print(f\"   Activation: {FINAL_ACTIVATION}\")\n",
    "print(f\"   Dropout: {FINAL_DROPOUT}\")\n",
    "print(f\"   RNN Units: {FINAL_RNN_UNITS}\")\n",
    "print(f\"   Batch Norm: {FINAL_BATCH_NORM}\")\n",
    "print(f\"   Learning Rate: {FINAL_LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {FINAL_EPOCHS}\")\n",
    "print(f\"   Batch Size: {FINAL_BATCH_SIZE}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine train and validation data\n",
    "print(\"\\nğŸ”„ Combining train and validation datasets...\")\n",
    "X_full = np.concatenate([X_tr_embeddings_chunked, X_val_embeddings_chunked], axis=0)\n",
    "y_full = np.concatenate([y_tr_indexed, y_val_indexed], axis=0)\n",
    "\n",
    "print(f\"âœ“ Full training set: {X_full.shape[0]} samples\")\n",
    "print(f\"   Original train: {X_tr_embeddings_chunked.shape[0]} samples\")\n",
    "print(f\"   Original validation: {X_val_embeddings_chunked.shape[0]} samples\")\n",
    "\n",
    "# Create full dataset\n",
    "full_ds = tf.data.Dataset.from_tensor_slices((X_full, y_full))\n",
    "full_ds = full_ds.shuffle(buffer_size=10000, seed=SEED).batch(FINAL_BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ Building final {FINAL_RNN_TYPE} model...\")\n",
    "\n",
    "# Build model\n",
    "final_model = build_rnn_model(\n",
    "    rnn_type=FINAL_RNN_TYPE,\n",
    "    rnn_units=FINAL_RNN_UNITS,\n",
    "    activation=FINAL_ACTIVATION,\n",
    "    dropout=FINAL_DROPOUT,\n",
    "    use_batch_norm=FINAL_BATCH_NORM\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=FINAL_LEARNING_RATE)\n",
    "\n",
    "final_model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nğŸ“Š Model Architecture:\")\n",
    "final_model.summary()\n",
    "\n",
    "# Initialize W&B\n",
    "print(\"\\nğŸ”— Initializing Weights & Biases tracking...\")\n",
    "RUN_ID = uuid.uuid4().hex[:8]\n",
    "final_run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    entity=WANDB_ENTITY,\n",
    "    name=FINAL_MODEL_NAME,\n",
    "    config={\n",
    "        \"model_type\": \"FINAL_PRODUCTION\",\n",
    "        \"architecture\": FINAL_RNN_TYPE,\n",
    "        \"optimizer\": FINAL_OPTIMIZER,\n",
    "        \"learning_rate\": FINAL_LEARNING_RATE,\n",
    "        \"activation\": FINAL_ACTIVATION,\n",
    "        \"dropout\": FINAL_DROPOUT,\n",
    "        \"rnn_units\": FINAL_RNN_UNITS,\n",
    "        \"batch_norm\": FINAL_BATCH_NORM,\n",
    "        \"epochs\": FINAL_EPOCHS,\n",
    "        \"batch_size\": FINAL_BATCH_SIZE,\n",
    "        \"training_samples\": X_full.shape[0],\n",
    "        \"seed\": SEED,\n",
    "        \"class_weights\": \"balanced\",\n",
    "        \"data\": \"train+validation_combined\",\n",
    "        \"run_id\": RUN_ID\n",
    "    },\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = f'models/{FINAL_MODEL_NAME}_best.keras'\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_freq='epoch',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "wandb_callback = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "\n",
    "# Train model with class weights\n",
    "print(f\"\\nğŸ¯ Training final model on {X_full.shape[0]} samples...\")\n",
    "print(f\"   Epochs: {FINAL_EPOCHS}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = final_model.fit(\n",
    "    full_ds,\n",
    "    epochs=FINAL_EPOCHS,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[checkpoint_callback, wandb_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… FINAL MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get final metrics\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "train_loss = history.history['loss'][-1]\n",
    "epochs_trained = len(history.history['accuracy'])\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Training Summary:\")\n",
    "print(f\"   Epochs trained: {epochs_trained}\")\n",
    "print(f\"   Final train accuracy: {train_acc:.4f}\")\n",
    "print(f\"   Final train loss: {train_loss:.4f}\")\n",
    "print(f\"   Training time: {train_time/60:.2f} minutes\")\n",
    "print(f\"   Model saved to: {checkpoint_path}\")\n",
    "\n",
    "# Log to W&B\n",
    "wandb.run.summary[\"final_train_acc\"] = train_acc\n",
    "wandb.run.summary[\"final_train_loss\"] = train_loss\n",
    "wandb.run.summary[\"training_time_sec\"] = train_time\n",
    "\n",
    "# ====================================================================\n",
    "# EVALUATE ON TEST SET\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§ª EVALUATING ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load final model\n",
    "print(f\"\\nğŸ“‚ Loading final model from: {checkpoint_path}\")\n",
    "best_final_model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(f\"\\nğŸ“Š Evaluating on test set ({X_test_embeddings_chunked.shape[0]} samples)...\")\n",
    "test_loss, test_acc = best_final_model.evaluate(test_ds_chunked, verbose=1)\n",
    "\n",
    "print(f\"\\nğŸ¯ TEST SET RESULTS:\")\n",
    "print(f\"   Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Generate predictions on test set\n",
    "print(f\"\\nğŸ”® Generating predictions on test set...\")\n",
    "test_predictions = best_final_model.predict(test_ds_chunked, verbose=1)\n",
    "test_pred_classes = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Save predictions\n",
    "predictions_file = f'{FINAL_MODEL_NAME}_test_predictions.csv'\n",
    "test_df = pd.DataFrame({\n",
    "    'prediction': test_pred_classes,\n",
    "    'confidence': np.max(test_predictions, axis=1)\n",
    "})\n",
    "test_df.to_csv(predictions_file, index=False)\n",
    "print(f\"âœ“ Predictions saved to: {predictions_file}\")\n",
    "\n",
    "# Log final results to W&B\n",
    "wandb.run.summary[\"test_acc\"] = test_acc\n",
    "wandb.run.summary[\"test_loss\"] = test_loss\n",
    "\n",
    "# ====================================================================\n",
    "# PLOT TRAINING HISTORY\n",
    "# ====================================================================\n",
    "print(\"\\nğŸ“Š Creating training history plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2, color='blue')\n",
    "axes[0].axhline(y=test_acc, color='red', linestyle='--', alpha=0.7, label=f'Test Accuracy ({test_acc:.4f})')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title(f'{FINAL_MODEL_NAME} - Training Progress', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2, color='blue')\n",
    "axes[1].axhline(y=test_loss, color='red', linestyle='--', alpha=0.7, label=f'Test Loss ({test_loss:.4f})')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title(f'{FINAL_MODEL_NAME} - Loss Progress', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_filename = f'{FINAL_MODEL_NAME}_training_history.png'\n",
    "plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"âœ“ Training history plot saved to: {plot_filename}\")\n",
    "\n",
    "# Finish W&B run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ ALL DONE! FINAL MODEL TRAINING AND EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“¦ Outputs:\")\n",
    "print(f\"   â€¢ Model: {checkpoint_path}\")\n",
    "print(f\"   â€¢ Predictions: {predictions_file}\")\n",
    "print(f\"   â€¢ Training plot: {plot_filename}\")\n",
    "print(f\"\\nğŸ† FINAL RESULTS:\")\n",
    "print(f\"   Training Samples: {X_full.shape[0]}\")\n",
    "print(f\"   Train Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"   Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb5d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ TRAINING FINAL MODEL: BiGRU_FINAL_adam_tanh_256_FULLDATA\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ CONFIGURATION:\n",
      "   Architecture: BiGRU\n",
      "   Optimizer: adam\n",
      "   Activation: tanh\n",
      "   Dropout: 0\n",
      "   RNN Units: 256\n",
      "   Batch Norm: True\n",
      "   Learning Rate: 0.001\n",
      "   Epochs: 100\n",
      "   Batch Size: 64\n",
      "================================================================================\n",
      "\n",
      "ğŸ”„ Combining train and validation datasets...\n",
      "âœ“ Full training set: 30850 samples\n",
      "   Original train: 24680 samples\n",
      "   Original validation: 6170 samples\n",
      "âœ“ Full training set: 30850 samples\n",
      "   Original train: 24680 samples\n",
      "   Original validation: 6170 samples\n",
      "\n",
      "ğŸ—ï¸ Building final BiGRU model...\n",
      "\n",
      "ğŸ“Š Model Architecture:\n",
      "\n",
      "ğŸ—ï¸ Building final BiGRU model...\n",
      "\n",
      "ğŸ“Š Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"BiGRU_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"BiGRU_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bigru_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,969,152</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_norm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ masking (\u001b[38;5;33mMasking\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bigru_layer (\u001b[38;5;33mBidirectional\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚     \u001b[38;5;34m1,969,152\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_norm (\u001b[38;5;33mBatchNormalization\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚         \u001b[38;5;34m2,048\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_hidden (\u001b[38;5;33mDense\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m32,832\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              â”‚           \u001b[38;5;34m325\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,004,357</span> (7.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,004,357\u001b[0m (7.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,003,333</span> (7.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,003,333\u001b[0m (7.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> (4.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,024\u001b[0m (4.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”— Initializing Weights & Biases tracking...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guldmand/DataScience/ml/DS807_AppliedML/Assignment4_RNN_classification/group/jannik/wandb/run-20251205_071326-472hei05</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked/runs/472hei05' target=\"_blank\">BiGRU_FINAL_adam_tanh_256_FULLDATA</a></strong> to <a href='https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked' target=\"_blank\">https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked/runs/472hei05' target=\"_blank\">https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked/runs/472hei05</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Training final model on 30850 samples...\n",
      "   Epochs: 100\n",
      "   ğŸ’¡ ModelCheckpoint will save best model (lowest training loss)\n",
      "================================================================================\n",
      "Epoch 1/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6278 - loss: 1.0776\n",
      "Epoch 1: loss improved from None to 0.93289, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6278 - loss: 1.0776\n",
      "Epoch 1: loss improved from None to 0.93289, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.6718 - loss: 0.9329\n",
      "Epoch 2/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.6718 - loss: 0.9329\n",
      "Epoch 2/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7279 - loss: 0.7354\n",
      "Epoch 2: loss improved from 0.93289 to 0.72956, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 2: loss improved from 0.93289 to 0.72956, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7325 - loss: 0.7296\n",
      "Epoch 3/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7325 - loss: 0.7296\n",
      "Epoch 3/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7504 - loss: 0.6418\n",
      "Epoch 3: loss improved from 0.72956 to 0.63245, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7504 - loss: 0.6418\n",
      "Epoch 3: loss improved from 0.72956 to 0.63245, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7511 - loss: 0.6324\n",
      "Epoch 4/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7511 - loss: 0.6324\n",
      "Epoch 4/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7784 - loss: 0.5412\n",
      "Epoch 4: loss improved from 0.63245 to 0.52882, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 4: loss improved from 0.63245 to 0.52882, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7803 - loss: 0.5288\n",
      "Epoch 5/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7803 - loss: 0.5288\n",
      "Epoch 5/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7985 - loss: 0.4688\n",
      "Epoch 5: loss improved from 0.52882 to 0.45168, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 5: loss improved from 0.52882 to 0.45168, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8027 - loss: 0.4517\n",
      "Epoch 6/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8027 - loss: 0.4517\n",
      "Epoch 6/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8070 - loss: 0.4006\n",
      "Epoch 6: loss improved from 0.45168 to 0.39561, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 6: loss improved from 0.45168 to 0.39561, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8138 - loss: 0.3956\n",
      "Epoch 7/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8138 - loss: 0.3956\n",
      "Epoch 7/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8295 - loss: 0.3552\n",
      "Epoch 7: loss improved from 0.39561 to 0.36283, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 7: loss improved from 0.39561 to 0.36283, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8278 - loss: 0.3628\n",
      "Epoch 8/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8278 - loss: 0.3628\n",
      "Epoch 8/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8429 - loss: 0.3157\n",
      "Epoch 8: loss improved from 0.36283 to 0.30973, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 8: loss improved from 0.36283 to 0.30973, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8447 - loss: 0.3097\n",
      "Epoch 9/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8447 - loss: 0.3097\n",
      "Epoch 9/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8634 - loss: 0.2647\n",
      "Epoch 9: loss improved from 0.30973 to 0.27605, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 9: loss improved from 0.30973 to 0.27605, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8581 - loss: 0.2760\n",
      "Epoch 10/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8581 - loss: 0.2760\n",
      "Epoch 10/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8642 - loss: 0.2539\n",
      "Epoch 10: loss improved from 0.27605 to 0.25434, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 10: loss improved from 0.27605 to 0.25434, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8637 - loss: 0.2543\n",
      "Epoch 11/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8637 - loss: 0.2543\n",
      "Epoch 11/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8839 - loss: 0.2203\n",
      "Epoch 11: loss improved from 0.25434 to 0.21842, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8839 - loss: 0.2203\n",
      "Epoch 11: loss improved from 0.25434 to 0.21842, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8796 - loss: 0.2184\n",
      "Epoch 12/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8796 - loss: 0.2184\n",
      "Epoch 12/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8904 - loss: 0.1970\n",
      "Epoch 12: loss improved from 0.21842 to 0.20720, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 12: loss improved from 0.21842 to 0.20720, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8861 - loss: 0.2072\n",
      "Epoch 13/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8861 - loss: 0.2072\n",
      "Epoch 13/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8883 - loss: 0.2037\n",
      "Epoch 13: loss improved from 0.20720 to 0.20456, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 13: loss improved from 0.20720 to 0.20456, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8890 - loss: 0.2046\n",
      "Epoch 14/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8890 - loss: 0.2046\n",
      "Epoch 14/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8737 - loss: 0.2305\n",
      "Epoch 14: loss did not improve from 0.20456\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8773 - loss: 0.2355\n",
      "\n",
      "Epoch 14: loss did not improve from 0.20456\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8773 - loss: 0.2355\n",
      "Epoch 15/100\n",
      "Epoch 15/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8908 - loss: 0.2135\n",
      "Epoch 15: loss improved from 0.20456 to 0.19502, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 15: loss improved from 0.20456 to 0.19502, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8947 - loss: 0.1950\n",
      "Epoch 16/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8947 - loss: 0.1950\n",
      "Epoch 16/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9132 - loss: 0.1523\n",
      "Epoch 16: loss improved from 0.19502 to 0.15062, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 16: loss improved from 0.19502 to 0.15062, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9129 - loss: 0.1506\n",
      "Epoch 17/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9129 - loss: 0.1506\n",
      "Epoch 17/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9182 - loss: 0.1371\n",
      "Epoch 17: loss improved from 0.15062 to 0.12913, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 17: loss improved from 0.15062 to 0.12913, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9225 - loss: 0.1291\n",
      "Epoch 18/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9225 - loss: 0.1291\n",
      "Epoch 18/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9312 - loss: 0.1112\n",
      "Epoch 18: loss improved from 0.12913 to 0.12892, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 18: loss improved from 0.12913 to 0.12892, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9257 - loss: 0.1289\n",
      "Epoch 19/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9257 - loss: 0.1289\n",
      "Epoch 19/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9194 - loss: 0.1603\n",
      "Epoch 19: loss did not improve from 0.12892\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9151 - loss: 0.1663\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 19: loss did not improve from 0.12892\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9151 - loss: 0.1663\n",
      "Epoch 20/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9101 - loss: 0.1805\n",
      "Epoch 20: loss did not improve from 0.12892\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9122 - loss: 0.1735\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 20: loss did not improve from 0.12892\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9122 - loss: 0.1735\n",
      "Epoch 21/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9355 - loss: 0.1158\n",
      "Epoch 21: loss improved from 0.12892 to 0.10382, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 21: loss improved from 0.12892 to 0.10382, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9390 - loss: 0.1038\n",
      "Epoch 22/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9390 - loss: 0.1038\n",
      "Epoch 22/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9475 - loss: 0.0878\n",
      "Epoch 22: loss improved from 0.10382 to 0.08317, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 22: loss improved from 0.10382 to 0.08317, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9492 - loss: 0.0832\n",
      "Epoch 23/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9492 - loss: 0.0832\n",
      "Epoch 23/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8840 - loss: 0.2406\n",
      "Epoch 23: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8911 - loss: 0.2186\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 23: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8911 - loss: 0.2186\n",
      "Epoch 24/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8558 - loss: 0.3554\n",
      "Epoch 24: loss did not improve from 0.08317\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8558 - loss: 0.3554\n",
      "Epoch 24: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8709 - loss: 0.2971\n",
      "Epoch 25/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8709 - loss: 0.2971\n",
      "Epoch 25/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9137 - loss: 0.1525\n",
      "Epoch 25: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9204 - loss: 0.1393\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 25: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9204 - loss: 0.1393\n",
      "Epoch 26/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9394 - loss: 0.1026\n",
      "Epoch 26: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9392 - loss: 0.1026\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 26: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9392 - loss: 0.1026\n",
      "Epoch 27/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9479 - loss: 0.0863\n",
      "Epoch 27: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9493 - loss: 0.0839\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 27: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9493 - loss: 0.0839\n",
      "Epoch 28/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9449 - loss: 0.0873\n",
      "Epoch 28: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9461 - loss: 0.0919\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9449 - loss: 0.0873\n",
      "Epoch 28: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9461 - loss: 0.0919\n",
      "Epoch 29/100\n",
      "Epoch 29/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8952 - loss: 0.2182\n",
      "Epoch 29: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9093 - loss: 0.1889\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 29: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9093 - loss: 0.1889\n",
      "Epoch 30/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8931 - loss: 0.2147\n",
      "Epoch 30: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9109 - loss: 0.1782\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 30: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9109 - loss: 0.1782\n",
      "Epoch 31/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9096 - loss: 0.1629\n",
      "Epoch 31: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9228 - loss: 0.1406\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 31: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9228 - loss: 0.1406\n",
      "Epoch 32/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9495 - loss: 0.0830\n",
      "Epoch 32: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9496 - loss: 0.0851\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 32: loss did not improve from 0.08317\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9496 - loss: 0.0851\n",
      "Epoch 33/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9462 - loss: 0.0897\n",
      "Epoch 33: loss improved from 0.08317 to 0.08242, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 33: loss improved from 0.08317 to 0.08242, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9497 - loss: 0.0824\n",
      "Epoch 34/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9497 - loss: 0.0824\n",
      "Epoch 34/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9476 - loss: 0.0884\n",
      "Epoch 34: loss improved from 0.08242 to 0.06938, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 34: loss improved from 0.08242 to 0.06938, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9575 - loss: 0.0694\n",
      "Epoch 35/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9575 - loss: 0.0694\n",
      "Epoch 35/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9591 - loss: 0.0690\n",
      "Epoch 35: loss did not improve from 0.06938\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9591 - loss: 0.0727\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 35: loss did not improve from 0.06938\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9591 - loss: 0.0727\n",
      "Epoch 36/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9523 - loss: 0.0993\n",
      "Epoch 36: loss did not improve from 0.06938\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9476 - loss: 0.1086\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 36: loss did not improve from 0.06938\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9476 - loss: 0.1086\n",
      "Epoch 37/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8828 - loss: 0.3220\n",
      "Epoch 37: loss did not improve from 0.06938\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9017 - loss: 0.2402\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 37: loss did not improve from 0.06938\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9017 - loss: 0.2402\n",
      "Epoch 38/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9513 - loss: 0.0908\n",
      "Epoch 38: loss did not improve from 0.06938\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9546 - loss: 0.0820\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 38: loss did not improve from 0.06938\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9546 - loss: 0.0820\n",
      "Epoch 39/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9692 - loss: 0.0549\n",
      "Epoch 39: loss improved from 0.06938 to 0.05653, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 39: loss improved from 0.06938 to 0.05653, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9683 - loss: 0.0565\n",
      "Epoch 40/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9683 - loss: 0.0565\n",
      "Epoch 40/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9665 - loss: 0.0605\n",
      "Epoch 40: loss did not improve from 0.05653\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9639 - loss: 0.0667\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 40: loss did not improve from 0.05653\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9639 - loss: 0.0667\n",
      "Epoch 41/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9647 - loss: 0.0581\n",
      "Epoch 41: loss improved from 0.05653 to 0.05347, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 41: loss improved from 0.05653 to 0.05347, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9679 - loss: 0.0535\n",
      "Epoch 42/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9679 - loss: 0.0535\n",
      "Epoch 42/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9713 - loss: 0.0504\n",
      "Epoch 42: loss improved from 0.05347 to 0.04803, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 42: loss improved from 0.05347 to 0.04803, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.0480\n",
      "Epoch 43/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.0480\n",
      "Epoch 43/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.0499\n",
      "Epoch 43: loss improved from 0.04803 to 0.04688, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 43: loss improved from 0.04803 to 0.04688, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9731 - loss: 0.0469\n",
      "Epoch 44/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9731 - loss: 0.0469\n",
      "Epoch 44/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9699 - loss: 0.0597\n",
      "Epoch 44: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9638 - loss: 0.0686\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 44: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9638 - loss: 0.0686\n",
      "Epoch 45/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9424 - loss: 0.1366\n",
      "Epoch 45: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9347 - loss: 0.1559\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 45: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9347 - loss: 0.1559\n",
      "Epoch 46/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9371 - loss: 0.1120\n",
      "Epoch 46: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9477 - loss: 0.1028\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 46: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9477 - loss: 0.1028\n",
      "Epoch 47/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9302 - loss: 0.1556\n",
      "Epoch 47: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9404 - loss: 0.1285\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 47: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9404 - loss: 0.1285\n",
      "Epoch 48/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9674 - loss: 0.0613\n",
      "Epoch 48: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9674 - loss: 0.0613\n",
      "Epoch 48: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9690 - loss: 0.0581\n",
      "Epoch 49/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9690 - loss: 0.0581\n",
      "Epoch 49/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9690 - loss: 0.0571\n",
      "Epoch 49: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9712 - loss: 0.0523\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9690 - loss: 0.0571\n",
      "Epoch 49: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9712 - loss: 0.0523\n",
      "Epoch 50/100\n",
      "Epoch 50/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9654 - loss: 0.0639\n",
      "Epoch 50: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9659 - loss: 0.0659\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 50: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9659 - loss: 0.0659\n",
      "Epoch 51/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9690 - loss: 0.0594\n",
      "Epoch 51: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9688 - loss: 0.0608\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 51: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9688 - loss: 0.0608\n",
      "Epoch 52/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9715 - loss: 0.0550\n",
      "Epoch 52: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.0672\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 52: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.0672\n",
      "Epoch 53/100\n",
      "\u001b[1m475/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9619 - loss: 0.0786\n",
      "Epoch 53: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9678 - loss: 0.0624\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 53: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9678 - loss: 0.0624\n",
      "Epoch 54/100\n",
      "\u001b[1m472/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9750 - loss: 0.0464\n",
      "Epoch 54: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9730 - loss: 0.0515\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 54: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9730 - loss: 0.0515\n",
      "Epoch 55/100\n",
      "\u001b[1m472/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9600 - loss: 0.0766\n",
      "Epoch 55: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9653 - loss: 0.0647\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 55: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9653 - loss: 0.0647\n",
      "Epoch 56/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9739 - loss: 0.0522\n",
      "Epoch 56: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9712 - loss: 0.0604\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 56: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9712 - loss: 0.0604\n",
      "Epoch 57/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9313 - loss: 0.1645\n",
      "Epoch 57: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9419 - loss: 0.1337\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9313 - loss: 0.1645\n",
      "Epoch 57: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9419 - loss: 0.1337\n",
      "Epoch 58/100\n",
      "Epoch 58/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9597 - loss: 0.0770\n",
      "Epoch 58: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9646 - loss: 0.0647\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 58: loss did not improve from 0.04688\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9646 - loss: 0.0647\n",
      "Epoch 59/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9797 - loss: 0.0377\n",
      "Epoch 59: loss improved from 0.04688 to 0.03369, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 59: loss improved from 0.04688 to 0.03369, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9817 - loss: 0.0337\n",
      "Epoch 60/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9817 - loss: 0.0337\n",
      "Epoch 60/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9794 - loss: 0.0409\n",
      "Epoch 60: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9751 - loss: 0.0498\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 60: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9751 - loss: 0.0498\n",
      "Epoch 61/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9686 - loss: 0.0680\n",
      "Epoch 61: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.0636\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 61: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.0636\n",
      "Epoch 62/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9754 - loss: 0.0531\n",
      "Epoch 62: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9760 - loss: 0.0498\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 62: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9760 - loss: 0.0498\n",
      "Epoch 63/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9749 - loss: 0.0471\n",
      "Epoch 63: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.0549\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 63: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.0549\n",
      "Epoch 64/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9685 - loss: 0.0704\n",
      "Epoch 64: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9672 - loss: 0.0711\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 64: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9672 - loss: 0.0711\n",
      "Epoch 65/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9758 - loss: 0.0471\n",
      "Epoch 65: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9759 - loss: 0.0489\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 65: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9759 - loss: 0.0489\n",
      "Epoch 66/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9605 - loss: 0.0849\n",
      "Epoch 66: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9648 - loss: 0.0756\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 66: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9648 - loss: 0.0756\n",
      "Epoch 67/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 0.0782\n",
      "Epoch 67: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.0650\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 67: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.0650\n",
      "Epoch 68/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9802 - loss: 0.0423\n",
      "Epoch 68: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9786 - loss: 0.0456\n",
      "Epoch 69/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9802 - loss: 0.0423\n",
      "Epoch 68: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9786 - loss: 0.0456\n",
      "Epoch 69/100\n",
      "\u001b[1m472/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9509 - loss: 0.0996\n",
      "Epoch 69: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9598 - loss: 0.0833\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 69: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9598 - loss: 0.0833\n",
      "Epoch 70/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9765 - loss: 0.0523\n",
      "Epoch 70: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.0485\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 70: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.0485\n",
      "Epoch 71/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9760 - loss: 0.0506\n",
      "Epoch 71: loss did not improve from 0.03369\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9760 - loss: 0.0506\n",
      "Epoch 71: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.0481\n",
      "Epoch 72/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.0481\n",
      "Epoch 72/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9532 - loss: 0.1178\n",
      "Epoch 72: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9591 - loss: 0.0947\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 72: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9591 - loss: 0.0947\n",
      "Epoch 73/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9770 - loss: 0.0438\n",
      "Epoch 73: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9776 - loss: 0.0439\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 73: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9776 - loss: 0.0439\n",
      "Epoch 74/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9672 - loss: 0.0642\n",
      "Epoch 74: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9706 - loss: 0.0555\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 74: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9706 - loss: 0.0555\n",
      "Epoch 75/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9839 - loss: 0.0303\n",
      "Epoch 75: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9806 - loss: 0.0378\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 75: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9806 - loss: 0.0378\n",
      "Epoch 76/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9779 - loss: 0.0538\n",
      "Epoch 76: loss did not improve from 0.03369\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9779 - loss: 0.0538\n",
      "Epoch 76: loss did not improve from 0.03369\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9779 - loss: 0.0528\n",
      "Epoch 77/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9779 - loss: 0.0528\n",
      "Epoch 77/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9810 - loss: 0.0338\n",
      "Epoch 77: loss improved from 0.03369 to 0.03101, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 77: loss improved from 0.03369 to 0.03101, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9826 - loss: 0.0310\n",
      "Epoch 78/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9826 - loss: 0.0310\n",
      "Epoch 78/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9839 - loss: 0.0300\n",
      "Epoch 78: loss improved from 0.03101 to 0.02826, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 78: loss improved from 0.03101 to 0.02826, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9849 - loss: 0.0283\n",
      "Epoch 79/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9849 - loss: 0.0283\n",
      "Epoch 79/100\n",
      "\u001b[1m476/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9805 - loss: 0.0383\n",
      "Epoch 79: loss did not improve from 0.02826\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9777 - loss: 0.0459\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 79: loss did not improve from 0.02826\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9777 - loss: 0.0459\n",
      "Epoch 80/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9282 - loss: 0.1759\n",
      "Epoch 80: loss did not improve from 0.02826\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9446 - loss: 0.1276\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 80: loss did not improve from 0.02826\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9446 - loss: 0.1276\n",
      "Epoch 81/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9710 - loss: 0.0624\n",
      "Epoch 81: loss did not improve from 0.02826\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.0568\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 81: loss did not improve from 0.02826\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.0568\n",
      "Epoch 82/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9829 - loss: 0.0332\n",
      "Epoch 82: loss did not improve from 0.02826\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9832 - loss: 0.0313\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 82: loss did not improve from 0.02826\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9832 - loss: 0.0313\n",
      "Epoch 83/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9888 - loss: 0.0273\n",
      "Epoch 83: loss improved from 0.02826 to 0.02789, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9888 - loss: 0.0273\n",
      "Epoch 83: loss improved from 0.02826 to 0.02789, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9869 - loss: 0.0279\n",
      "Epoch 84/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9869 - loss: 0.0279\n",
      "Epoch 84/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9688 - loss: 0.0567\n",
      "Epoch 84: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.0560\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 84: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.0560\n",
      "Epoch 85/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9730 - loss: 0.0624\n",
      "Epoch 85: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.0504\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 85: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.0504\n",
      "Epoch 86/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9784 - loss: 0.0526\n",
      "Epoch 86: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9767 - loss: 0.0566\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 86: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9767 - loss: 0.0566\n",
      "Epoch 87/100\n",
      "\u001b[1m472/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9664 - loss: 0.0711\n",
      "Epoch 87: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.0561\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 87: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.0561\n",
      "Epoch 88/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9799 - loss: 0.0431\n",
      "Epoch 88: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0390\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 88: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0390\n",
      "Epoch 89/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9845 - loss: 0.0338\n",
      "Epoch 89: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9827 - loss: 0.0362\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 89: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9827 - loss: 0.0362\n",
      "Epoch 90/100\n",
      "\u001b[1m481/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9819 - loss: 0.0397\n",
      "Epoch 90: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9795 - loss: 0.0432\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 90: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9795 - loss: 0.0432\n",
      "Epoch 91/100\n",
      "\u001b[1m477/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9693 - loss: 0.0652\n",
      "Epoch 91: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9714 - loss: 0.0598\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 91: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9714 - loss: 0.0598\n",
      "Epoch 92/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9743 - loss: 0.0572\n",
      "Epoch 92: loss did not improve from 0.02789\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9743 - loss: 0.0572\n",
      "Epoch 92: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9741 - loss: 0.0590\n",
      "Epoch 93/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9741 - loss: 0.0590\n",
      "Epoch 93/100\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.0624\n",
      "Epoch 93: loss did not improve from 0.02789\n",
      "\u001b[1m482/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.0624\n",
      "Epoch 93: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9785 - loss: 0.0503\n",
      "Epoch 94/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9785 - loss: 0.0503\n",
      "Epoch 94/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9849 - loss: 0.0325\n",
      "Epoch 94: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9815 - loss: 0.0450\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 94: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9815 - loss: 0.0450\n",
      "Epoch 95/100\n",
      "\u001b[1m479/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9527 - loss: 0.1070\n",
      "Epoch 95: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9612 - loss: 0.0851\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 95: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9612 - loss: 0.0851\n",
      "Epoch 96/100\n",
      "\u001b[1m474/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9773 - loss: 0.0398\n",
      "Epoch 96: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9819 - loss: 0.0348\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 96: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9819 - loss: 0.0348\n",
      "Epoch 97/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9772 - loss: 0.0426\n",
      "Epoch 97: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9826 - loss: 0.0327\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 97: loss did not improve from 0.02789\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9826 - loss: 0.0327\n",
      "Epoch 98/100\n",
      "\u001b[1m480/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9893 - loss: 0.0197\n",
      "Epoch 98: loss improved from 0.02789 to 0.02023, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "Epoch 98: loss improved from 0.02789 to 0.02023, saving model to models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9895 - loss: 0.0202\n",
      "Epoch 99/100\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9895 - loss: 0.0202\n",
      "Epoch 99/100\n",
      "\u001b[1m473/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9899 - loss: 0.0212\n",
      "Epoch 99: loss did not improve from 0.02023\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9853 - loss: 0.0321\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 99: loss did not improve from 0.02023\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9853 - loss: 0.0321\n",
      "Epoch 100/100\n",
      "\u001b[1m478/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9650 - loss: 0.0831\n",
      "Epoch 100: loss did not improve from 0.02023\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9695 - loss: 0.0690\n",
      "\n",
      "Epoch 100: loss did not improve from 0.02023\n",
      "\u001b[1m483/483\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9695 - loss: 0.0690\n",
      "\n",
      "================================================================================\n",
      "âœ… FINAL MODEL TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ Training Summary:\n",
      "   Epochs trained: 100\n",
      "   Final train accuracy: 0.9695\n",
      "   Final train loss: 0.0690\n",
      "   ğŸ† BEST Epoch: 98\n",
      "   ğŸ† BEST train accuracy: 0.9895\n",
      "   ğŸ† BEST train loss: 0.0202\n",
      "   Training time: 4.39 minutes\n",
      "   Model saved to: models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "================================================================================\n",
      "ğŸ”® GENERATING KAGGLE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‚ Loading BEST model from: models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "   (This is the model from epoch 98 with lowest training loss)\n",
      "\n",
      "================================================================================\n",
      "âœ… FINAL MODEL TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ Training Summary:\n",
      "   Epochs trained: 100\n",
      "   Final train accuracy: 0.9695\n",
      "   Final train loss: 0.0690\n",
      "   ğŸ† BEST Epoch: 98\n",
      "   ğŸ† BEST train accuracy: 0.9895\n",
      "   ğŸ† BEST train loss: 0.0202\n",
      "   Training time: 4.39 minutes\n",
      "   Model saved to: models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "\n",
      "================================================================================\n",
      "ğŸ”® GENERATING KAGGLE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‚ Loading BEST model from: models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "   (This is the model from epoch 98 with lowest training loss)\n",
      "\n",
      "ğŸ”® Generating predictions on test set (3428 samples)...\n",
      "\n",
      "ğŸ”® Generating predictions on test set (3428 samples)...\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n",
      "âœ“ Predictions saved to: BiGRU_FINAL_adam_tanh_256_FULLDATA_kaggle_submission.csv\n",
      "\n",
      "ğŸ“Š Creating training history plots...\n",
      "âœ“ Predictions saved to: BiGRU_FINAL_adam_tanh_256_FULLDATA_kaggle_submission.csv\n",
      "\n",
      "ğŸ“Š Creating training history plots...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training history plot saved to: BiGRU_FINAL_adam_tanh_256_FULLDATA_training_history.png\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>â–â–‚â–ƒâ–…â–†â–†â–‡â–†â–‡â–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/learning_rate</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/loss</td><td>â–ˆâ–†â–…â–„â–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>98</td></tr><tr><td>best_train_acc</td><td>0.98953</td></tr><tr><td>best_train_loss</td><td>0.02023</td></tr><tr><td>epoch/accuracy</td><td>0.96947</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.06902</td></tr><tr><td>final_train_acc</td><td>0.96947</td></tr><tr><td>final_train_loss</td><td>0.06902</td></tr><tr><td>training_time_sec</td><td>263.66058</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BiGRU_FINAL_adam_tanh_256_FULLDATA</strong> at: <a href='https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked/runs/472hei05' target=\"_blank\">https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked/runs/472hei05</a><br> View project at: <a href='https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked' target=\"_blank\">https://wandb.ai/guldmand-university-of-southern-denmark/Assignment4-RNN-Chunked</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251205_071326-472hei05/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ‰ ALL DONE! FINAL MODEL TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“¦ Outputs:\n",
      "   â€¢ Best Model: models/BiGRU_FINAL_adam_tanh_256_FULLDATA_best.keras\n",
      "   â€¢ Kaggle Predictions: BiGRU_FINAL_adam_tanh_256_FULLDATA_kaggle_submission.csv\n",
      "   â€¢ Training plot: BiGRU_FINAL_adam_tanh_256_FULLDATA_training_history.png\n",
      "\n",
      "ğŸ† FINAL RESULTS:\n",
      "   Training Samples: 30850\n",
      "   Best Epoch: 98\n",
      "   Best Train Accuracy: 0.9895 (98.95%)\n",
      "   Best Train Loss: 0.0202\n",
      "   Final Train Accuracy: 0.9695 (96.95%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# FINAL BiGRU MODEL - TRAIN ON FULL DATA (TRAIN + VAL COMBINED)\n",
    "# ====================================================================\n",
    "import uuid\n",
    "\n",
    "# Model configuration from best hyperparameters\n",
    "FINAL_MODEL_NAME = \"BiGRU_FINAL_adam_tanh_256_FULLDATA\"\n",
    "FINAL_RNN_TYPE = 'BiGRU'\n",
    "FINAL_OPTIMIZER = 'adam'\n",
    "FINAL_ACTIVATION = 'tanh'\n",
    "FINAL_DROPOUT = 0 #0.1, #0.2\n",
    "FINAL_RNN_UNITS = 256\n",
    "FINAL_BATCH_NORM = True\n",
    "FINAL_LEARNING_RATE = 0.001\n",
    "\n",
    "# Training configuration\n",
    "FINAL_EPOCHS = 100  # Based on tuning results showing convergence around 40-50 epochs\n",
    "FINAL_BATCH_SIZE = 64\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"ğŸš€ TRAINING FINAL MODEL: {FINAL_MODEL_NAME}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“‹ CONFIGURATION:\")\n",
    "print(f\"   Architecture: {FINAL_RNN_TYPE}\")\n",
    "print(f\"   Optimizer: {FINAL_OPTIMIZER}\")\n",
    "print(f\"   Activation: {FINAL_ACTIVATION}\")\n",
    "print(f\"   Dropout: {FINAL_DROPOUT}\")\n",
    "print(f\"   RNN Units: {FINAL_RNN_UNITS}\")\n",
    "print(f\"   Batch Norm: {FINAL_BATCH_NORM}\")\n",
    "print(f\"   Learning Rate: {FINAL_LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {FINAL_EPOCHS}\")\n",
    "print(f\"   Batch Size: {FINAL_BATCH_SIZE}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine train and validation data\n",
    "print(\"\\nğŸ”„ Combining train and validation datasets...\")\n",
    "X_full = np.concatenate([X_tr_embeddings_chunked, X_val_embeddings_chunked], axis=0)\n",
    "y_full = np.concatenate([y_tr_indexed, y_val_indexed], axis=0)\n",
    "\n",
    "print(f\"âœ“ Full training set: {X_full.shape[0]} samples\")\n",
    "print(f\"   Original train: {X_tr_embeddings_chunked.shape[0]} samples\")\n",
    "print(f\"   Original validation: {X_val_embeddings_chunked.shape[0]} samples\")\n",
    "\n",
    "# Create full dataset\n",
    "full_ds = tf.data.Dataset.from_tensor_slices((X_full, y_full))\n",
    "full_ds = full_ds.shuffle(buffer_size=10000, seed=SEED).batch(FINAL_BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ Building final {FINAL_RNN_TYPE} model...\")\n",
    "\n",
    "# Build model\n",
    "final_model = build_rnn_model(\n",
    "    rnn_type=FINAL_RNN_TYPE,\n",
    "    rnn_units=FINAL_RNN_UNITS,\n",
    "    activation=FINAL_ACTIVATION,\n",
    "    dropout=FINAL_DROPOUT,\n",
    "    use_batch_norm=FINAL_BATCH_NORM\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=FINAL_LEARNING_RATE)\n",
    "\n",
    "final_model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nğŸ“Š Model Architecture:\")\n",
    "final_model.summary()\n",
    "\n",
    "# Initialize W&B\n",
    "print(\"\\nğŸ”— Initializing Weights & Biases tracking...\")\n",
    "RUN_ID = uuid.uuid4().hex[:8]\n",
    "final_run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    entity=WANDB_ENTITY,\n",
    "    name=FINAL_MODEL_NAME,\n",
    "    config={\n",
    "        \"model_type\": \"FINAL_PRODUCTION\",\n",
    "        \"architecture\": FINAL_RNN_TYPE,\n",
    "        \"optimizer\": FINAL_OPTIMIZER,\n",
    "        \"learning_rate\": FINAL_LEARNING_RATE,\n",
    "        \"activation\": FINAL_ACTIVATION,\n",
    "        \"dropout\": FINAL_DROPOUT,\n",
    "        \"rnn_units\": FINAL_RNN_UNITS,\n",
    "        \"batch_norm\": FINAL_BATCH_NORM,\n",
    "        \"epochs\": FINAL_EPOCHS,\n",
    "        \"batch_size\": FINAL_BATCH_SIZE,\n",
    "        \"training_samples\": X_full.shape[0],\n",
    "        \"seed\": SEED,\n",
    "        \"class_weights\": \"balanced\",\n",
    "        \"data\": \"train+validation_combined\",\n",
    "        \"run_id\": RUN_ID\n",
    "    },\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = f'models/{FINAL_MODEL_NAME}_best.keras'\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='loss',           # Monitor training loss\n",
    "    save_best_only=True,      # Only save when loss improves\n",
    "    save_weights_only=False,  # Save full model\n",
    "    mode='min',               # Minimize loss\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "wandb_callback = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "\n",
    "# Train model with class weights\n",
    "print(f\"\\nğŸ¯ Training final model on {X_full.shape[0]} samples...\")\n",
    "print(f\"   Epochs: {FINAL_EPOCHS}\")\n",
    "print(f\"   ğŸ’¡ ModelCheckpoint will save best model (lowest training loss)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = final_model.fit(\n",
    "    full_ds,\n",
    "    epochs=FINAL_EPOCHS,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[checkpoint_callback, wandb_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… FINAL MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get final metrics\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "train_loss = history.history['loss'][-1]\n",
    "epochs_trained = len(history.history['accuracy'])\n",
    "\n",
    "# Find best epoch\n",
    "best_epoch = np.argmin(history.history['loss']) + 1\n",
    "best_train_loss = np.min(history.history['loss'])\n",
    "best_train_acc = history.history['accuracy'][np.argmin(history.history['loss'])]\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Training Summary:\")\n",
    "print(f\"   Epochs trained: {epochs_trained}\")\n",
    "print(f\"   Final train accuracy: {train_acc:.4f}\")\n",
    "print(f\"   Final train loss: {train_loss:.4f}\")\n",
    "print(f\"   ğŸ† BEST Epoch: {best_epoch}\")\n",
    "print(f\"   ğŸ† BEST train accuracy: {best_train_acc:.4f}\")\n",
    "print(f\"   ğŸ† BEST train loss: {best_train_loss:.4f}\")\n",
    "print(f\"   Training time: {train_time/60:.2f} minutes\")\n",
    "print(f\"   Model saved to: {checkpoint_path}\")\n",
    "\n",
    "# Log to W&B\n",
    "wandb.run.summary[\"final_train_acc\"] = train_acc\n",
    "wandb.run.summary[\"final_train_loss\"] = train_loss\n",
    "wandb.run.summary[\"best_epoch\"] = best_epoch\n",
    "wandb.run.summary[\"best_train_acc\"] = best_train_acc\n",
    "wandb.run.summary[\"best_train_loss\"] = best_train_loss\n",
    "wandb.run.summary[\"training_time_sec\"] = train_time\n",
    "\n",
    "# ====================================================================\n",
    "# GENERATE PREDICTIONS FOR KAGGLE SUBMISSION\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”® GENERATING KAGGLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model\n",
    "print(f\"\\nğŸ“‚ Loading BEST model from: {checkpoint_path}\")\n",
    "print(f\"   (This is the model from epoch {best_epoch} with lowest training loss)\")\n",
    "best_final_model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "# Generate predictions on test set\n",
    "print(f\"\\nğŸ”® Generating predictions on test set ({X_test_embeddings_chunked.shape[0]} samples)...\")\n",
    "test_predictions = best_final_model.predict(test_ds_chunked, verbose=1)\n",
    "test_pred_classes = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Save predictions for Kaggle\n",
    "predictions_file = f'{FINAL_MODEL_NAME}_kaggle_submission.csv'\n",
    "test_df = pd.DataFrame({\n",
    "    'id': range(len(test_pred_classes)),  # Add ID column if needed by Kaggle\n",
    "    'prediction': test_pred_classes,\n",
    "    'confidence': np.max(test_predictions, axis=1)\n",
    "})\n",
    "test_df.to_csv(predictions_file, index=False)\n",
    "print(f\"âœ“ Predictions saved to: {predictions_file}\")\n",
    "\n",
    "# ====================================================================\n",
    "# PLOT TRAINING HISTORY\n",
    "# ====================================================================\n",
    "print(\"\\nğŸ“Š Creating training history plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2, color='blue')\n",
    "axes[0].axvline(x=best_epoch-1, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Best Epoch ({best_epoch})')\n",
    "axes[0].axhline(y=best_train_acc, color='green', linestyle=':', alpha=0.5)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title(f'{FINAL_MODEL_NAME} - Training Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2, color='blue')\n",
    "axes[1].axvline(x=best_epoch-1, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Best Epoch ({best_epoch})')\n",
    "axes[1].axhline(y=best_train_loss, color='green', linestyle=':', alpha=0.5)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title(f'{FINAL_MODEL_NAME} - Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_filename = f'{FINAL_MODEL_NAME}_training_history.png'\n",
    "plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"âœ“ Training history plot saved to: {plot_filename}\")\n",
    "\n",
    "# Finish W&B run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ ALL DONE! FINAL MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“¦ Outputs:\")\n",
    "print(f\"   â€¢ Best Model: {checkpoint_path}\")\n",
    "print(f\"   â€¢ Kaggle Predictions: {predictions_file}\")\n",
    "print(f\"   â€¢ Training plot: {plot_filename}\")\n",
    "print(f\"\\nğŸ† FINAL RESULTS:\")\n",
    "print(f\"   Training Samples: {X_full.shape[0]}\")\n",
    "print(f\"   Best Epoch: {best_epoch}\")\n",
    "print(f\"   Best Train Accuracy: {best_train_acc:.4f} ({best_train_acc*100:.2f}%)\")\n",
    "print(f\"   Best Train Loss: {best_train_loss:.4f}\")\n",
    "print(f\"   Final Train Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc57b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created Kaggle-compatible submission: BiGRU_FINAL_adam_tanh_256_FULLDATA_kaggle_submission_CORRECTED.csv\n",
      "\n",
      "First 5 rows:\n",
      "   Id  Predicted\n",
      "0   0          4\n",
      "1   1          4\n",
      "2   2          4\n",
      "3   3          4\n",
      "4   4          4\n"
     ]
    }
   ],
   "source": [
    "# Quick fix: Konverter eksisterende submission til korrekt format\n",
    "old_file = 'BiGRU_FINAL_adam_tanh_256_FULLDATA_kaggle_submission.csv'\n",
    "new_file = 'BiGRU_FINAL_adam_tanh_256_FULLDATA_kaggle_submission_CORRECTED.csv'\n",
    "\n",
    "# LÃ¦s eksisterende fil\n",
    "df = pd.read_csv(old_file)\n",
    "\n",
    "# Lav korrekt format\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': df['id'],\n",
    "    'Predicted': df['prediction']\n",
    "})\n",
    "\n",
    "# Gem i korrekt format\n",
    "submission_df.to_csv(new_file, index=False)\n",
    "print(f\"âœ… Created Kaggle-compatible submission: {new_file}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9aa6d01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ—ï¸  BUILDING FINAL MODEL WITH BEST HYPERPARAMETERS\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ—ï¸  BUILDING FINAL MODEL WITH BEST HYPERPARAMETERS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m      6\u001b[39m final_model = build_rnn_model(\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     rnn_type=\u001b[43mbest_config\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mrnn_type\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      8\u001b[39m     rnn_units=best_config[\u001b[33m'\u001b[39m\u001b[33mrnn_units\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      9\u001b[39m     activation=best_config[\u001b[33m'\u001b[39m\u001b[33mactivation\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     10\u001b[39m     dropout=best_config[\u001b[33m'\u001b[39m\u001b[33mdropout\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     11\u001b[39m     use_batch_norm=best_config[\u001b[33m'\u001b[39m\u001b[33mbatch_norm\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Compile model\u001b[39;00m\n\u001b[32m     15\u001b[39m optimizer_instance = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'best_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Build final model with best hyperparameters\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ—ï¸  BUILDING FINAL MODEL WITH BEST HYPERPARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_model = build_rnn_model(\n",
    "    rnn_type=best_config['rnn_type'],\n",
    "    rnn_units=best_config['rnn_units'],\n",
    "    activation=best_config['activation'],\n",
    "    dropout=best_config['dropout'],\n",
    "    use_batch_norm=best_config['batch_norm']\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "optimizer_instance = None\n",
    "if best_config['optimizer'] == 'adam':\n",
    "    optimizer_instance = tf.keras.optimizers.Adam(learning_rate=best_config['learning_rate'])\n",
    "elif best_config['optimizer'] == 'rmsprop':\n",
    "    optimizer_instance = tf.keras.optimizers.RMSprop(\n",
    "        learning_rate=best_config['learning_rate'],\n",
    "        momentum=best_config['momentum']\n",
    "    )\n",
    "elif best_config['optimizer'] == 'sgd':\n",
    "    optimizer_instance = tf.keras.optimizers.SGD(\n",
    "        learning_rate=best_config['learning_rate'],\n",
    "        momentum=best_config['momentum']\n",
    "    )\n",
    "\n",
    "final_model.compile(\n",
    "    optimizer=optimizer_instance,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Final Model Architecture:\")\n",
    "print(f\"   RNN Type: {best_config['rnn_type']}\")\n",
    "print(f\"   RNN Units: {best_config['rnn_units']}\")\n",
    "print(f\"   Optimizer: {best_config['optimizer']}\")\n",
    "print(f\"   Learning Rate: {best_config['learning_rate']}\")\n",
    "print(f\"   Momentum: {best_config['momentum']}\")\n",
    "print(f\"   Activation: {best_config['activation']}\")\n",
    "print(f\"   Dropout: {best_config['dropout']}\")\n",
    "print(f\"   Batch Norm: {best_config['batch_norm']}\")\n",
    "\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9000c078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ” DIAGNOSTIK: Tjek label mapping\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Original training labels (y_train):\n",
      "   Unique values: [1 2 3 4 5]\n",
      "   Value counts: [    0   985  1346  3496  7050 17973]\n",
      "\n",
      "ğŸ“Š Indexed training labels (y_tr_indexed):\n",
      "   Unique values: [0 1 2 3 4]\n",
      "   Value counts: [  788  1077  2797  5640 14378]\n",
      "\n",
      "ğŸ“Š Model predictions (test_pred_classes):\n",
      "   Unique predicted values: [0 1 2 3 4]\n",
      "   Prediction distribution:\n",
      "      Class 0: 120 samples (3.5%)\n",
      "      Class 1: 120 samples (3.5%)\n",
      "      Class 2: 442 samples (12.9%)\n",
      "      Class 3: 708 samples (20.7%)\n",
      "      Class 4: 2038 samples (59.5%)\n",
      "\n",
      "ğŸ“Š Submission file check:\n",
      "   File: BiGRU_FINAL_adam_tanh_256_FULLDATA_kaggle_submission_CORRECTED.csv\n",
      "Predicted\n",
      "0     120\n",
      "1     120\n",
      "2     442\n",
      "3     708\n",
      "4    2038\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“Š Reference file (y_test_hat.csv):\n",
      "Predicted\n",
      "3      14\n",
      "4     182\n",
      "5    3232\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ” Reference predictions range: 3 to 5\n",
      "ğŸ” Your predictions range: 0 to 4\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” DIAGNOSTIK: Tjek label mapping\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ” DIAGNOSTIK: Tjek label mapping\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tjek original training labels\n",
    "print(\"\\nğŸ“Š Original training labels (y_train):\")\n",
    "print(f\"   Unique values: {np.unique(y_train)}\")\n",
    "print(f\"   Value counts: {np.bincount(y_train.flatten())}\")\n",
    "\n",
    "# Tjek indexed labels (hvad modellen er trÃ¦net pÃ¥)\n",
    "print(\"\\nğŸ“Š Indexed training labels (y_tr_indexed):\")\n",
    "print(f\"   Unique values: {np.unique(y_tr_indexed)}\")\n",
    "print(f\"   Value counts: {np.bincount(y_tr_indexed)}\")\n",
    "\n",
    "# Tjek predictions\n",
    "print(\"\\nğŸ“Š Model predictions (test_pred_classes):\")\n",
    "print(f\"   Unique predicted values: {np.unique(test_pred_classes)}\")\n",
    "print(f\"   Prediction distribution:\")\n",
    "for cls in np.unique(test_pred_classes):\n",
    "    count = np.sum(test_pred_classes == cls)\n",
    "    print(f\"      Class {cls}: {count} samples ({count/len(test_pred_classes)*100:.1f}%)\")\n",
    "\n",
    "# Tjek submission fil\n",
    "print(\"\\nğŸ“Š Submission file check:\")\n",
    "print(f\"   File: {new_file}\")\n",
    "print(submission_df['Predicted'].value_counts().sort_index())\n",
    "\n",
    "# Sammenlign med reference fil\n",
    "print(\"\\nğŸ“Š Reference file (y_test_hat.csv):\")\n",
    "ref_df = pd.read_csv('../../y_test_hat.csv')\n",
    "print(ref_df['Predicted'].value_counts().sort_index())\n",
    "print(f\"\\nğŸ” Reference predictions range: {ref_df['Predicted'].min()} to {ref_df['Predicted'].max()}\")\n",
    "print(f\"ğŸ” Your predictions range: {submission_df['Predicted'].min()} to {submission_df['Predicted'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72128ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ”§ FIXING LABEL MAPPING\n",
      "================================================================================\n",
      "\n",
      "âŒ BEFORE fix:\n",
      "   Prediction range: 0 to 4\n",
      "   Distribution:\n",
      "Predicted\n",
      "0     104\n",
      "1     149\n",
      "2     494\n",
      "3     680\n",
      "4    2001\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… AFTER fix:\n",
      "   Prediction range: 1 to 5\n",
      "   Distribution:\n",
      "Predicted\n",
      "1     104\n",
      "2     149\n",
      "3     494\n",
      "4     680\n",
      "5    2001\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ Upload this file to Kaggle: BiGRU_FINAL_adam_tanh_256_FULLDATA_FIXED_LABELS.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ FIX: Shift predictions back to original label space\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ”§ FIXING LABEL MAPPING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# LÃ¦s original submission\n",
    "df_wrong = pd.read_csv('BiGRU_FINAL_adam_tanh_256_FULLDATA_kaggle_submission_CORRECTED.csv')\n",
    "\n",
    "print(\"\\nâŒ BEFORE fix:\")\n",
    "print(f\"   Prediction range: {df_wrong['Predicted'].min()} to {df_wrong['Predicted'].max()}\")\n",
    "print(f\"   Distribution:\\n{df_wrong['Predicted'].value_counts().sort_index()}\")\n",
    "\n",
    "# Shift predictions +1 (0â†’1, 1â†’2, 2â†’3, 3â†’4, 4â†’5)\n",
    "df_fixed = pd.DataFrame({\n",
    "    'Id': df_wrong['Id'],\n",
    "    'Predicted': df_wrong['Predicted'] + 1  # SHIFT UP BY 1!\n",
    "})\n",
    "\n",
    "# Gem korrekt fil\n",
    "fixed_file = 'BiGRU_FINAL_adam_tanh_256_FULLDATA_FIXED_LABELS.csv'\n",
    "df_fixed.to_csv(fixed_file, index=False)\n",
    "\n",
    "print(\"\\nâœ… AFTER fix:\")\n",
    "print(f\"   Prediction range: {df_fixed['Predicted'].min()} to {df_fixed['Predicted'].max()}\")\n",
    "print(f\"   Distribution:\\n{df_fixed['Predicted'].value_counts().sort_index()}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Upload this file to Kaggle: {fixed_file}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a4102",
   "metadata": {},
   "source": [
    "## Performance documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd692f",
   "metadata": {},
   "source": [
    "### 3.1 TrainningCurves (loss/accuracy/AUC)\n",
    "\n",
    "Vi plotter den **sidst trÃ¦nede models** historik (her: ViT). Du kan let skifte til `history_cnn` eller kombinere i Ã©t plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist, title=\"Training curves\"):\n",
    "    hist = hist.history\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(hist.get(\"loss\",[]), label=\"loss\")\n",
    "    plt.plot(hist.get(\"val_loss\",[]), label=\"val_loss\")\n",
    "    plt.title(title + \" â€” loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.show()\n",
    "\n",
    "    if \"accuracy\" in hist or \"acc\" in hist:\n",
    "        acc_key = \"accuracy\" if \"accuracy\" in hist else \"acc\"\n",
    "        val_acc_key = \"val_accuracy\" if \"val_accuracy\" in hist else \"val_acc\"\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(hist.get(acc_key,[]), label=acc_key)\n",
    "        plt.plot(hist.get(val_acc_key,[]), label=val_acc_key)\n",
    "        plt.title(title + \" â€” accuracy\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.show()\n",
    "\n",
    "    if \"auc\" in hist:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(hist.get(\"auc\",[]), label=\"auc\")\n",
    "        plt.plot(hist.get(\"val_auc\",[]), label=\"val_auc\")\n",
    "        plt.title(title + \" â€” AUC\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"AUC\"); plt.legend(); plt.show()\n",
    "\n",
    "# Plot for CNN (head + finetune)\n",
    "plot_history(history_cnn, \"CNN (frozen backbone)\")\n",
    "plot_history(history_cnn_ft, \"CNN (finetuned)\")\n",
    "\n",
    "\n",
    "# create directory for plots in save_dir\n",
    "os.makedirs(os.path.join(save_dir, \"plots\"), exist_ok=True)\n",
    "\n",
    "# after creating all plots, save them to save_dir\n",
    "for i in plt.get_fignums():\n",
    "    fig = plt.figure(i)\n",
    "    fig.savefig(os.path.join(save_dir, f\"plots/training_curve_{i}_{WANDB_ID}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57dbeb",
   "metadata": {},
   "source": [
    "## 4.1) Prepare final re-fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('preparing final refit')\n",
    "\n",
    "# Train* DataSet (val + train)\n",
    "#train_star = train_ds.concatenate(val_ds).shuffle(\n",
    "#    buffer_size=2048, seed=SEED, reshuffle_each_iteration=True\n",
    "#)\n",
    "\t\n",
    "train_star = train_ds.concatenate(val_ds)\n",
    "\n",
    "# update W&B for refit\n",
    "final_callbacks = [\n",
    "    WandbMetricsLogger(log_freq=\"epoch\"),\n",
    "    WandbModelCheckpoint(\n",
    "        filepath=os.path.join(save_dir, \"final_refit.weights.h5\"),\n",
    "        monitor=\"auc\",    # ingen val-data, sÃ¥ brug trÃ¦nings-AUC\n",
    "        mode=\"max\",\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        save_freq=\"epoch\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "wandb.log({\"phase\": \"final_refit\"})\n",
    "\n",
    "final_model = finetune_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6e571",
   "metadata": {},
   "source": [
    "## 4.2) Final re-fit on train* (train+val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f7507",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[FINAL REFIT]')\n",
    "\n",
    "# Reuse best model weights and ReFit on train*\n",
    "history_final = final_model.fit(\n",
    "\ttrain_star,\n",
    "\tepochs=best_ft_epoch,\n",
    "\tverbose=1,\n",
    "\tcallbacks=final_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final refitted model\n",
    "final_model.save(os.path.join(save_dir, f\"model_final_refit_{WANDB_ID}.keras\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f7217",
   "metadata": {},
   "source": [
    "## 4.3) Log to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B Artifact\n",
    "artifact = wandb.Artifact(\n",
    "    name=f\"{MODEL_NAME}_seed{SEED}_{WANDB_ID}_final_refit\",\n",
    "    type=\"model\",\n",
    "    metadata={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"img_size\": IMG_SIZE,\n",
    "        \"seed\": SEED,\n",
    "        \"finetune_lr\": LR * 0.1,\n",
    "        \"refit_epochs_total\": int(best_ft_epoch),\n",
    "        \"phase\": \"final_refit\",\n",
    "        \"source_run_id\": WANDB_ID,\n",
    "    },\n",
    ")\n",
    "\n",
    "artifact.add_file(os.path.join(save_dir, f\"model_final_refit_{WANDB_ID}.keras\"))\n",
    "\n",
    "artifact.add_file(save_dir + \"/final_refit.weights.h5\")\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "# W&B Metrics ğŸ“Š for final refit\n",
    "final_metrics = {\n",
    "    \"final_refit_best_epoch\": np.argmax(history_final.history[\"auc\"]) + 1,\n",
    "    \"final_refit_last_epoch\": len(history_final.history[\"loss\"]),\n",
    "    \"final_refit_best_auc\": float(np.max(history_final.history[\"auc\"])),\n",
    "    \"final_refit_last_auc\": float(history_final.history[\"auc\"][-1]),\n",
    "    \"final_refit_best_loss\": float(np.min(history_final.history[\"loss\"])),\n",
    "    \"final_refit_last_loss\": float(history_final.history[\"loss\"][-1]),\n",
    "}\n",
    "\n",
    "wandb.log(final_metrics)\n",
    "\n",
    "print(f\"[FINAL REFIT] Done âœ…\")\n",
    "print(f\"[FINAL REFIT] âœ… Model saved: {save_dir}\")\n",
    "print(f\"[FINAL REFIT] ğŸª„ W&B artifact: {artifact.name}\")\n",
    "\n",
    "#finish wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c93bde",
   "metadata": {},
   "source": [
    "### 5) Export CSV with probabilities (on test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission csv using test_ds\n",
    "y_test_hat = final_model.predict(test_ds, verbose=1)\n",
    "ytest_hat = pd.DataFrame({\n",
    "    'Id': list(range(len(y_test_hat))),\n",
    "    'Predicted': y_test_hat.reshape(-1,),\n",
    "})\n",
    "\n",
    "# create directory for csv export inside save_dir\n",
    "os.makedirs(os.path.join(save_dir, \"csv\"), exist_ok=True)\n",
    "ytest_hat.to_csv(f'{save_dir}/csv/ytest_hat_{WANDB_ID}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c070ecb",
   "metadata": {},
   "source": [
    "### 6) Find optimal accuracy threshold based on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === A) Find optimal threshold pÃ¥ val_ds (max accuracy) ===\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"[THR] Finder optimal threshold pÃ¥ val_ds (baseret pÃ¥ accuracy)\")\n",
    "\n",
    "# 1) Hent y_true og p_hat fra val_ds\n",
    "y_val_true = []\n",
    "for _, y in val_ds:\n",
    "    y_val_true.append(y.numpy())\n",
    "y_val_true = np.concatenate(y_val_true).astype(int)\n",
    "\n",
    "# Keras .predict giver sandsynligheder (antager sigmoid i sidste lag)\n",
    "y_val_prob = final_model.predict(val_ds, verbose=0).reshape(-1)\n",
    "\n",
    "# 2) Grid-scan thresholds i [0.0, 1.0]\n",
    "thr_grid = np.linspace(0.0, 1.0, 1001)  # oplÃ¸sning 0.001\n",
    "accs = []\n",
    "\n",
    "for thr in thr_grid:\n",
    "    y_val_pred = (y_val_prob >= thr).astype(int)\n",
    "    accs.append(accuracy_score(y_val_true, y_val_pred))\n",
    "\n",
    "accs = np.array(accs)\n",
    "best_idx = int(np.argmax(accs))\n",
    "best_thr = float(thr_grid[best_idx])\n",
    "best_acc = float(accs[best_idx])\n",
    "\n",
    "print(f\"[THR] Bedste threshold = {best_thr:.3f} med val-accuracy = {best_acc:.4f}\")\n",
    "\n",
    "# (valgfrit) log til W&B\n",
    "try:\n",
    "    wandb.log({\"threshold/best\": best_thr, \"threshold/val_accuracy\": best_acc})\n",
    "except Exception as e:\n",
    "    print(\"[THR] W&B log skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate best threshold plot based on Cross validation k-fold 10, with 0.8 train and 0.2 val splits\n",
    "\n",
    "# setup new datasplit from original train data\n",
    "X_tr_final, X_val_final, y_tr_final, y_val_final = train_test_split(\n",
    "\tXtrain, ytrain, test_size=0.2, random_state=42, stratify=ytrain\n",
    ")\n",
    "\n",
    "# use cross validation to find best threshold with 10 folds\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# for each fold find best threshold\n",
    "best_thresholds = []\n",
    "for train_index, val_index in kf.split(X_tr_final):\n",
    "\tX_tr_fold, X_val_fold = X_tr_final[train_index], X_tr_final[val_index]\n",
    "\ty_tr_fold, y_val_fold = y_tr_final[train_index], y_tr_final[val_index]\n",
    "\n",
    "\t# create datasets\n",
    "\ttrain_ds_fold = make_train_ds(X_tr_fold, y_tr_fold)\n",
    "\tval_ds_fold = make_eval_ds(X_val_fold, y_val_fold)\n",
    "\n",
    "\t# predict on val_ds_fold\n",
    "\ty_val_prob_fold = final_model.predict(val_ds_fold, verbose=0).reshape(-1)\n",
    "\n",
    "\t# get true labels\n",
    "\ty_val_true_fold = []\n",
    "\tfor _, y in val_ds_fold:\n",
    "\t\ty_val_true_fold.append(y.numpy())\n",
    "\ty_val_true_fold = np.concatenate(y_val_true_fold).astype(int)\n",
    "\n",
    "\t# grid scan thresholds\n",
    "\tthr_grid = np.linspace(0.0, 1.0, 1001)\n",
    "\taccs = []\n",
    "\n",
    "\tfor thr in thr_grid:\n",
    "\t\ty_val_pred_fold = (y_val_prob_fold >= thr).astype(int)\n",
    "\t\taccs.append(accuracy_score(y_val_true_fold, y_val_pred_fold))\n",
    "\n",
    "\taccs = np.array(accs)\n",
    "\tbest_idx = int(np.argmax(accs))\n",
    "\tbest_thr = float(thr_grid[best_idx])\n",
    "\tbest_thresholds.append(best_thr)\n",
    "\n",
    "mean_best_thr = np.mean(best_thresholds)\n",
    "std_best_thr = np.std(best_thresholds)\n",
    "\n",
    "print(f\"[THR CV] Mean best threshold from CV = {mean_best_thr:.3f} Â± {std_best_thr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99544d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === B) GenerÃ©r 0/1 Kaggle-CSV fra test_ds ved brug af best_thr ===\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "assert \"best_thr\" in globals(), \"KÃ¸r fÃ¸rst threshold-cellen (A), sÃ¥ 'best_thr' findes.\"\n",
    "\n",
    "# 1) Hent P(y=1) pÃ¥ test\n",
    "y_test_prob = final_model.predict(test_ds, verbose=1).reshape(-1)\n",
    "\n",
    "# 2) Threshold til 0/1\n",
    "y_test_label = (y_test_prob >= best_thr).astype(int)\n",
    "\n",
    "# 3) Gem CSV\n",
    "os.makedirs(os.path.join(save_dir, \"csv\"), exist_ok=True)\n",
    "csv_path = os.path.join(save_dir, \"csv\", f\"submission_threshold_{best_thr:.3f}_{WANDB_ID}.csv\")\n",
    "pd.DataFrame({\"Id\": np.arange(len(y_test_label)), \"Predicted\": y_test_label}).to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"[SUBMIT] Skrev 0/1-label CSV til:\\n{csv_path}\")\n",
    "\n",
    "# (valgfrit) log filen som W&B-artifact\n",
    "try:\n",
    "    art = wandb.Artifact(\n",
    "        name=f\"{MODEL_NAME}_seed{SEED}_{WANDB_ID}_submission_thr\",\n",
    "        type=\"submission\",\n",
    "        metadata={\"threshold\": best_thr, \"source_run_id\": WANDB_ID}\n",
    "    )\n",
    "    art.add_file(csv_path)\n",
    "    wandb.log_artifact(art)\n",
    "except Exception as e:\n",
    "    print(\"[SUBMIT] W&B artifact skip:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
